{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearningRegressionImpute_train10_iff_DA_flow4_optim_xin.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmq9WnX9O8jW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "outputId": "47c46d22-ea7e-4d8f-82b8-46d559db6c7a"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on 12-05-2019\n",
        "\n",
        "@author: Xin Huang\n",
        "\"\"\"\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "# from keras.optimizers import RMSprop\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('content/')\n",
        "# data = np.load('Sat_data_small.npz')\n",
        "# data = np.load('/content/content/My Drive/Colab Notebooks/Sat_data_small.npz')\n",
        "# data = np.load('/content/content/My Drive/Colab Notebooks/kddworkshop/train7.npz')\n",
        "data = np.load('/content/content/My Drive/Colab Notebooks/kddworkshop/fulldata/train10.npz')\n",
        "\n",
        "# passive = 1\n",
        "#load common data\n",
        "latlon = data['latlon']\n",
        "iff = data['iff']\n",
        "\n",
        "X_v = data['viirs']\n",
        "Y_v = data['label']\n",
        "print ('X_v shape:');\n",
        "print (X_v.shape);\n",
        "\n",
        "X_c = data['calipso']\n",
        "Y_c = data['label']\n",
        "print ('X_c shape:');\n",
        "print (X_c.shape);\n",
        "\n",
        "inds_v,vals_v = np.where(Y_v>0)\n",
        "Y_v = Y_v[inds_v]\n",
        "X_v = X_v[inds_v]\n",
        "print ('X_v')\n",
        "print (X_v)\n",
        "\n",
        "inds_c,vals_c = np.where(Y_c>0)\n",
        "Y_c = Y_c[inds_c]\n",
        "X_c = X_c[inds_c]\n",
        "print ('X_c')\n",
        "print (X_c)\n",
        "\n",
        "# process common data\n",
        "Latlon = latlon[inds_v]\n",
        "Iff = iff[inds_v]\n",
        "\n",
        "print('original X_v: ', X_v.shape)\n",
        "rows = np.where((X_v[:,0] >= 0) & (X_v[:,0] <= 83) & (X_v[:,15] > 100) & (X_v[:,15] < 400) & (X_v[:,16] > 100) & (X_v[:,16] < 400) & (X_v[:,17] > 100) & (X_v[:,17] < 400) & (X_v[:,18] > 100) & (X_v[:,18] < 400) & (X_v[:,19] > 100) & (X_v[:,19] < 400) & (X_v[:,10] > 0))\n",
        "print(\"rows:\", rows)\n",
        "print(\"rows.shape:\", len(rows))\n",
        "\n",
        "Latlon = Latlon[rows]\n",
        "Iff = Iff[rows]\n",
        "\n",
        "Y_v = Y_v[rows]\n",
        "X_v = X_v[rows]\n",
        "\n",
        "Y_c = Y_c[rows]\n",
        "X_c = X_c[rows]\n",
        "\n",
        "print('after SZA X_v: ', X_v.shape)\n",
        "print('after SZA X_c: ', X_c.shape)\n",
        "\n",
        "#concanate common data\n",
        "# X_v = np.concatenate((X_v, Latlon, Iff), axis=1)\n",
        "X_c = np.concatenate((X_c, Latlon, Iff), axis=1)\n",
        "print (X_v.shape)\n",
        "print (X_c.shape)\n",
        "\n",
        "X_v = np.nan_to_num(X_v)\n",
        "X_c = np.nan_to_num(X_c)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at content/; to attempt to forcibly remount, call drive.mount(\"content/\", force_remount=True).\n",
            "X_v shape:\n",
            "(2099451, 20)\n",
            "X_c shape:\n",
            "(2099451, 25)\n",
            "X_v\n",
            "[[ 97.43999481 -55.29999924   2.52999997 ... 246.61909485 246.91960144\n",
            "  246.64631653]\n",
            " [ 97.43999481 -55.2899971    2.51999998 ... 247.22109985 247.5663147\n",
            "  247.28843689]\n",
            " [ 97.40999603 -55.22999954   2.5        ... 248.27958679 248.7721405\n",
            "  248.57876587]\n",
            " ...\n",
            " [ 94.68000031 -62.64999771   2.12999988 ... 228.96557617 228.62413025\n",
            "  227.59910583]\n",
            " [ 94.68000031 -62.62999725   2.12999988 ... 229.51361084 229.17892456\n",
            "  227.92941284]\n",
            " [ 94.66999817 -62.6099968    2.12999988 ... 230.20637512 230.03952026\n",
            "  228.89453125]]\n",
            "X_c\n",
            "[[ 1.000e+00  2.000e+00  0.000e+00 ...        nan -9.999e+03        nan]\n",
            " [ 1.000e+00  2.000e+00  0.000e+00 ...        nan -9.999e+03        nan]\n",
            " [ 1.000e+00  2.000e+00  0.000e+00 ...        nan -9.999e+03        nan]\n",
            " ...\n",
            " [ 1.000e+00  1.000e+00  0.000e+00 ...        nan -9.999e+03        nan]\n",
            " [ 1.000e+00  1.000e+00  0.000e+00 ...        nan -9.999e+03        nan]\n",
            " [ 1.000e+00  1.000e+00  0.000e+00 ...        nan -9.999e+03        nan]]\n",
            "original X_v:  (1667850, 20)\n",
            "rows: (array([    402,     403,     404, ..., 1666325, 1666326, 1666327]),)\n",
            "rows.shape: 1\n",
            "after SZA X_v:  (704800, 20)\n",
            "after SZA X_c:  (704800, 25)\n",
            "(704800, 20)\n",
            "(704800, 31)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtFEmQpsPM6H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "70f08531-3020-42f9-df1c-886eec67eba3"
      },
      "source": [
        "# combine data and split latter to define ground truth for MLR\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "n1=20\n",
        "n2=25\n",
        "X=np.concatenate((X_v, X_c), axis=1)\n",
        "Y=Y_v\n",
        "print (X.shape)\n",
        "print (Y_v)\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(X, Y,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=0,\n",
        "                                                    stratify=Y)\n",
        "\n",
        "# x_valid, x_test, y_valid, y_test = train_test_split(x_temp, y_temp,\n",
        "#                                                     test_size=0.5,\n",
        "#                                                     random_state=0,\n",
        "#                                                     stratify=y_temp)\n",
        "\n",
        "# feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler()\n",
        "x_train=sc_X.fit_transform(x_train)\n",
        "x_valid=sc_X.transform(x_valid)\n",
        "# x_test=sc_X.fit_transform(x_test)\n",
        "\n",
        "x_train_v = x_train[:, 0:20]\n",
        "x_train_c = x_train[:, 20:45]\n",
        "x_train_comm = x_train[:, 45:51]\n",
        "x_train_src = x_train[:, 20:51]\n",
        "\n",
        "print(x_train_v.shape)\n",
        "print(x_train_c.shape)\n",
        "print(x_train_comm.shape)\n",
        "print(x_train_src.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "x_valid_v = x_valid[:, 0:20]\n",
        "x_valid_c = x_valid[:, 20:45]\n",
        "x_valid_comm = x_valid[:, 45:51]\n",
        "\n",
        "print(x_valid_v.shape)\n",
        "print(x_valid_c.shape)\n",
        "print(x_valid_comm.shape)\n",
        "\n",
        "# x_test_v = x_test[:, 0:20]\n",
        "# x_test_c = x_test[:, 20:45]\n",
        "# x_test_comm = x_test[:, 45:51]\n",
        "\n",
        "# print(x_test_v.shape)\n",
        "# print(x_test_c.shape)\n",
        "# print(x_test_comm.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(704800, 51)\n",
            "[[2]\n",
            " [2]\n",
            " [2]\n",
            " ...\n",
            " [3]\n",
            " [3]\n",
            " [3]]\n",
            "(493360, 20)\n",
            "(493360, 25)\n",
            "(493360, 6)\n",
            "(493360, 31)\n",
            "(493360, 1)\n",
            "(211440, 20)\n",
            "(211440, 25)\n",
            "(211440, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaTlsOtPPUUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # run multivariate linear regression to impute the missing calipso data\n",
        "# mlrs = []\n",
        "# for k in range(0, 25):\n",
        "#     print (k)\n",
        "#     # fit linear model for each kth feature of Calipos train data\n",
        "#     mlr_k = LinearRegression()\n",
        "#     mlr_k.fit(x_train_v, x_train_c[:, k]) \n",
        "#     print(mlr_k.intercept_)\n",
        "# #     print(mlr_k.coef_)\n",
        "#     mlrs.append(mlr_k)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jBO3IdqPYe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #defien the x_test_pt used for DL training\n",
        "# x_test_c_p = np.zeros(shape=(25, x_test_v.shape[0]))\n",
        "# print(x_test_c_p.shape)\n",
        "# # predict the kth feature of the Calipso data from the Viris test data, Assume the Calipso test data is not existed. \n",
        "# for k in range(0, 25):\n",
        "#     predi = mlrs[k].predict(x_test_v)\n",
        "# #     print(predi)\n",
        "#     x_test_c_p[k] = predi\n",
        "    \n",
        "# print(x_test_c_p.shape)\n",
        "\n",
        "# x_test_c_pt = x_test_c_p.T\n",
        "# print(x_test_v.shape)\n",
        "# print(x_test_c_pt.shape)\n",
        "\n",
        "# # #concanate common data\n",
        "# # X_v = np.concatenate((X_v, Latlon, Iff), axis=1)\n",
        "# # X_c = np.concatenate((X_c, Latlon, Iff), axis=1)\n",
        "# # print (X_v.shape)\n",
        "# # print (X_c.shape)\n",
        "\n",
        "# # x_test_pt = np.concatenate((x_test_v, x_test_c_pt, x_test_comm),axis=1)\n",
        "# x_test_pt = np.concatenate((x_test_c_pt, x_test_comm),axis=1)\n",
        "\n",
        "# print(x_test_pt.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBfGwo7aUiF1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee21209b-f7f6-4753-dcda-97db77e706ab"
      },
      "source": [
        "# define the sequential model with keras\n",
        "model_reg = Sequential()\n",
        "model_reg.add(Dense(256,input_shape=(x_train_v.shape[1],)))\n",
        "model_reg.add(Dense(256, activation= \"relu\"))\n",
        "model_reg.add(Dropout(0.5))\n",
        "model_reg.add(Dense(256, activation= \"relu\"))\n",
        "model_reg.add(Dropout(0.5))\n",
        "model_reg.add(Dense(128, activation= \"relu\"))\n",
        "model_reg.add(Dropout(0.5))\n",
        "model_reg.add(Dense(64, activation= \"relu\"))\n",
        "model_reg.add(Dropout(0.5))\n",
        "model_reg.add(Dense(x_train_c.shape[1], activation= \"linear\"))\n",
        "model_reg.summary()\n",
        "model_reg.compile(loss= \"mse\" , optimizer='adam', metrics=['mse','mae'])\n",
        "model_reg.fit(x_train_v, x_train_c, validation_data=(x_valid_v, x_valid_c), epochs=150, batch_size=128, verbose=1)\n",
        "\n",
        "#0.001\n",
        "#0.00001\n",
        "#batch size \n",
        "#last layer: relu, softmax\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_7 (Dense)              (None, 256)               5376      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 25)                1625      \n",
            "=================================================================\n",
            "Total params: 179,737\n",
            "Trainable params: 179,737\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 493360 samples, validate on 211440 samples\n",
            "Epoch 1/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.7346 - mse: 0.7346 - mae: 0.4565 - val_loss: 0.6990 - val_mse: 0.6990 - val_mae: 0.4020\n",
            "Epoch 2/150\n",
            "493360/493360 [==============================] - 41s 82us/step - loss: 0.6903 - mse: 0.6903 - mae: 0.4222 - val_loss: 0.6667 - val_mse: 0.6667 - val_mae: 0.3923\n",
            "Epoch 3/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6832 - mse: 0.6832 - mae: 0.4173 - val_loss: 0.6719 - val_mse: 0.6719 - val_mae: 0.3838\n",
            "Epoch 4/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6805 - mse: 0.6805 - mae: 0.4153 - val_loss: 0.6677 - val_mse: 0.6677 - val_mae: 0.3794\n",
            "Epoch 5/150\n",
            "493360/493360 [==============================] - 41s 82us/step - loss: 0.6775 - mse: 0.6775 - mae: 0.4133 - val_loss: 0.6595 - val_mse: 0.6595 - val_mae: 0.3725\n",
            "Epoch 6/150\n",
            "493360/493360 [==============================] - 40s 82us/step - loss: 0.6767 - mse: 0.6767 - mae: 0.4127 - val_loss: 0.6568 - val_mse: 0.6568 - val_mae: 0.3791\n",
            "Epoch 7/150\n",
            "493360/493360 [==============================] - 41s 82us/step - loss: 0.6749 - mse: 0.6749 - mae: 0.4117 - val_loss: 0.6566 - val_mse: 0.6566 - val_mae: 0.3861\n",
            "Epoch 8/150\n",
            "493360/493360 [==============================] - 41s 84us/step - loss: 0.6737 - mse: 0.6737 - mae: 0.4110 - val_loss: 0.6544 - val_mse: 0.6544 - val_mae: 0.3846\n",
            "Epoch 9/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6727 - mse: 0.6727 - mae: 0.4104 - val_loss: 0.6569 - val_mse: 0.6569 - val_mae: 0.3826\n",
            "Epoch 10/150\n",
            "493360/493360 [==============================] - 41s 82us/step - loss: 0.6723 - mse: 0.6723 - mae: 0.4102 - val_loss: 0.6559 - val_mse: 0.6559 - val_mae: 0.3626\n",
            "Epoch 11/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6711 - mse: 0.6711 - mae: 0.4095 - val_loss: 0.6556 - val_mse: 0.6556 - val_mae: 0.3771\n",
            "Epoch 12/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6704 - mse: 0.6704 - mae: 0.4090 - val_loss: 0.6535 - val_mse: 0.6535 - val_mae: 0.3786\n",
            "Epoch 13/150\n",
            "493360/493360 [==============================] - 41s 84us/step - loss: 0.6709 - mse: 0.6709 - mae: 0.4096 - val_loss: 0.6557 - val_mse: 0.6557 - val_mae: 0.3756\n",
            "Epoch 14/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6700 - mse: 0.6700 - mae: 0.4089 - val_loss: 0.6512 - val_mse: 0.6512 - val_mae: 0.3796\n",
            "Epoch 15/150\n",
            "493360/493360 [==============================] - 42s 84us/step - loss: 0.6699 - mse: 0.6699 - mae: 0.4090 - val_loss: 0.6526 - val_mse: 0.6526 - val_mae: 0.3773\n",
            "Epoch 16/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6692 - mse: 0.6692 - mae: 0.4083 - val_loss: 0.6548 - val_mse: 0.6548 - val_mae: 0.3684\n",
            "Epoch 17/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6692 - mse: 0.6692 - mae: 0.4086 - val_loss: 0.6523 - val_mse: 0.6523 - val_mae: 0.3840\n",
            "Epoch 18/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6689 - mse: 0.6689 - mae: 0.4083 - val_loss: 0.6501 - val_mse: 0.6501 - val_mae: 0.3788\n",
            "Epoch 19/150\n",
            "493360/493360 [==============================] - 39s 78us/step - loss: 0.6681 - mse: 0.6681 - mae: 0.4078 - val_loss: 0.6500 - val_mse: 0.6500 - val_mae: 0.3793\n",
            "Epoch 20/150\n",
            "493360/493360 [==============================] - 39s 78us/step - loss: 0.6681 - mse: 0.6681 - mae: 0.4079 - val_loss: 0.6502 - val_mse: 0.6502 - val_mae: 0.3855\n",
            "Epoch 21/150\n",
            "493360/493360 [==============================] - 37s 74us/step - loss: 0.6683 - mse: 0.6683 - mae: 0.4080 - val_loss: 0.6481 - val_mse: 0.6481 - val_mae: 0.3778\n",
            "Epoch 22/150\n",
            "493360/493360 [==============================] - 36s 74us/step - loss: 0.6685 - mse: 0.6685 - mae: 0.4080 - val_loss: 0.6457 - val_mse: 0.6457 - val_mae: 0.3696\n",
            "Epoch 23/150\n",
            "493360/493360 [==============================] - 38s 76us/step - loss: 0.6671 - mse: 0.6671 - mae: 0.4071 - val_loss: 0.6445 - val_mse: 0.6445 - val_mae: 0.3733\n",
            "Epoch 24/150\n",
            "493360/493360 [==============================] - 38s 76us/step - loss: 0.6677 - mse: 0.6677 - mae: 0.4076 - val_loss: 0.6485 - val_mse: 0.6485 - val_mae: 0.3756\n",
            "Epoch 25/150\n",
            "493360/493360 [==============================] - 37s 75us/step - loss: 0.6673 - mse: 0.6673 - mae: 0.4073 - val_loss: 0.6463 - val_mse: 0.6463 - val_mae: 0.3792\n",
            "Epoch 26/150\n",
            "493360/493360 [==============================] - 37s 75us/step - loss: 0.6672 - mse: 0.6672 - mae: 0.4072 - val_loss: 0.6450 - val_mse: 0.6450 - val_mae: 0.3695\n",
            "Epoch 27/150\n",
            "493360/493360 [==============================] - 37s 76us/step - loss: 0.6680 - mse: 0.6680 - mae: 0.4080 - val_loss: 0.6420 - val_mse: 0.6420 - val_mae: 0.3705\n",
            "Epoch 28/150\n",
            "493360/493360 [==============================] - 37s 76us/step - loss: 0.6686 - mse: 0.6686 - mae: 0.4082 - val_loss: 0.6471 - val_mse: 0.6471 - val_mae: 0.3755\n",
            "Epoch 29/150\n",
            "493360/493360 [==============================] - 38s 78us/step - loss: 0.6677 - mse: 0.6677 - mae: 0.4076 - val_loss: 0.6541 - val_mse: 0.6541 - val_mae: 0.3664\n",
            "Epoch 30/150\n",
            "493360/493360 [==============================] - 37s 75us/step - loss: 0.6678 - mse: 0.6678 - mae: 0.4079 - val_loss: 0.6535 - val_mse: 0.6535 - val_mae: 0.3849\n",
            "Epoch 31/150\n",
            "493360/493360 [==============================] - 38s 78us/step - loss: 0.6677 - mse: 0.6677 - mae: 0.4078 - val_loss: 0.6423 - val_mse: 0.6423 - val_mae: 0.3605\n",
            "Epoch 32/150\n",
            "493360/493360 [==============================] - 38s 77us/step - loss: 0.6677 - mse: 0.6677 - mae: 0.4078 - val_loss: 0.6457 - val_mse: 0.6457 - val_mae: 0.3723\n",
            "Epoch 33/150\n",
            "493360/493360 [==============================] - 38s 77us/step - loss: 0.6674 - mse: 0.6674 - mae: 0.4075 - val_loss: 0.6436 - val_mse: 0.6436 - val_mae: 0.3680\n",
            "Epoch 34/150\n",
            "493360/493360 [==============================] - 38s 77us/step - loss: 0.6677 - mse: 0.6677 - mae: 0.4077 - val_loss: 0.6470 - val_mse: 0.6470 - val_mae: 0.3806\n",
            "Epoch 35/150\n",
            "493360/493360 [==============================] - 37s 75us/step - loss: 0.6671 - mse: 0.6671 - mae: 0.4071 - val_loss: 0.6471 - val_mse: 0.6471 - val_mae: 0.3828\n",
            "Epoch 36/150\n",
            "493360/493360 [==============================] - 38s 77us/step - loss: 0.6672 - mse: 0.6672 - mae: 0.4073 - val_loss: 0.6447 - val_mse: 0.6447 - val_mae: 0.3705\n",
            "Epoch 37/150\n",
            "493360/493360 [==============================] - 39s 78us/step - loss: 0.6677 - mse: 0.6677 - mae: 0.4077 - val_loss: 0.6505 - val_mse: 0.6505 - val_mae: 0.3746\n",
            "Epoch 38/150\n",
            "493360/493360 [==============================] - 40s 81us/step - loss: 0.6666 - mse: 0.6666 - mae: 0.4071 - val_loss: 0.6447 - val_mse: 0.6447 - val_mae: 0.3660\n",
            "Epoch 39/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6673 - mse: 0.6673 - mae: 0.4075 - val_loss: 0.6483 - val_mse: 0.6483 - val_mae: 0.3704\n",
            "Epoch 40/150\n",
            "493360/493360 [==============================] - 42s 85us/step - loss: 0.6667 - mse: 0.6667 - mae: 0.4072 - val_loss: 0.6545 - val_mse: 0.6545 - val_mae: 0.3668\n",
            "Epoch 41/150\n",
            "493360/493360 [==============================] - 41s 84us/step - loss: 0.6674 - mse: 0.6674 - mae: 0.4077 - val_loss: 0.6461 - val_mse: 0.6461 - val_mae: 0.3720\n",
            "Epoch 42/150\n",
            "493360/493360 [==============================] - 41s 84us/step - loss: 0.6673 - mse: 0.6673 - mae: 0.4077 - val_loss: 0.6454 - val_mse: 0.6454 - val_mae: 0.3701\n",
            "Epoch 43/150\n",
            "493360/493360 [==============================] - 41s 84us/step - loss: 0.6672 - mse: 0.6672 - mae: 0.4075 - val_loss: 0.6460 - val_mse: 0.6460 - val_mae: 0.3788\n",
            "Epoch 44/150\n",
            "493360/493360 [==============================] - 43s 87us/step - loss: 0.6679 - mse: 0.6679 - mae: 0.4078 - val_loss: 0.6474 - val_mse: 0.6474 - val_mae: 0.3728\n",
            "Epoch 45/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6677 - mse: 0.6677 - mae: 0.4080 - val_loss: 0.6406 - val_mse: 0.6406 - val_mae: 0.3706\n",
            "Epoch 46/150\n",
            "493360/493360 [==============================] - 41s 84us/step - loss: 0.6672 - mse: 0.6672 - mae: 0.4075 - val_loss: 0.6445 - val_mse: 0.6445 - val_mae: 0.3712\n",
            "Epoch 47/150\n",
            "493360/493360 [==============================] - 42s 85us/step - loss: 0.6677 - mse: 0.6677 - mae: 0.4080 - val_loss: 0.6481 - val_mse: 0.6481 - val_mae: 0.3718\n",
            "Epoch 48/150\n",
            "493360/493360 [==============================] - 41s 84us/step - loss: 0.6682 - mse: 0.6682 - mae: 0.4082 - val_loss: 0.6572 - val_mse: 0.6572 - val_mae: 0.3691\n",
            "Epoch 49/150\n",
            "493360/493360 [==============================] - 41s 82us/step - loss: 0.6673 - mse: 0.6673 - mae: 0.4078 - val_loss: 0.6575 - val_mse: 0.6575 - val_mae: 0.3713\n",
            "Epoch 50/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6698 - mse: 0.6698 - mae: 0.4091 - val_loss: 0.6468 - val_mse: 0.6468 - val_mae: 0.3716\n",
            "Epoch 51/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6678 - mse: 0.6678 - mae: 0.4079 - val_loss: 0.6500 - val_mse: 0.6500 - val_mae: 0.3756\n",
            "Epoch 52/150\n",
            "493360/493360 [==============================] - 40s 82us/step - loss: 0.6680 - mse: 0.6680 - mae: 0.4082 - val_loss: 0.6510 - val_mse: 0.6510 - val_mae: 0.3789\n",
            "Epoch 53/150\n",
            "493360/493360 [==============================] - 40s 81us/step - loss: 0.6689 - mse: 0.6689 - mae: 0.4089 - val_loss: 0.6525 - val_mse: 0.6525 - val_mae: 0.3784\n",
            "Epoch 54/150\n",
            "493360/493360 [==============================] - 40s 81us/step - loss: 0.6684 - mse: 0.6684 - mae: 0.4082 - val_loss: 0.6464 - val_mse: 0.6464 - val_mae: 0.3727\n",
            "Epoch 55/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6679 - mse: 0.6679 - mae: 0.4080 - val_loss: 0.6441 - val_mse: 0.6441 - val_mae: 0.3760\n",
            "Epoch 56/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6685 - mse: 0.6685 - mae: 0.4084 - val_loss: 0.6488 - val_mse: 0.6488 - val_mae: 0.3830\n",
            "Epoch 57/150\n",
            "493360/493360 [==============================] - 41s 84us/step - loss: 0.6679 - mse: 0.6679 - mae: 0.4080 - val_loss: 0.6455 - val_mse: 0.6455 - val_mae: 0.3703\n",
            "Epoch 58/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6675 - mse: 0.6675 - mae: 0.4079 - val_loss: 0.6438 - val_mse: 0.6438 - val_mae: 0.3658\n",
            "Epoch 59/150\n",
            "493360/493360 [==============================] - 43s 87us/step - loss: 0.6689 - mse: 0.6689 - mae: 0.4088 - val_loss: 0.6516 - val_mse: 0.6516 - val_mae: 0.3768\n",
            "Epoch 60/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6714 - mse: 0.6714 - mae: 0.4103 - val_loss: 0.6470 - val_mse: 0.6470 - val_mae: 0.3678\n",
            "Epoch 61/150\n",
            "493360/493360 [==============================] - 40s 80us/step - loss: 0.6692 - mse: 0.6692 - mae: 0.4088 - val_loss: 0.6465 - val_mse: 0.6465 - val_mae: 0.3705\n",
            "Epoch 62/150\n",
            "493360/493360 [==============================] - 40s 81us/step - loss: 0.6692 - mse: 0.6692 - mae: 0.4088 - val_loss: 0.6476 - val_mse: 0.6476 - val_mae: 0.3786\n",
            "Epoch 63/150\n",
            "493360/493360 [==============================] - 40s 80us/step - loss: 0.6693 - mse: 0.6693 - mae: 0.4089 - val_loss: 0.6501 - val_mse: 0.6501 - val_mae: 0.3750\n",
            "Epoch 64/150\n",
            "493360/493360 [==============================] - 40s 82us/step - loss: 0.6702 - mse: 0.6702 - mae: 0.4094 - val_loss: 0.6456 - val_mse: 0.6456 - val_mae: 0.3743\n",
            "Epoch 65/150\n",
            "493360/493360 [==============================] - 39s 80us/step - loss: 0.6692 - mse: 0.6692 - mae: 0.4092 - val_loss: 0.6521 - val_mse: 0.6521 - val_mae: 0.3765\n",
            "Epoch 66/150\n",
            "493360/493360 [==============================] - 38s 77us/step - loss: 0.6701 - mse: 0.6701 - mae: 0.4096 - val_loss: 0.6460 - val_mse: 0.6460 - val_mae: 0.3690\n",
            "Epoch 67/150\n",
            "493360/493360 [==============================] - 38s 76us/step - loss: 0.6685 - mse: 0.6685 - mae: 0.4085 - val_loss: 0.6489 - val_mse: 0.6489 - val_mae: 0.3730\n",
            "Epoch 68/150\n",
            "493360/493360 [==============================] - 37s 75us/step - loss: 0.6700 - mse: 0.6700 - mae: 0.4093 - val_loss: 0.6741 - val_mse: 0.6741 - val_mae: 0.3920\n",
            "Epoch 69/150\n",
            "493360/493360 [==============================] - 38s 77us/step - loss: 0.6698 - mse: 0.6698 - mae: 0.4094 - val_loss: 0.6469 - val_mse: 0.6469 - val_mae: 0.3771\n",
            "Epoch 70/150\n",
            "493360/493360 [==============================] - 38s 78us/step - loss: 0.6708 - mse: 0.6708 - mae: 0.4103 - val_loss: 0.6487 - val_mse: 0.6487 - val_mae: 0.3701\n",
            "Epoch 71/150\n",
            "493360/493360 [==============================] - 37s 76us/step - loss: 0.6695 - mse: 0.6695 - mae: 0.4092 - val_loss: 0.6433 - val_mse: 0.6433 - val_mae: 0.3755\n",
            "Epoch 72/150\n",
            "493360/493360 [==============================] - 38s 76us/step - loss: 0.6704 - mse: 0.6704 - mae: 0.4099 - val_loss: 0.6537 - val_mse: 0.6537 - val_mae: 0.3827\n",
            "Epoch 73/150\n",
            "493360/493360 [==============================] - 38s 76us/step - loss: 0.6696 - mse: 0.6696 - mae: 0.4093 - val_loss: 0.6555 - val_mse: 0.6555 - val_mae: 0.3837\n",
            "Epoch 74/150\n",
            "493360/493360 [==============================] - 37s 75us/step - loss: 0.6696 - mse: 0.6696 - mae: 0.4092 - val_loss: 0.6456 - val_mse: 0.6456 - val_mae: 0.3733\n",
            "Epoch 75/150\n",
            "493360/493360 [==============================] - 38s 77us/step - loss: 0.6697 - mse: 0.6697 - mae: 0.4094 - val_loss: 0.6503 - val_mse: 0.6503 - val_mae: 0.3727\n",
            "Epoch 76/150\n",
            "493360/493360 [==============================] - 37s 75us/step - loss: 0.6736 - mse: 0.6736 - mae: 0.4120 - val_loss: 0.6452 - val_mse: 0.6452 - val_mae: 0.3671\n",
            "Epoch 77/150\n",
            "493360/493360 [==============================] - 37s 75us/step - loss: 0.6706 - mse: 0.6706 - mae: 0.4101 - val_loss: 0.6470 - val_mse: 0.6470 - val_mae: 0.3755\n",
            "Epoch 78/150\n",
            "493360/493360 [==============================] - 37s 75us/step - loss: 0.6696 - mse: 0.6696 - mae: 0.4092 - val_loss: 0.6462 - val_mse: 0.6462 - val_mae: 0.3727\n",
            "Epoch 79/150\n",
            "493360/493360 [==============================] - 37s 75us/step - loss: 0.6699 - mse: 0.6699 - mae: 0.4095 - val_loss: 0.6453 - val_mse: 0.6453 - val_mae: 0.3728\n",
            "Epoch 80/150\n",
            "493360/493360 [==============================] - 37s 75us/step - loss: 0.6703 - mse: 0.6703 - mae: 0.4098 - val_loss: 0.6515 - val_mse: 0.6515 - val_mae: 0.3800\n",
            "Epoch 81/150\n",
            "493360/493360 [==============================] - 37s 75us/step - loss: 0.6726 - mse: 0.6726 - mae: 0.4113 - val_loss: 0.6511 - val_mse: 0.6511 - val_mae: 0.3667\n",
            "Epoch 82/150\n",
            "493360/493360 [==============================] - 37s 74us/step - loss: 0.6705 - mse: 0.6705 - mae: 0.4098 - val_loss: 0.6805 - val_mse: 0.6805 - val_mae: 0.3970\n",
            "Epoch 83/150\n",
            "493360/493360 [==============================] - 37s 75us/step - loss: 0.6714 - mse: 0.6714 - mae: 0.4104 - val_loss: 0.6511 - val_mse: 0.6511 - val_mae: 0.3776\n",
            "Epoch 84/150\n",
            "493360/493360 [==============================] - 37s 75us/step - loss: 0.6723 - mse: 0.6722 - mae: 0.4110 - val_loss: 0.6501 - val_mse: 0.6501 - val_mae: 0.3737\n",
            "Epoch 85/150\n",
            "493360/493360 [==============================] - 38s 76us/step - loss: 0.6710 - mse: 0.6710 - mae: 0.4101 - val_loss: 0.6476 - val_mse: 0.6476 - val_mae: 0.3680\n",
            "Epoch 86/150\n",
            "493360/493360 [==============================] - 37s 76us/step - loss: 0.6709 - mse: 0.6709 - mae: 0.4102 - val_loss: 0.6508 - val_mse: 0.6508 - val_mae: 0.3714\n",
            "Epoch 87/150\n",
            "493360/493360 [==============================] - 37s 76us/step - loss: 0.6719 - mse: 0.6719 - mae: 0.4106 - val_loss: 0.6517 - val_mse: 0.6517 - val_mae: 0.3778\n",
            "Epoch 88/150\n",
            "493360/493360 [==============================] - 37s 75us/step - loss: 0.6740 - mse: 0.6740 - mae: 0.4121 - val_loss: 0.6512 - val_mse: 0.6512 - val_mae: 0.3724\n",
            "Epoch 89/150\n",
            "493360/493360 [==============================] - 38s 77us/step - loss: 0.6734 - mse: 0.6734 - mae: 0.4116 - val_loss: 0.6500 - val_mse: 0.6500 - val_mae: 0.3714\n",
            "Epoch 90/150\n",
            "493360/493360 [==============================] - 38s 77us/step - loss: 0.6740 - mse: 0.6740 - mae: 0.4122 - val_loss: 0.6466 - val_mse: 0.6466 - val_mae: 0.3734\n",
            "Epoch 91/150\n",
            "493360/493360 [==============================] - 39s 80us/step - loss: 0.6731 - mse: 0.6731 - mae: 0.4116 - val_loss: 0.6528 - val_mse: 0.6528 - val_mae: 0.3794\n",
            "Epoch 92/150\n",
            "493360/493360 [==============================] - 37s 75us/step - loss: 0.6739 - mse: 0.6739 - mae: 0.4119 - val_loss: 0.6587 - val_mse: 0.6587 - val_mae: 0.3792\n",
            "Epoch 93/150\n",
            "493360/493360 [==============================] - 38s 76us/step - loss: 0.6719 - mse: 0.6719 - mae: 0.4109 - val_loss: 0.6531 - val_mse: 0.6531 - val_mae: 0.3816\n",
            "Epoch 94/150\n",
            "493360/493360 [==============================] - 38s 77us/step - loss: 0.6731 - mse: 0.6731 - mae: 0.4117 - val_loss: 0.6552 - val_mse: 0.6552 - val_mae: 0.3769\n",
            "Epoch 95/150\n",
            "493360/493360 [==============================] - 38s 78us/step - loss: 0.6726 - mse: 0.6726 - mae: 0.4112 - val_loss: 0.6504 - val_mse: 0.6504 - val_mae: 0.3686\n",
            "Epoch 96/150\n",
            "493360/493360 [==============================] - 38s 77us/step - loss: 0.6726 - mse: 0.6726 - mae: 0.4111 - val_loss: 0.6489 - val_mse: 0.6489 - val_mae: 0.3741\n",
            "Epoch 97/150\n",
            "493360/493360 [==============================] - 39s 78us/step - loss: 0.6748 - mse: 0.6748 - mae: 0.4128 - val_loss: 0.6530 - val_mse: 0.6530 - val_mae: 0.3751\n",
            "Epoch 98/150\n",
            "493360/493360 [==============================] - 38s 77us/step - loss: 0.6740 - mse: 0.6740 - mae: 0.4122 - val_loss: 0.6516 - val_mse: 0.6516 - val_mae: 0.3772\n",
            "Epoch 99/150\n",
            "493360/493360 [==============================] - 38s 77us/step - loss: 0.6769 - mse: 0.6769 - mae: 0.4142 - val_loss: 0.6498 - val_mse: 0.6498 - val_mae: 0.3856\n",
            "Epoch 100/150\n",
            "493360/493360 [==============================] - 39s 79us/step - loss: 0.6744 - mse: 0.6744 - mae: 0.4127 - val_loss: 0.6486 - val_mse: 0.6486 - val_mae: 0.3674\n",
            "Epoch 101/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6738 - mse: 0.6738 - mae: 0.4119 - val_loss: 0.6478 - val_mse: 0.6478 - val_mae: 0.3705\n",
            "Epoch 102/150\n",
            "493360/493360 [==============================] - 41s 84us/step - loss: 0.6733 - mse: 0.6733 - mae: 0.4118 - val_loss: 0.6508 - val_mse: 0.6508 - val_mae: 0.3794\n",
            "Epoch 103/150\n",
            "493360/493360 [==============================] - 40s 82us/step - loss: 0.6745 - mse: 0.6745 - mae: 0.4124 - val_loss: 0.6496 - val_mse: 0.6496 - val_mae: 0.3704\n",
            "Epoch 104/150\n",
            "493360/493360 [==============================] - 41s 84us/step - loss: 0.6761 - mse: 0.6761 - mae: 0.4135 - val_loss: 0.6557 - val_mse: 0.6557 - val_mae: 0.3850\n",
            "Epoch 105/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6744 - mse: 0.6744 - mae: 0.4122 - val_loss: 0.6504 - val_mse: 0.6504 - val_mae: 0.3761\n",
            "Epoch 106/150\n",
            "493360/493360 [==============================] - 40s 81us/step - loss: 0.6750 - mse: 0.6750 - mae: 0.4129 - val_loss: 0.6509 - val_mse: 0.6509 - val_mae: 0.3783\n",
            "Epoch 107/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6742 - mse: 0.6742 - mae: 0.4121 - val_loss: 0.6564 - val_mse: 0.6564 - val_mae: 0.3801\n",
            "Epoch 108/150\n",
            "493360/493360 [==============================] - 39s 79us/step - loss: 0.6757 - mse: 0.6757 - mae: 0.4130 - val_loss: 0.6529 - val_mse: 0.6529 - val_mae: 0.3783\n",
            "Epoch 109/150\n",
            "493360/493360 [==============================] - 39s 79us/step - loss: 0.6767 - mse: 0.6767 - mae: 0.4137 - val_loss: 0.6535 - val_mse: 0.6535 - val_mae: 0.3786\n",
            "Epoch 110/150\n",
            "493360/493360 [==============================] - 38s 78us/step - loss: 0.6775 - mse: 0.6775 - mae: 0.4140 - val_loss: 0.6637 - val_mse: 0.6637 - val_mae: 0.3904\n",
            "Epoch 111/150\n",
            "493360/493360 [==============================] - 43s 86us/step - loss: 0.6762 - mse: 0.6762 - mae: 0.4135 - val_loss: 0.6612 - val_mse: 0.6612 - val_mae: 0.3887\n",
            "Epoch 112/150\n",
            "493360/493360 [==============================] - 42s 85us/step - loss: 0.6760 - mse: 0.6760 - mae: 0.4138 - val_loss: 0.6522 - val_mse: 0.6522 - val_mae: 0.3821\n",
            "Epoch 113/150\n",
            "493360/493360 [==============================] - 40s 82us/step - loss: 0.6763 - mse: 0.6763 - mae: 0.4138 - val_loss: 0.6565 - val_mse: 0.6565 - val_mae: 0.3809\n",
            "Epoch 114/150\n",
            "493360/493360 [==============================] - 43s 87us/step - loss: 0.6757 - mse: 0.6757 - mae: 0.4134 - val_loss: 0.6554 - val_mse: 0.6554 - val_mae: 0.3773\n",
            "Epoch 115/150\n",
            "493360/493360 [==============================] - 40s 82us/step - loss: 0.6754 - mse: 0.6754 - mae: 0.4134 - val_loss: 0.6476 - val_mse: 0.6476 - val_mae: 0.3704\n",
            "Epoch 116/150\n",
            "493360/493360 [==============================] - 42s 84us/step - loss: 0.6782 - mse: 0.6782 - mae: 0.4149 - val_loss: 0.6555 - val_mse: 0.6555 - val_mae: 0.3856\n",
            "Epoch 117/150\n",
            "493360/493360 [==============================] - 42s 85us/step - loss: 0.6778 - mse: 0.6778 - mae: 0.4149 - val_loss: 0.6543 - val_mse: 0.6543 - val_mae: 0.3779\n",
            "Epoch 118/150\n",
            "493360/493360 [==============================] - 42s 85us/step - loss: 0.6778 - mse: 0.6778 - mae: 0.4147 - val_loss: 0.6568 - val_mse: 0.6568 - val_mae: 0.3839\n",
            "Epoch 119/150\n",
            "493360/493360 [==============================] - 41s 84us/step - loss: 0.6795 - mse: 0.6795 - mae: 0.4158 - val_loss: 0.6546 - val_mse: 0.6546 - val_mae: 0.3783\n",
            "Epoch 120/150\n",
            "493360/493360 [==============================] - 39s 79us/step - loss: 0.6780 - mse: 0.6780 - mae: 0.4147 - val_loss: 0.6592 - val_mse: 0.6592 - val_mae: 0.3777\n",
            "Epoch 121/150\n",
            "493360/493360 [==============================] - 42s 86us/step - loss: 0.6762 - mse: 0.6762 - mae: 0.4134 - val_loss: 0.6537 - val_mse: 0.6537 - val_mae: 0.3859\n",
            "Epoch 122/150\n",
            "493360/493360 [==============================] - 44s 89us/step - loss: 0.6790 - mse: 0.6790 - mae: 0.4154 - val_loss: 0.6516 - val_mse: 0.6516 - val_mae: 0.3748\n",
            "Epoch 123/150\n",
            "493360/493360 [==============================] - 42s 85us/step - loss: 0.6774 - mse: 0.6774 - mae: 0.4145 - val_loss: 0.7127 - val_mse: 0.7127 - val_mae: 0.4324\n",
            "Epoch 124/150\n",
            "493360/493360 [==============================] - 43s 87us/step - loss: 0.6794 - mse: 0.6794 - mae: 0.4157 - val_loss: 0.6531 - val_mse: 0.6531 - val_mae: 0.3714\n",
            "Epoch 125/150\n",
            "493360/493360 [==============================] - 40s 82us/step - loss: 0.6807 - mse: 0.6807 - mae: 0.4164 - val_loss: 0.6596 - val_mse: 0.6596 - val_mae: 0.3816\n",
            "Epoch 126/150\n",
            "493360/493360 [==============================] - 41s 84us/step - loss: 0.6820 - mse: 0.6820 - mae: 0.4175 - val_loss: 0.6558 - val_mse: 0.6558 - val_mae: 0.3750\n",
            "Epoch 127/150\n",
            "493360/493360 [==============================] - 43s 86us/step - loss: 0.6782 - mse: 0.6782 - mae: 0.4151 - val_loss: 0.6570 - val_mse: 0.6570 - val_mae: 0.3784\n",
            "Epoch 128/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6793 - mse: 0.6793 - mae: 0.4156 - val_loss: 0.6582 - val_mse: 0.6582 - val_mae: 0.3888\n",
            "Epoch 129/150\n",
            "493360/493360 [==============================] - 42s 86us/step - loss: 0.6833 - mse: 0.6833 - mae: 0.4183 - val_loss: 0.6525 - val_mse: 0.6525 - val_mae: 0.3807\n",
            "Epoch 130/150\n",
            "493360/493360 [==============================] - 41s 83us/step - loss: 0.6802 - mse: 0.6802 - mae: 0.4162 - val_loss: 0.6509 - val_mse: 0.6509 - val_mae: 0.3832\n",
            "Epoch 131/150\n",
            "493360/493360 [==============================] - 42s 84us/step - loss: 0.6799 - mse: 0.6799 - mae: 0.4159 - val_loss: 0.7562 - val_mse: 0.7562 - val_mae: 0.4470\n",
            "Epoch 132/150\n",
            "493360/493360 [==============================] - 43s 86us/step - loss: 0.6861 - mse: 0.6861 - mae: 0.4203 - val_loss: 0.6598 - val_mse: 0.6598 - val_mae: 0.3909\n",
            "Epoch 133/150\n",
            "493360/493360 [==============================] - 42s 85us/step - loss: 0.6805 - mse: 0.6805 - mae: 0.4163 - val_loss: 0.6537 - val_mse: 0.6537 - val_mae: 0.3828\n",
            "Epoch 134/150\n",
            "493360/493360 [==============================] - 42s 85us/step - loss: 0.6803 - mse: 0.6803 - mae: 0.4164 - val_loss: 0.6610 - val_mse: 0.6610 - val_mae: 0.3777\n",
            "Epoch 135/150\n",
            "493360/493360 [==============================] - 41s 82us/step - loss: 0.6818 - mse: 0.6818 - mae: 0.4172 - val_loss: 0.6685 - val_mse: 0.6685 - val_mae: 0.3842\n",
            "Epoch 136/150\n",
            "493360/493360 [==============================] - 42s 85us/step - loss: 0.6828 - mse: 0.6828 - mae: 0.4179 - val_loss: 0.6521 - val_mse: 0.6521 - val_mae: 0.3781\n",
            "Epoch 137/150\n",
            "493360/493360 [==============================] - 42s 85us/step - loss: 0.6849 - mse: 0.6849 - mae: 0.4192 - val_loss: 0.6621 - val_mse: 0.6621 - val_mae: 0.3899\n",
            "Epoch 138/150\n",
            "493360/493360 [==============================] - 42s 85us/step - loss: 0.6833 - mse: 0.6833 - mae: 0.4183 - val_loss: 0.6576 - val_mse: 0.6576 - val_mae: 0.3759\n",
            "Epoch 139/150\n",
            "493360/493360 [==============================] - 44s 90us/step - loss: 0.6821 - mse: 0.6821 - mae: 0.4173 - val_loss: 0.6625 - val_mse: 0.6625 - val_mae: 0.3930\n",
            "Epoch 140/150\n",
            "493360/493360 [==============================] - 44s 90us/step - loss: 0.6856 - mse: 0.6856 - mae: 0.4198 - val_loss: 0.6623 - val_mse: 0.6623 - val_mae: 0.3863\n",
            "Epoch 141/150\n",
            "493360/493360 [==============================] - 43s 88us/step - loss: 0.6879 - mse: 0.6879 - mae: 0.4215 - val_loss: 0.6693 - val_mse: 0.6693 - val_mae: 0.3940\n",
            "Epoch 142/150\n",
            "493360/493360 [==============================] - 43s 88us/step - loss: 0.6845 - mse: 0.6845 - mae: 0.4189 - val_loss: 0.6592 - val_mse: 0.6592 - val_mae: 0.3862\n",
            "Epoch 143/150\n",
            "493360/493360 [==============================] - 42s 85us/step - loss: 0.6830 - mse: 0.6830 - mae: 0.4179 - val_loss: 0.6592 - val_mse: 0.6592 - val_mae: 0.3863\n",
            "Epoch 144/150\n",
            "493360/493360 [==============================] - 42s 85us/step - loss: 0.6813 - mse: 0.6813 - mae: 0.4170 - val_loss: 0.6565 - val_mse: 0.6565 - val_mae: 0.3777\n",
            "Epoch 145/150\n",
            "493360/493360 [==============================] - 44s 88us/step - loss: 0.6816 - mse: 0.6816 - mae: 0.4171 - val_loss: 0.6574 - val_mse: 0.6574 - val_mae: 0.3818\n",
            "Epoch 146/150\n",
            "493360/493360 [==============================] - 42s 86us/step - loss: 0.6852 - mse: 0.6852 - mae: 0.4199 - val_loss: 0.6606 - val_mse: 0.6606 - val_mae: 0.3869\n",
            "Epoch 147/150\n",
            "493360/493360 [==============================] - 42s 84us/step - loss: 0.6856 - mse: 0.6856 - mae: 0.4197 - val_loss: 0.6655 - val_mse: 0.6655 - val_mae: 0.3846\n",
            "Epoch 148/150\n",
            "493360/493360 [==============================] - 42s 85us/step - loss: 0.6836 - mse: 0.6836 - mae: 0.4183 - val_loss: 0.6677 - val_mse: 0.6677 - val_mae: 0.3908\n",
            "Epoch 149/150\n",
            "493360/493360 [==============================] - 40s 82us/step - loss: 0.6853 - mse: 0.6853 - mae: 0.4194 - val_loss: 0.7102 - val_mse: 0.7102 - val_mae: 0.4361\n",
            "Epoch 150/150\n",
            "493360/493360 [==============================] - 41s 84us/step - loss: 0.6901 - mse: 0.6901 - mae: 0.4228 - val_loss: 0.6595 - val_mse: 0.6595 - val_mae: 0.3786\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f6aacab1c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT2iV-MHXVLw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "53327aaf-d0e7-4b5c-fe60-e142db6838c9"
      },
      "source": [
        "x_train_c_pt = model_reg.predict(x_train_v)\n",
        "# x_test_c_pt = model_reg.predict(x_test_v)\n",
        "x_valid_c_pt = model_reg.predict(x_valid_v)\n",
        "print(x_train_c_pt.shape)\n",
        "# print(x_test_c_pt.shape)\n",
        "print(x_valid_c_pt.shape)\n",
        "\n",
        "# DLR imputed target domain\n",
        "x_train_pt = np.concatenate((x_train_c_pt, x_train_comm),axis=1)\n",
        "\n",
        "print(x_train_pt.shape)\n",
        "\n",
        "# x_test_pt = np.concatenate((x_test_c_pt, x_test_comm),axis=1)\n",
        "\n",
        "# print(x_test_pt.shape)\n",
        "\n",
        "x_valid_pt = np.concatenate((x_valid_c_pt, x_valid_comm),axis=1)\n",
        "\n",
        "print(x_valid_pt.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(493360, 25)\n",
            "(211440, 25)\n",
            "(493360, 31)\n",
            "(211440, 31)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgtTsUZ1PhQB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "bd868e1f-7168-4208-dd69-cee86d94b933"
      },
      "source": [
        "# use Logistic regression model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
        "                         multi_class='multinomial').fit(x_train_pt, y_train)\n",
        "y_pred = clf.predict(x_valid_pt)\n",
        "print (clf.score(x_valid_pt, y_valid))\n",
        "\n",
        "num_classes1 = 6\n",
        "y_lr_gd = keras.utils.to_categorical(y_valid-1, num_classes1)\n",
        "y_lr_pred = keras.utils.to_categorical(y_pred-1, num_classes1)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "roc_auc_score(y_lr_gd, y_lr_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.7056280741581537\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7953723707855836"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezrco0x5PkI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # use deep learning model\n",
        "# num_classes = 6\n",
        "\n",
        "# print ('y_train')\n",
        "# print (y_train)\n",
        "# # convert class vectors to binary class matrices\n",
        "# y_train = keras.utils.to_categorical(y_train-1, num_classes)\n",
        "# y_valid = keras.utils.to_categorical(y_valid-1, num_classes)\n",
        "# y_test = keras.utils.to_categorical(y_test-1, num_classes)\n",
        "\n",
        "# print ('y_train converted')\n",
        "# print (y_train)\n",
        "\n",
        "# batch_size = 1024\n",
        "# epochs = 50\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Dense(128, activation='relu', input_shape=(x_train_pt.shape[1],)))\n",
        "# model.add(Dropout(0.4))\n",
        "# model.add(Dense(512, activation='relu'))\n",
        "# model.add(Dropout(0.4))\n",
        "# model.add(Dense(256, activation='relu'))\n",
        "# model.add(Dropout(0.4))\n",
        "# model.add(Dense(128, activation='relu'))\n",
        "# model.add(Dropout(0.4))\n",
        "# model.add(Dense(64, activation='relu'))\n",
        "# model.add(Dropout(0.4))\n",
        "# model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "# model.compile(loss='categorical_crossentropy',\n",
        "#               optimizer=RMSprop(lr=0.001),\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# history = model.fit(x_train_pt, y_train,\n",
        "#                     batch_size=batch_size,\n",
        "#                     epochs=epochs,\n",
        "#                     verbose=1,\n",
        "#                     validation_data=(x_valid_pt, y_valid))\n",
        "# score = model.evaluate(x_test_pt, y_test, verbose=0)\n",
        "# print('Test loss:', score[0])\n",
        "# print('Test accuracy:', score[1])\n",
        "# print('Test score:', score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQvvmIUhQTFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# score = model.evaluate(x_test_pt, y_test, verbose=0)\n",
        "# print('Test loss:', score[0])\n",
        "# print('Test accuracy:', score[1])\n",
        "# print('Test score:', score)\n",
        "# predict_result = model.predict_proba(x_test_pt);\n",
        "# print (predict_result)\n",
        "# from sklearn.metrics import roc_auc_score\n",
        "# roc_auc_score(y_test, predict_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMSGjBnV2e8z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "3ef6425c-5a4b-436f-91b8-98728fb50250"
      },
      "source": [
        "data_test = np.load('/content/content/My Drive/Colab Notebooks/kddworkshop/fulldata/test_138_day.npz')\n",
        "\n",
        "passive =1\n",
        "\n",
        "#load common data\n",
        "latlon_test = data_test['latlon']\n",
        "iff_test = data_test['iff']\n",
        "\n",
        "# if passive ==1:\n",
        "x_t_test = data_test['viirs']\n",
        "y_t_test = data_test['label']\n",
        "# else:\n",
        "x_s_test = data_test['calipso']\n",
        "y_s_test = data_test['label']\n",
        "    \n",
        "inds_test,vals_test = np.where(y_t_test>0)\n",
        "\n",
        "# process common data\n",
        "Latlon_test = latlon_test[inds_test]\n",
        "Iff_test = iff_test[inds_test]\n",
        "\n",
        "Y_t_test = y_t_test[inds_test]\n",
        "X_t_test = x_t_test[inds_test]\n",
        "\n",
        "Y_s_test = y_s_test[inds_test]\n",
        "X_s_test = x_s_test[inds_test]\n",
        "\n",
        "# 0 =< SZA <= 83\n",
        "print('original X_t_test: ', X_t_test.shape)\n",
        "rows_test = np.where((X_t_test[:,0] >= 0) & (X_t_test[:,0] <= 83) & (X_t_test[:,15] > 100) & (X_t_test[:,15] < 400) & (X_t_test[:,16] > 100) & (X_t_test[:,16] < 400) & (X_t_test[:,17] > 100) & (X_t_test[:,17] < 400) & (X_t_test[:,18] > 100) & (X_t_test[:,18] < 400) & (X_t_test[:,19] > 100) & (X_t_test[:,19] < 400) & (X_t_test[:,10] > 0))\n",
        "print(\"rows_test:\", rows_test)\n",
        "print(\"rows_test.shape:\", len(rows_test))\n",
        "\n",
        "Latlon_test = Latlon_test[rows_test]\n",
        "Iff_test = Iff_test[rows_test]\n",
        "\n",
        "Y_t_test = Y_t_test[rows_test]\n",
        "X_t_test = X_t_test[rows_test]\n",
        "\n",
        "Y_s_test = Y_s_test[rows_test]\n",
        "X_s_test = X_s_test[rows_test]\n",
        "\n",
        "X_s_test = np.nan_to_num(X_s_test)\n",
        "X_t_test = np.nan_to_num(X_t_test)\n",
        "\n",
        "print('after SZA X_t_test: ', X_t_test.shape)\n",
        "print('after SZA X_s_test: ', X_s_test.shape)\n",
        "\n",
        "# pca = decomposition.PCA(n_components=20)\n",
        "# pca.fit(X_s_test)\n",
        "# X_s_test = pca.transform(X_s_test)\n",
        "# print (X_s_test.shape)\n",
        "\n",
        "#concanate common data\n",
        "# X_t_test = np.concatenate((X_t_test, Latlon_test, Iff_test), axis=1)\n",
        "X_s_test = np.concatenate((X_s_test, Latlon_test, Iff_test), axis=1)\n",
        "\n",
        "print (X_s_test.shape)\n",
        "print (X_t_test.shape)\n",
        "\n",
        "X_test=np.concatenate((X_t_test, X_s_test), axis=1)\n",
        "\n",
        "# scaler_t = StandardScaler()\n",
        "# scaler_t.fit(X_t_test)\n",
        "# X_t_test = scaler_t.transform(X_t_test)\n",
        "\n",
        "# scaler_s = StandardScaler()\n",
        "# scaler_s.fit(X_s_test)\n",
        "# X_s_test= scaler_s.transform(X_s_test)\n",
        "\n",
        "x_test2=sc_X.transform(X_test)\n",
        "\n",
        "# x_train_v = x_train[:, 0:20]\n",
        "# x_train_c = x_train[:, 20:45]\n",
        "# x_train_comm = x_train[:, 45:51]\n",
        "# print(x_train_v.shape)\n",
        "# print(x_train_c.shape)\n",
        "# print(x_train_comm.shape)\n",
        "\n",
        "# x_valid_v = x_valid[:, 0:20]\n",
        "# x_valid_c = x_valid[:, 20:45]\n",
        "# x_valid_comm = x_valid[:, 45:51]\n",
        "\n",
        "# print(x_valid_v.shape)\n",
        "# print(x_valid_c.shape)\n",
        "# print(x_valid_comm.shape)\n",
        "\n",
        "X_t_test = x_test2[:, 0:20]\n",
        "x_test_c2 = x_test2[:, 20:45]\n",
        "x_test_comm2 = x_test2[:, 45:51]\n",
        "\n",
        "\n",
        "# DLR imputed target domain\n",
        "x_test_t_pt = model_reg.predict(X_t_test)\n",
        "print(x_test_t_pt.shape)\n",
        "\n",
        "x_test_pt_test = np.concatenate((x_test_t_pt, x_test_comm2),axis=1)\n",
        "print(x_test_pt_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original X_t_test:  (144703, 20)\n",
            "rows_test: (array([  1502,   1503,   1504, ..., 144700, 144701, 144702]),)\n",
            "rows_test.shape: 1\n",
            "after SZA X_t_test:  (60487, 20)\n",
            "after SZA X_s_test:  (60487, 25)\n",
            "(60487, 31)\n",
            "(60487, 20)\n",
            "(60487, 25)\n",
            "(60487, 31)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIiDX5w-_avP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# score = model.evaluate(x_test_pt_test, Y_t_test, verbose=0)\n",
        "# print('Test loss:', score[0])\n",
        "# print('Test accuracy:', score[1])\n",
        "# print('Test score:', score)\n",
        "# predict_result = model.predict_proba(x_test_pt);\n",
        "# print (predict_result)\n",
        "# from sklearn.metrics import roc_auc_score\n",
        "# roc_auc_score(y_test, predict_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvcysY7RaZLD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "18f1f6eb-e317-444f-9cec-1fe0af07f333"
      },
      "source": [
        "# run the Correlation based DA\n",
        "# pytorch mlp for multiclass classification\n",
        "from numpy import vstack\n",
        "from numpy import argmax\n",
        "from pandas import read_csv\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch.nn import Linear\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import Softmax\n",
        "from torch.nn import Module\n",
        "from torch.nn import Dropout\n",
        "from torch.nn import BatchNorm1d\n",
        "from torch.optim import SGD,RMSprop,Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn.init import kaiming_uniform_\n",
        "from torch.nn.init import xavier_uniform_\n",
        "import torch\n",
        "\n",
        "n_epochs = 100\n",
        "lambda_ = 0.00001\n",
        "NUM = 31\n",
        "# DIFFERECE_COL = 5\n",
        "BATCH_SIZE = 2048\n",
        "\n",
        "# dataset definition\n",
        "class CSVDataset(Dataset):\n",
        "    # load the dataset\n",
        "    def __init__(self, X1, Y1):\n",
        "        # load the csv file as a dataframe\n",
        "        self.X=X1\n",
        "        self.y=Y1\n",
        "        print(\"self.X before fit_transform\")\n",
        "        print(self.X)\n",
        "        print(\"self.y before fit_transform\")\n",
        "        print(self.y)\n",
        "        self.X = self.X.astype('float32')\n",
        "        # label encode target and ensure the values are floats\n",
        "        self.y = LabelEncoder().fit_transform(self.y)\n",
        "        print(\"self.X before fit_transform\")\n",
        "        print(self.X)\n",
        "        print(\"self.y after fit_transform\")\n",
        "        print(self.y)\n",
        "\n",
        "    # number of rows in the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    # get a row at an index\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.X[idx], self.y[idx]]\n",
        "\n",
        "    # get indexes for train and test rows\n",
        "    def get_splits(self, n_test=0.33):\n",
        "        # determine sizes\n",
        "        test_size = round(n_test * len(self.X))\n",
        "        train_size = len(self.X) - test_size\n",
        "        # calculate the split\n",
        "        return random_split(self, [train_size, test_size])\n",
        "\n",
        "def CORAL(src,tgt):\n",
        "    d = src.size(1)\n",
        "    src_c = coral(src)\n",
        "    tgt_c = coral(tgt)\n",
        "\n",
        "    loss = torch.sum(torch.mul((src_c-tgt_c),(src_c-tgt_c)))\n",
        "    loss = loss/(4*d*d)\n",
        "    return loss\n",
        "\n",
        "def LOG_CORAL(src,tgt):\n",
        "    d = src.size(1)\n",
        "    src_c = coral(src)\n",
        "    tgt_c = coral(tgt)\n",
        "    src_vals, src_vecs = torch.symeig(src_c,eigenvectors = True)\n",
        "    tgt_vals, tgt_vecs = torch.symeig(tgt_c,eigenvectors = True)\n",
        "    src_cc = torch.mm(src_vecs,torch.mm(torch.diag(torch.log(src_vals)),src_vecs.t()))\n",
        "    tgt_cc = torch.mm(tgt_vecs,torch.mm(torch.diag(torch.log(tgt_vals)),tgt_vecs.t()))\n",
        "    loss = torch.sum(torch.mul((src_cc - tgt_cc), (src_cc - tgt_cc)))\n",
        "    loss = loss / (4 * d * d)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def coral(data):\n",
        "    n = data.size(0)\n",
        "    id_row = torch.ones(n).resize(1,n)\n",
        "    if torch.cuda.is_available():\n",
        "        id_row = id_row.cuda()\n",
        "    sum_column = torch.mm(id_row,data)\n",
        "    mean_column = torch.div(sum_column,n)\n",
        "    mean_mean = torch.mm(mean_column.t(),mean_column)\n",
        "    d_d = torch.mm(data.t(),data)\n",
        "    coral_result = torch.add(d_d,(-1*mean_mean))*1.0/(n-1)\n",
        "    return coral_result\n",
        "\n",
        "class Deep_coral(Module):\n",
        "    def __init__(self,num_classes = 6):\n",
        "        super(Deep_coral,self).__init__()\n",
        "        self.feature = MLP(n_inputs=NUM)\n",
        "        self.fc = Linear(64,num_classes)\n",
        "        xavier_uniform_(self.fc.weight)\n",
        "        #  initial layer\n",
        "        # self.init_layer = Linear(NUM+5, NUM)\n",
        "        # xavier_uniform_(self.init_layer.weight)\n",
        "        # self.act3 = Softmax(dim=1)\n",
        "        # self.fc.weight.data.normal_(0,0.005)# initialization\n",
        "\n",
        "    def forward(self,src,tgt):\n",
        "        src = self.feature(src)\n",
        "        src = self.fc(src)\n",
        "        # output layer\n",
        "        # src = self.act3(src)\n",
        "        # tgt = self.init_layer(tgt)\n",
        "        tgt = self.feature(tgt)\n",
        "        tgt = self.fc(tgt)\n",
        "        # tgt = self.act3(tgt)\n",
        "        return src,tgt\n",
        "\n",
        "# model definition\n",
        "class MLP(Module):\n",
        "    # define model elements\n",
        "    def __init__(self, n_inputs=NUM):\n",
        "        super(MLP, self).__init__()\n",
        "        # # input to very beginning hidden layer\n",
        "        self.hidden = Linear(n_inputs, 128)\n",
        "        kaiming_uniform_(self.hidden.weight, nonlinearity='relu')\n",
        "        self.act = ReLU()\n",
        "        # input to beginning hidden layer\n",
        "        self.hidden0 = Linear(128, 256)\n",
        "        kaiming_uniform_(self.hidden0.weight, nonlinearity='relu')\n",
        "        self.act0 = ReLU()\n",
        "        # input to first hidden layer\n",
        "        self.hidden1 = Linear(256, 128)\n",
        "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
        "        self.act1 = ReLU()\n",
        "        # second hidden layer\n",
        "        self.hidden2 = Linear(128, 64)\n",
        "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
        "        self.act2 = ReLU()\n",
        "        # # third hidden layer and output\n",
        "        # self.hidden3 = Linear(64, 6)\n",
        "        # xavier_uniform_(self.hidden3.weight)\n",
        "        # self.act3 = Softmax(dim=1)\n",
        "        self.dropout = Dropout(p=0.5)\n",
        "        self.batchnorm = BatchNorm1d(128)\n",
        "        self.batchnorm0 = BatchNorm1d(256)\n",
        "        self.batchnorm1 = BatchNorm1d(128)\n",
        "        self.batchnorm2 = BatchNorm1d(64)\n",
        "\n",
        "    # forward propagate input\n",
        "    def forward(self, X):\n",
        "        # input to very first hidden layer\n",
        "        X = self.hidden(X)\n",
        "        X = self.batchnorm(X)\n",
        "        X = self.act(X)\n",
        "        X = self.dropout(X)\n",
        "        # input to first hidden layer\n",
        "        X = self.hidden0(X)\n",
        "        X = self.batchnorm0(X)\n",
        "        X = self.act0(X)\n",
        "        X = self.dropout(X)\n",
        "        # input to second hidden layer\n",
        "        X = self.hidden1(X)\n",
        "        X = self.batchnorm1(X)\n",
        "        X = self.act1(X)\n",
        "        X = self.dropout(X)\n",
        "        # third hidden layer\n",
        "        X = self.hidden2(X)\n",
        "        X = self.batchnorm2(X)\n",
        "        X = self.act2(X)\n",
        "        # X = self.dropout(X)\n",
        "        # # output layer\n",
        "        # X = self.hidden3(X)\n",
        "        # X = self.act3(X)\n",
        "        return X\n",
        "\n",
        "# # prepare the dataset - random split within a dataset\n",
        "# def prepare_data(X2, Y2):\n",
        "#     # load the dataset\n",
        "#     dataset = CSVDataset(X2, Y2)\n",
        "#     # calculate split\n",
        "#     train, test = dataset.get_splits()\n",
        "#     # prepare data loaders\n",
        "#     train_dl = DataLoader(train, batch_size=512, shuffle=True)\n",
        "#     test_dl = DataLoader(test, batch_size=512, shuffle=False)\n",
        "#     return train_dl, test_dl\n",
        "\n",
        "# prepare the dataset - random split within a dataset\n",
        "def prepare_data(X2_train, Y2_train, X2_test, Y2_test):\n",
        "    # load the train dataset\n",
        "    train = CSVDataset(X2_train, Y2_train)\n",
        "    # load the test dataset\n",
        "    test = CSVDataset(X2_test, Y2_test)\n",
        "    # prepare data loaders\n",
        "    train_dl = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_dl = DataLoader(test, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    return train_dl, test_dl\n",
        "\n",
        "aggre_losses = []\n",
        "aggre_train_acc = []\n",
        "aggre_test_acc = []\n",
        "aggre_train_tgt_acc = []\n",
        "\n",
        "# train the model\n",
        "def train_model(train_src, train_tgt, model):\n",
        "    # define the optimization\n",
        "    criterion = CrossEntropyLoss()\n",
        "    # optimizer = SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
        "    optimizer = RMSprop(model.parameters(), lr=0.0001)\n",
        "\n",
        "    # enumerate epochs\n",
        "    j = 0\n",
        "    for epoch in range(n_epochs):\n",
        "        j += 1\n",
        "        # enumerate mini batches of src domain and target domain\n",
        "        train_steps = min(len(train_src), len(train_tgt))\n",
        "        print(\"train_steps:\", train_steps)\n",
        "        iter_src = iter(train_src)\n",
        "        iter_tgt = iter(train_tgt)\n",
        "\n",
        "        epoch_loss = 0\n",
        "        for i in range(train_steps):\n",
        "            # inputs, targets = train_data.next()\n",
        "            # load the src and target training data\n",
        "            src_data, src_label = iter_src.next()\n",
        "            tgt_data, tgt_label = iter_tgt.next()\n",
        "            # clear the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # compute the model output\n",
        "            # yhat = model(inputs)\n",
        "            src_out, tgt_out = model(src_data, tgt_data)\n",
        "\n",
        "            # calculate loss\n",
        "            # loss = criterion(yhat, targets)\n",
        "            # epoch_loss = loss\n",
        "            loss_classifier = criterion(src_out, src_label)\n",
        "            loss_coral = CORAL(src_out, tgt_out)\n",
        "            sum_loss = lambda_ * loss_coral + loss_classifier\n",
        "            epoch_loss += sum_loss\n",
        "\n",
        "            # credit assignment\n",
        "            sum_loss.backward()\n",
        "            # update model weights\n",
        "            optimizer.step()\n",
        "\n",
        "        print('Train Epoch: {:2d} [{:2d}/{:2d}]\\t'\n",
        "              'Lambda: {:.4f}, Class: {:.6f}, CORAL: {:.6f}, Total_Loss: {:.6f}'.format(\n",
        "            epoch,\n",
        "            i + 1,\n",
        "            train_steps,\n",
        "            lambda_,\n",
        "            loss_classifier.item(),\n",
        "            loss_coral.item(),\n",
        "            sum_loss.item()\n",
        "        ))\n",
        "        print('Train ith Epoch %d result:' % epoch)\n",
        "        # calculate train src accuracy\n",
        "        train_acc = evaluate_model(train_src, model)\n",
        "        aggre_train_acc.append(train_acc)\n",
        "        print('train_acc: %.3f' % train_acc)\n",
        "\n",
        "        # calculate train tgt accuracy\n",
        "        train_tgt_acc = evaluate_model(train_tgt, model)\n",
        "        aggre_train_tgt_acc.append(train_tgt_acc)\n",
        "        print('train_tgt_acc: %.3f' % train_tgt_acc)\n",
        "\n",
        "        # calculate validate accuracy\n",
        "        test_acc = evaluate_model(test_tgt, model)\n",
        "        aggre_test_acc.append(test_acc)\n",
        "        print('test_acc: %.3f' % test_acc)\n",
        "\n",
        "        epoch_loss = epoch_loss / train_steps\n",
        "        aggre_losses.append(epoch_loss)\n",
        "        print(f'epoch: {j:3} loss: {epoch_loss.item():6.4f}')\n",
        "\n",
        "# evaluate the model\n",
        "def evaluate_model(test_dl, model):\n",
        "    predictions, actuals = list(), list()\n",
        "    test_steps = len(test_dl)\n",
        "    iter_test = iter(test_dl)\n",
        "    # for i, (inputs, targets) in enumerate(test_dl):\n",
        "    for i in range(test_steps):\n",
        "        # evaluate the model on the test set\n",
        "        tgt_data, targets = iter_test.next()\n",
        "        # temp = torch.zeros((tgt_data.shape[0], DIFFERECE_COL))\n",
        "        # tmp_data = torch.cat((tgt_data, temp), 1)\n",
        "        with torch.no_grad():\n",
        "          yhat, _ = model(tgt_data, tgt_data)\n",
        "        # retrieve numpy array\n",
        "        yhat = yhat.detach().numpy()\n",
        "        actual = targets.numpy()\n",
        "        # convert to class labels\n",
        "        yhat = argmax(yhat, axis=1)\n",
        "        # reshape for stacking\n",
        "        actual = actual.reshape((len(actual), 1))\n",
        "        yhat = yhat.reshape((len(yhat), 1))\n",
        "        # store\n",
        "        predictions.append(yhat)\n",
        "        actuals.append(actual)\n",
        "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
        "    # calculate accuracy\n",
        "    acc = accuracy_score(actuals, predictions)\n",
        "    return acc\n",
        "\n",
        "# make a class prediction for one row of data\n",
        "def predict(row, model):\n",
        "    # convert row to data\n",
        "    row = Tensor([row])\n",
        "    # make prediction\n",
        "    yhat = model(row)\n",
        "    # retrieve numpy array\n",
        "    yhat = yhat.detach().numpy()\n",
        "    return yhat\n",
        "\n",
        "X_s = x_train_src\n",
        "Y_s = y_train\n",
        "\n",
        "X_t = x_train_pt\n",
        "Y_t = Y_s\n",
        "\n",
        "X_s_test = X_s_test\n",
        "Y_s_test = Y_s_test\n",
        "X_t_test = x_test_pt_test\n",
        "Y_t_test = Y_s_test\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "# y_valid = keras.utils.to_categorical(y_valid-1, num_classes)\n",
        "# y_test = keras.utils.to_categorical(y_test-1, num_classes)\n",
        "\n",
        "# prepare the data\n",
        "train_src, test_src = prepare_data(X_s, Y_s, X_s_test, Y_s_test)\n",
        "train_tgt, test_tgt = prepare_data(X_t, Y_t, X_t_test, Y_t_test)\n",
        "\n",
        "# print(\"train_dl\")\n",
        "# print(train_dl)\n",
        "# print(len(train_dl.dataset), len(test_dl.dataset))\n",
        "# define the network\n",
        "# model = MLP(20)\n",
        "model = Deep_coral(num_classes=6)\n",
        "# train the model\n",
        "train_model(train_src, train_tgt, model)\n",
        "# train_model(train_tgt, train_src, model)\n",
        "\n",
        "# evaluate the model\n",
        "# acc = evaluate_model(test_tgt, model)\n",
        "acc = evaluate_model(test_tgt, model)\n",
        "\n",
        "print('Accuracy: %.3f' % acc)\n",
        "# make a single prediction\n",
        "# row = [5.1,3.5,1.4,0.2]\n",
        "# yhat = predict(row, model)\n",
        "# print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))\n",
        "\n",
        "#plot the loss\n",
        "plt.figure()\n",
        "plt.plot(range(n_epochs), aggre_losses)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('epoch');\n",
        "\n",
        "#plot the train accuracy\n",
        "plt.figure()\n",
        "plt.plot(range(n_epochs), aggre_train_acc, '-', label='train src')\n",
        "plt.plot(range(n_epochs), aggre_train_tgt_acc, '-', label='train tgt')\n",
        "plt.plot(range(n_epochs), aggre_test_acc, '-', label='test tgt')\n",
        "plt.title('Classification accuracy history')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Clasification accuracy')\n",
        "plt.legend()\n",
        "plt.gcf().set_size_inches(14, 4)\n",
        "\n",
        "\n",
        "#plot the combined\n",
        "plt.figure()\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(n_epochs), aggre_losses, '-')\n",
        "plt.title('Loss history')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(n_epochs), aggre_train_acc, '-', label='train src')\n",
        "plt.plot(range(n_epochs), aggre_train_tgt_acc, '-', label='train tgt')\n",
        "plt.plot(range(n_epochs), aggre_test_acc, '-', label='test tgt')\n",
        "plt.title('Classification accuracy history')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Clasification accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.gcf().set_size_inches(14, 4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "self.X before fit_transform\n",
            "[[ 0.21177404 -0.1636653   1.35447067 ...  1.65943487 -1.99399076\n",
            "  -0.51567848]\n",
            " [-1.24009487 -0.95303212 -0.81044312 ... -0.61386271  0.4848845\n",
            "   1.34519875]\n",
            " [-1.24009487 -0.95303212 -0.81044312 ...  1.65002284 -1.16769901\n",
            "  -0.51567848]\n",
            " ...\n",
            " [-1.24009487 -0.95303212 -0.81044312 ...  1.66011902 -0.96112607\n",
            "  -0.51567848]\n",
            " [-1.24009487 -0.95303212 -0.81044312 ...  1.64380342  0.27831156\n",
            "  -0.51567848]\n",
            " [ 0.21177404 -0.1636653  -0.81044312 ...  1.65868855 -0.13483431\n",
            "   4.22955846]]\n",
            "self.y before fit_transform\n",
            "[[2]\n",
            " [1]\n",
            " [1]\n",
            " ...\n",
            " [5]\n",
            " [1]\n",
            " [3]]\n",
            "self.X before fit_transform\n",
            "[[ 0.21177405 -0.1636653   1.3544706  ...  1.6594349  -1.9939908\n",
            "  -0.51567847]\n",
            " [-1.2400949  -0.95303214 -0.8104431  ... -0.6138627   0.4848845\n",
            "   1.3451988 ]\n",
            " [-1.2400949  -0.95303214 -0.8104431  ...  1.6500229  -1.167699\n",
            "  -0.51567847]\n",
            " ...\n",
            " [-1.2400949  -0.95303214 -0.8104431  ...  1.660119   -0.9611261\n",
            "  -0.51567847]\n",
            " [-1.2400949  -0.95303214 -0.8104431  ...  1.6438035   0.27831155\n",
            "  -0.51567847]\n",
            " [ 0.21177405 -0.1636653  -0.8104431  ...  1.6586885  -0.13483432\n",
            "   4.2295585 ]]\n",
            "self.y after fit_transform\n",
            "[1 0 0 ... 4 0 2]\n",
            "self.X before fit_transform\n",
            "[[ 0.         0.         0.        ... -9.9989996 17.         0.       ]\n",
            " [ 0.         0.         0.        ... -9.9989996 17.         0.       ]\n",
            " [ 0.         0.         0.        ... -9.9989996 17.         0.       ]\n",
            " ...\n",
            " [ 1.         2.         1.        ... -9.9989996 17.         0.       ]\n",
            " [ 1.         2.         1.        ... -9.9989996 17.         0.       ]\n",
            " [ 1.         2.         1.        ... -9.9989996 17.         0.       ]]\n",
            "self.y before fit_transform\n",
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " ...\n",
            " [6]\n",
            " [6]\n",
            " [6]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "self.X before fit_transform\n",
            "[[ 0.     0.     0.    ... -9.999 17.     0.   ]\n",
            " [ 0.     0.     0.    ... -9.999 17.     0.   ]\n",
            " [ 0.     0.     0.    ... -9.999 17.     0.   ]\n",
            " ...\n",
            " [ 1.     2.     1.    ... -9.999 17.     0.   ]\n",
            " [ 1.     2.     1.    ... -9.999 17.     0.   ]\n",
            " [ 1.     2.     1.    ... -9.999 17.     0.   ]]\n",
            "self.y after fit_transform\n",
            "[0 0 0 ... 5 5 5]\n",
            "self.X before fit_transform\n",
            "[[ 0.35823467  0.12620217  0.9982332  ...  1.65943487 -1.99399076\n",
            "  -0.51567848]\n",
            " [-0.96388888 -0.69021094 -0.65073675 ... -0.61386271  0.4848845\n",
            "   1.34519875]\n",
            " [-0.96073598 -0.68975067 -0.6470691  ...  1.65002284 -1.16769901\n",
            "  -0.51567848]\n",
            " ...\n",
            " [-0.88279784 -0.63213605 -0.57883036 ...  1.66011902 -0.96112607\n",
            "  -0.51567848]\n",
            " [-1.02602828 -0.76340628 -0.64171576 ...  1.64380342  0.27831156\n",
            "  -0.51567848]\n",
            " [ 0.38450146  0.1448171  -0.65297019 ...  1.65868855 -0.13483431\n",
            "   4.22955846]]\n",
            "self.y before fit_transform\n",
            "[[2]\n",
            " [1]\n",
            " [1]\n",
            " ...\n",
            " [5]\n",
            " [1]\n",
            " [3]]\n",
            "self.X before fit_transform\n",
            "[[ 0.35823467  0.12620217  0.9982332  ...  1.6594349  -1.9939908\n",
            "  -0.51567847]\n",
            " [-0.9638889  -0.69021094 -0.65073675 ... -0.6138627   0.4848845\n",
            "   1.3451988 ]\n",
            " [-0.960736   -0.6897507  -0.6470691  ...  1.6500229  -1.167699\n",
            "  -0.51567847]\n",
            " ...\n",
            " [-0.88279784 -0.63213605 -0.57883036 ...  1.660119   -0.9611261\n",
            "  -0.51567847]\n",
            " [-1.0260283  -0.7634063  -0.64171576 ...  1.6438035   0.27831155\n",
            "  -0.51567847]\n",
            " [ 0.38450146  0.1448171  -0.6529702  ...  1.6586885  -0.13483432\n",
            "   4.2295585 ]]\n",
            "self.y after fit_transform\n",
            "[1 0 0 ... 4 0 2]\n",
            "self.X before fit_transform\n",
            "[[-1.13421071 -0.84875751 -0.66122597 ... -0.61386271  0.4848845\n",
            "  -0.51567848]\n",
            " [-1.14985156 -0.85990697 -0.67347807 ... -0.61386271  0.4848845\n",
            "  -0.51567848]\n",
            " [-1.1347903  -0.84920704 -0.66171968 ... -0.61386271  0.4848845\n",
            "  -0.51567848]\n",
            " ...\n",
            " [-1.0369283  -0.7770859  -0.59257787 ... -0.61386271  0.4848845\n",
            "  -0.51567848]\n",
            " [-1.03785098 -0.77679843 -0.5955677  ... -0.61386271  0.4848845\n",
            "  -0.51567848]\n",
            " [-1.05599523 -0.78691089 -0.61557055 ... -0.61386271  0.4848845\n",
            "  -0.51567848]]\n",
            "self.y before fit_transform\n",
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " ...\n",
            " [6]\n",
            " [6]\n",
            " [6]]\n",
            "self.X before fit_transform\n",
            "[[-1.1342107  -0.8487575  -0.661226   ... -0.6138627   0.4848845\n",
            "  -0.51567847]\n",
            " [-1.1498516  -0.859907   -0.67347807 ... -0.6138627   0.4848845\n",
            "  -0.51567847]\n",
            " [-1.1347903  -0.84920704 -0.6617197  ... -0.6138627   0.4848845\n",
            "  -0.51567847]\n",
            " ...\n",
            " [-1.0369283  -0.7770859  -0.5925779  ... -0.6138627   0.4848845\n",
            "  -0.51567847]\n",
            " [-1.037851   -0.7767984  -0.5955677  ... -0.6138627   0.4848845\n",
            "  -0.51567847]\n",
            " [-1.0559952  -0.7869109  -0.61557055 ... -0.6138627   0.4848845\n",
            "  -0.51567847]]\n",
            "self.y after fit_transform\n",
            "[0 0 0 ... 5 5 5]\n",
            "train_steps: 241\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/tensor.py:365: UserWarning: non-inplace resize is deprecated\n",
            "  warnings.warn(\"non-inplace resize is deprecated\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch:  0 [241/241]\tLambda: 0.0000, Class: 0.534969, CORAL: 0.008578, Total_Loss: 0.534969\n",
            "Train ith Epoch 0 result:\n",
            "train_acc: 0.832\n",
            "train_tgt_acc: 0.644\n",
            "test_acc: 0.622\n",
            "epoch:   1 loss: 0.9100\n",
            "train_steps: 241\n",
            "Train Epoch:  1 [241/241]\tLambda: 0.0000, Class: 0.307672, CORAL: 0.016956, Total_Loss: 0.307672\n",
            "Train ith Epoch 1 result:\n",
            "train_acc: 0.910\n",
            "train_tgt_acc: 0.665\n",
            "test_acc: 0.640\n",
            "epoch:   2 loss: 0.4177\n",
            "train_steps: 241\n",
            "Train Epoch:  2 [241/241]\tLambda: 0.0000, Class: 0.191309, CORAL: 0.039971, Total_Loss: 0.191309\n",
            "Train ith Epoch 2 result:\n",
            "train_acc: 0.939\n",
            "train_tgt_acc: 0.673\n",
            "test_acc: 0.646\n",
            "epoch:   3 loss: 0.2520\n",
            "train_steps: 241\n",
            "Train Epoch:  3 [241/241]\tLambda: 0.0000, Class: 0.153182, CORAL: 0.036531, Total_Loss: 0.153182\n",
            "Train ith Epoch 3 result:\n",
            "train_acc: 0.956\n",
            "train_tgt_acc: 0.677\n",
            "test_acc: 0.651\n",
            "epoch:   4 loss: 0.1728\n",
            "train_steps: 241\n",
            "Train Epoch:  4 [241/241]\tLambda: 0.0000, Class: 0.115752, CORAL: 0.085738, Total_Loss: 0.115753\n",
            "Train ith Epoch 4 result:\n",
            "train_acc: 0.966\n",
            "train_tgt_acc: 0.679\n",
            "test_acc: 0.652\n",
            "epoch:   5 loss: 0.1287\n",
            "train_steps: 241\n",
            "Train Epoch:  5 [241/241]\tLambda: 0.0000, Class: 0.084593, CORAL: 0.119013, Total_Loss: 0.084594\n",
            "Train ith Epoch 5 result:\n",
            "train_acc: 0.972\n",
            "train_tgt_acc: 0.683\n",
            "test_acc: 0.656\n",
            "epoch:   6 loss: 0.1010\n",
            "train_steps: 241\n",
            "Train Epoch:  6 [241/241]\tLambda: 0.0000, Class: 0.073845, CORAL: 0.121338, Total_Loss: 0.073846\n",
            "Train ith Epoch 6 result:\n",
            "train_acc: 0.976\n",
            "train_tgt_acc: 0.684\n",
            "test_acc: 0.657\n",
            "epoch:   7 loss: 0.0841\n",
            "train_steps: 241\n",
            "Train Epoch:  7 [241/241]\tLambda: 0.0000, Class: 0.073661, CORAL: 0.125498, Total_Loss: 0.073662\n",
            "Train ith Epoch 7 result:\n",
            "train_acc: 0.980\n",
            "train_tgt_acc: 0.686\n",
            "test_acc: 0.659\n",
            "epoch:   8 loss: 0.0714\n",
            "train_steps: 241\n",
            "Train Epoch:  8 [241/241]\tLambda: 0.0000, Class: 0.075348, CORAL: 0.149592, Total_Loss: 0.075350\n",
            "Train ith Epoch 8 result:\n",
            "train_acc: 0.982\n",
            "train_tgt_acc: 0.686\n",
            "test_acc: 0.658\n",
            "epoch:   9 loss: 0.0616\n",
            "train_steps: 241\n",
            "Train Epoch:  9 [241/241]\tLambda: 0.0000, Class: 0.037906, CORAL: 0.142982, Total_Loss: 0.037908\n",
            "Train ith Epoch 9 result:\n",
            "train_acc: 0.984\n",
            "train_tgt_acc: 0.688\n",
            "test_acc: 0.658\n",
            "epoch:  10 loss: 0.0548\n",
            "train_steps: 241\n",
            "Train Epoch: 10 [241/241]\tLambda: 0.0000, Class: 0.061082, CORAL: 0.131272, Total_Loss: 0.061084\n",
            "Train ith Epoch 10 result:\n",
            "train_acc: 0.986\n",
            "train_tgt_acc: 0.689\n",
            "test_acc: 0.661\n",
            "epoch:  11 loss: 0.0489\n",
            "train_steps: 241\n",
            "Train Epoch: 11 [241/241]\tLambda: 0.0000, Class: 0.050520, CORAL: 0.140141, Total_Loss: 0.050522\n",
            "Train ith Epoch 11 result:\n",
            "train_acc: 0.987\n",
            "train_tgt_acc: 0.689\n",
            "test_acc: 0.659\n",
            "epoch:  12 loss: 0.0443\n",
            "train_steps: 241\n",
            "Train Epoch: 12 [241/241]\tLambda: 0.0000, Class: 0.054177, CORAL: 0.219581, Total_Loss: 0.054179\n",
            "Train ith Epoch 12 result:\n",
            "train_acc: 0.988\n",
            "train_tgt_acc: 0.690\n",
            "test_acc: 0.662\n",
            "epoch:  13 loss: 0.0399\n",
            "train_steps: 241\n",
            "Train Epoch: 13 [241/241]\tLambda: 0.0000, Class: 0.028074, CORAL: 0.280507, Total_Loss: 0.028077\n",
            "Train ith Epoch 13 result:\n",
            "train_acc: 0.989\n",
            "train_tgt_acc: 0.690\n",
            "test_acc: 0.661\n",
            "epoch:  14 loss: 0.0371\n",
            "train_steps: 241\n",
            "Train Epoch: 14 [241/241]\tLambda: 0.0000, Class: 0.029937, CORAL: 0.230451, Total_Loss: 0.029939\n",
            "Train ith Epoch 14 result:\n",
            "train_acc: 0.990\n",
            "train_tgt_acc: 0.691\n",
            "test_acc: 0.661\n",
            "epoch:  15 loss: 0.0333\n",
            "train_steps: 241\n",
            "Train Epoch: 15 [241/241]\tLambda: 0.0000, Class: 0.027399, CORAL: 0.263182, Total_Loss: 0.027402\n",
            "Train ith Epoch 15 result:\n",
            "train_acc: 0.991\n",
            "train_tgt_acc: 0.691\n",
            "test_acc: 0.663\n",
            "epoch:  16 loss: 0.0311\n",
            "train_steps: 241\n",
            "Train Epoch: 16 [241/241]\tLambda: 0.0000, Class: 0.032847, CORAL: 0.340888, Total_Loss: 0.032850\n",
            "Train ith Epoch 16 result:\n",
            "train_acc: 0.991\n",
            "train_tgt_acc: 0.692\n",
            "test_acc: 0.663\n",
            "epoch:  17 loss: 0.0290\n",
            "train_steps: 241\n",
            "Train Epoch: 17 [241/241]\tLambda: 0.0000, Class: 0.017891, CORAL: 0.254080, Total_Loss: 0.017893\n",
            "Train ith Epoch 17 result:\n",
            "train_acc: 0.992\n",
            "train_tgt_acc: 0.692\n",
            "test_acc: 0.664\n",
            "epoch:  18 loss: 0.0272\n",
            "train_steps: 241\n",
            "Train Epoch: 18 [241/241]\tLambda: 0.0000, Class: 0.024917, CORAL: 0.406800, Total_Loss: 0.024921\n",
            "Train ith Epoch 18 result:\n",
            "train_acc: 0.993\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.663\n",
            "epoch:  19 loss: 0.0256\n",
            "train_steps: 241\n",
            "Train Epoch: 19 [241/241]\tLambda: 0.0000, Class: 0.017657, CORAL: 0.528070, Total_Loss: 0.017662\n",
            "Train ith Epoch 19 result:\n",
            "train_acc: 0.993\n",
            "train_tgt_acc: 0.692\n",
            "test_acc: 0.662\n",
            "epoch:  20 loss: 0.0237\n",
            "train_steps: 241\n",
            "Train Epoch: 20 [241/241]\tLambda: 0.0000, Class: 0.030322, CORAL: 0.441831, Total_Loss: 0.030326\n",
            "Train ith Epoch 20 result:\n",
            "train_acc: 0.994\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.662\n",
            "epoch:  21 loss: 0.0223\n",
            "train_steps: 241\n",
            "Train Epoch: 21 [241/241]\tLambda: 0.0000, Class: 0.022628, CORAL: 0.526363, Total_Loss: 0.022634\n",
            "Train ith Epoch 21 result:\n",
            "train_acc: 0.994\n",
            "train_tgt_acc: 0.692\n",
            "test_acc: 0.663\n",
            "epoch:  22 loss: 0.0209\n",
            "train_steps: 241\n",
            "Train Epoch: 22 [241/241]\tLambda: 0.0000, Class: 0.016962, CORAL: 0.846477, Total_Loss: 0.016970\n",
            "Train ith Epoch 22 result:\n",
            "train_acc: 0.994\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.663\n",
            "epoch:  23 loss: 0.0197\n",
            "train_steps: 241\n",
            "Train Epoch: 23 [241/241]\tLambda: 0.0000, Class: 0.026039, CORAL: 0.359611, Total_Loss: 0.026042\n",
            "Train ith Epoch 23 result:\n",
            "train_acc: 0.995\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.663\n",
            "epoch:  24 loss: 0.0187\n",
            "train_steps: 241\n",
            "Train Epoch: 24 [241/241]\tLambda: 0.0000, Class: 0.017700, CORAL: 0.371553, Total_Loss: 0.017703\n",
            "Train ith Epoch 24 result:\n",
            "train_acc: 0.995\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.662\n",
            "epoch:  25 loss: 0.0178\n",
            "train_steps: 241\n",
            "Train Epoch: 25 [241/241]\tLambda: 0.0000, Class: 0.022787, CORAL: 0.473736, Total_Loss: 0.022792\n",
            "Train ith Epoch 25 result:\n",
            "train_acc: 0.995\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.663\n",
            "epoch:  26 loss: 0.0167\n",
            "train_steps: 241\n",
            "Train Epoch: 26 [241/241]\tLambda: 0.0000, Class: 0.013907, CORAL: 0.699055, Total_Loss: 0.013914\n",
            "Train ith Epoch 26 result:\n",
            "train_acc: 0.996\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.663\n",
            "epoch:  27 loss: 0.0165\n",
            "train_steps: 241\n",
            "Train Epoch: 27 [241/241]\tLambda: 0.0000, Class: 0.009127, CORAL: 0.402903, Total_Loss: 0.009131\n",
            "Train ith Epoch 27 result:\n",
            "train_acc: 0.996\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.663\n",
            "epoch:  28 loss: 0.0150\n",
            "train_steps: 241\n",
            "Train Epoch: 28 [241/241]\tLambda: 0.0000, Class: 0.017389, CORAL: 0.882469, Total_Loss: 0.017398\n",
            "Train ith Epoch 28 result:\n",
            "train_acc: 0.996\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.665\n",
            "epoch:  29 loss: 0.0150\n",
            "train_steps: 241\n",
            "Train Epoch: 29 [241/241]\tLambda: 0.0000, Class: 0.012433, CORAL: 0.610773, Total_Loss: 0.012439\n",
            "Train ith Epoch 29 result:\n",
            "train_acc: 0.996\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.664\n",
            "epoch:  30 loss: 0.0142\n",
            "train_steps: 241\n",
            "Train Epoch: 30 [241/241]\tLambda: 0.0000, Class: 0.013191, CORAL: 1.063620, Total_Loss: 0.013201\n",
            "Train ith Epoch 30 result:\n",
            "train_acc: 0.996\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.664\n",
            "epoch:  31 loss: 0.0132\n",
            "train_steps: 241\n",
            "Train Epoch: 31 [241/241]\tLambda: 0.0000, Class: 0.010100, CORAL: 0.855845, Total_Loss: 0.010109\n",
            "Train ith Epoch 31 result:\n",
            "train_acc: 0.997\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.664\n",
            "epoch:  32 loss: 0.0124\n",
            "train_steps: 241\n",
            "Train Epoch: 32 [241/241]\tLambda: 0.0000, Class: 0.019145, CORAL: 0.575102, Total_Loss: 0.019151\n",
            "Train ith Epoch 32 result:\n",
            "train_acc: 0.997\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.665\n",
            "epoch:  33 loss: 0.0123\n",
            "train_steps: 241\n",
            "Train Epoch: 33 [241/241]\tLambda: 0.0000, Class: 0.008438, CORAL: 0.785314, Total_Loss: 0.008446\n",
            "Train ith Epoch 33 result:\n",
            "train_acc: 0.997\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.664\n",
            "epoch:  34 loss: 0.0114\n",
            "train_steps: 241\n",
            "Train Epoch: 34 [241/241]\tLambda: 0.0000, Class: 0.008619, CORAL: 0.667087, Total_Loss: 0.008626\n",
            "Train ith Epoch 34 result:\n",
            "train_acc: 0.997\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.666\n",
            "epoch:  35 loss: 0.0114\n",
            "train_steps: 241\n",
            "Train Epoch: 35 [241/241]\tLambda: 0.0000, Class: 0.005896, CORAL: 0.396578, Total_Loss: 0.005900\n",
            "Train ith Epoch 35 result:\n",
            "train_acc: 0.997\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.665\n",
            "epoch:  36 loss: 0.0104\n",
            "train_steps: 241\n",
            "Train Epoch: 36 [241/241]\tLambda: 0.0000, Class: 0.008135, CORAL: 0.562096, Total_Loss: 0.008140\n",
            "Train ith Epoch 36 result:\n",
            "train_acc: 0.997\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.665\n",
            "epoch:  37 loss: 0.0104\n",
            "train_steps: 241\n",
            "Train Epoch: 37 [241/241]\tLambda: 0.0000, Class: 0.009245, CORAL: 1.835577, Total_Loss: 0.009263\n",
            "Train ith Epoch 37 result:\n",
            "train_acc: 0.997\n",
            "train_tgt_acc: 0.695\n",
            "test_acc: 0.665\n",
            "epoch:  38 loss: 0.0100\n",
            "train_steps: 241\n",
            "Train Epoch: 38 [241/241]\tLambda: 0.0000, Class: 0.008598, CORAL: 0.432853, Total_Loss: 0.008602\n",
            "Train ith Epoch 38 result:\n",
            "train_acc: 0.997\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.664\n",
            "epoch:  39 loss: 0.0097\n",
            "train_steps: 241\n",
            "Train Epoch: 39 [241/241]\tLambda: 0.0000, Class: 0.006778, CORAL: 1.783370, Total_Loss: 0.006796\n",
            "Train ith Epoch 39 result:\n",
            "train_acc: 0.998\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.665\n",
            "epoch:  40 loss: 0.0093\n",
            "train_steps: 241\n",
            "Train Epoch: 40 [241/241]\tLambda: 0.0000, Class: 0.007180, CORAL: 0.853594, Total_Loss: 0.007189\n",
            "Train ith Epoch 40 result:\n",
            "train_acc: 0.998\n",
            "train_tgt_acc: 0.695\n",
            "test_acc: 0.665\n",
            "epoch:  41 loss: 0.0090\n",
            "train_steps: 241\n",
            "Train Epoch: 41 [241/241]\tLambda: 0.0000, Class: 0.004808, CORAL: 0.524861, Total_Loss: 0.004813\n",
            "Train ith Epoch 41 result:\n",
            "train_acc: 0.998\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.664\n",
            "epoch:  42 loss: 0.0084\n",
            "train_steps: 241\n",
            "Train Epoch: 42 [241/241]\tLambda: 0.0000, Class: 0.004943, CORAL: 0.782264, Total_Loss: 0.004951\n",
            "Train ith Epoch 42 result:\n",
            "train_acc: 0.998\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.667\n",
            "epoch:  43 loss: 0.0080\n",
            "train_steps: 241\n",
            "Train Epoch: 43 [241/241]\tLambda: 0.0000, Class: 0.006455, CORAL: 1.031607, Total_Loss: 0.006466\n",
            "Train ith Epoch 43 result:\n",
            "train_acc: 0.998\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.664\n",
            "epoch:  44 loss: 0.0082\n",
            "train_steps: 241\n",
            "Train Epoch: 44 [241/241]\tLambda: 0.0000, Class: 0.005240, CORAL: 1.190746, Total_Loss: 0.005252\n",
            "Train ith Epoch 44 result:\n",
            "train_acc: 0.998\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.665\n",
            "epoch:  45 loss: 0.0079\n",
            "train_steps: 241\n",
            "Train Epoch: 45 [241/241]\tLambda: 0.0000, Class: 0.004504, CORAL: 1.156746, Total_Loss: 0.004516\n",
            "Train ith Epoch 45 result:\n",
            "train_acc: 0.998\n",
            "train_tgt_acc: 0.695\n",
            "test_acc: 0.664\n",
            "epoch:  46 loss: 0.0078\n",
            "train_steps: 241\n",
            "Train Epoch: 46 [241/241]\tLambda: 0.0000, Class: 0.007080, CORAL: 0.581409, Total_Loss: 0.007086\n",
            "Train ith Epoch 46 result:\n",
            "train_acc: 0.998\n",
            "train_tgt_acc: 0.695\n",
            "test_acc: 0.663\n",
            "epoch:  47 loss: 0.0073\n",
            "train_steps: 241\n",
            "Train Epoch: 47 [241/241]\tLambda: 0.0000, Class: 0.007078, CORAL: 0.668345, Total_Loss: 0.007085\n",
            "Train ith Epoch 47 result:\n",
            "train_acc: 0.998\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.664\n",
            "epoch:  48 loss: 0.0073\n",
            "train_steps: 241\n",
            "Train Epoch: 48 [241/241]\tLambda: 0.0000, Class: 0.001609, CORAL: 0.955110, Total_Loss: 0.001619\n",
            "Train ith Epoch 48 result:\n",
            "train_acc: 0.998\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.664\n",
            "epoch:  49 loss: 0.0070\n",
            "train_steps: 241\n",
            "Train Epoch: 49 [241/241]\tLambda: 0.0000, Class: 0.009145, CORAL: 1.203034, Total_Loss: 0.009157\n",
            "Train ith Epoch 49 result:\n",
            "train_acc: 0.998\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.665\n",
            "epoch:  50 loss: 0.0067\n",
            "train_steps: 241\n",
            "Train Epoch: 50 [241/241]\tLambda: 0.0000, Class: 0.003352, CORAL: 0.536381, Total_Loss: 0.003357\n",
            "Train ith Epoch 50 result:\n",
            "train_acc: 0.998\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.664\n",
            "epoch:  51 loss: 0.0064\n",
            "train_steps: 241\n",
            "Train Epoch: 51 [241/241]\tLambda: 0.0000, Class: 0.003269, CORAL: 1.454923, Total_Loss: 0.003284\n",
            "Train ith Epoch 51 result:\n",
            "train_acc: 0.998\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.664\n",
            "epoch:  52 loss: 0.0063\n",
            "train_steps: 241\n",
            "Train Epoch: 52 [241/241]\tLambda: 0.0000, Class: 0.007206, CORAL: 1.397967, Total_Loss: 0.007220\n",
            "Train ith Epoch 52 result:\n",
            "train_acc: 0.998\n",
            "train_tgt_acc: 0.695\n",
            "test_acc: 0.664\n",
            "epoch:  53 loss: 0.0061\n",
            "train_steps: 241\n",
            "Train Epoch: 53 [241/241]\tLambda: 0.0000, Class: 0.004472, CORAL: 0.952289, Total_Loss: 0.004482\n",
            "Train ith Epoch 53 result:\n",
            "train_acc: 0.998\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.663\n",
            "epoch:  54 loss: 0.0059\n",
            "train_steps: 241\n",
            "Train Epoch: 54 [241/241]\tLambda: 0.0000, Class: 0.005947, CORAL: 1.078688, Total_Loss: 0.005958\n",
            "Train ith Epoch 54 result:\n",
            "train_acc: 0.998\n",
            "train_tgt_acc: 0.695\n",
            "test_acc: 0.664\n",
            "epoch:  55 loss: 0.0059\n",
            "train_steps: 241\n",
            "Train Epoch: 55 [241/241]\tLambda: 0.0000, Class: 0.002858, CORAL: 0.408803, Total_Loss: 0.002863\n",
            "Train ith Epoch 55 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.664\n",
            "epoch:  56 loss: 0.0055\n",
            "train_steps: 241\n",
            "Train Epoch: 56 [241/241]\tLambda: 0.0000, Class: 0.003002, CORAL: 1.815207, Total_Loss: 0.003020\n",
            "Train ith Epoch 56 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.663\n",
            "epoch:  57 loss: 0.0055\n",
            "train_steps: 241\n",
            "Train Epoch: 57 [241/241]\tLambda: 0.0000, Class: 0.011232, CORAL: 0.880749, Total_Loss: 0.011241\n",
            "Train ith Epoch 57 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.695\n",
            "test_acc: 0.663\n",
            "epoch:  58 loss: 0.0055\n",
            "train_steps: 241\n",
            "Train Epoch: 58 [241/241]\tLambda: 0.0000, Class: 0.004024, CORAL: 1.387486, Total_Loss: 0.004038\n",
            "Train ith Epoch 58 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.662\n",
            "epoch:  59 loss: 0.0053\n",
            "train_steps: 241\n",
            "Train Epoch: 59 [241/241]\tLambda: 0.0000, Class: 0.003239, CORAL: 1.101689, Total_Loss: 0.003250\n",
            "Train ith Epoch 59 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.663\n",
            "epoch:  60 loss: 0.0050\n",
            "train_steps: 241\n",
            "Train Epoch: 60 [241/241]\tLambda: 0.0000, Class: 0.004699, CORAL: 1.033123, Total_Loss: 0.004709\n",
            "Train ith Epoch 60 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.664\n",
            "epoch:  61 loss: 0.0051\n",
            "train_steps: 241\n",
            "Train Epoch: 61 [241/241]\tLambda: 0.0000, Class: 0.003911, CORAL: 1.219323, Total_Loss: 0.003923\n",
            "Train ith Epoch 61 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.663\n",
            "epoch:  62 loss: 0.0051\n",
            "train_steps: 241\n",
            "Train Epoch: 62 [241/241]\tLambda: 0.0000, Class: 0.002160, CORAL: 1.991351, Total_Loss: 0.002180\n",
            "Train ith Epoch 62 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.663\n",
            "epoch:  63 loss: 0.0048\n",
            "train_steps: 241\n",
            "Train Epoch: 63 [241/241]\tLambda: 0.0000, Class: 0.002621, CORAL: 1.623120, Total_Loss: 0.002637\n",
            "Train ith Epoch 63 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.663\n",
            "epoch:  64 loss: 0.0048\n",
            "train_steps: 241\n",
            "Train Epoch: 64 [241/241]\tLambda: 0.0000, Class: 0.005039, CORAL: 1.590504, Total_Loss: 0.005055\n",
            "Train ith Epoch 64 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.663\n",
            "epoch:  65 loss: 0.0046\n",
            "train_steps: 241\n",
            "Train Epoch: 65 [241/241]\tLambda: 0.0000, Class: 0.008391, CORAL: 1.373919, Total_Loss: 0.008404\n",
            "Train ith Epoch 65 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.662\n",
            "epoch:  66 loss: 0.0045\n",
            "train_steps: 241\n",
            "Train Epoch: 66 [241/241]\tLambda: 0.0000, Class: 0.001764, CORAL: 1.350960, Total_Loss: 0.001777\n",
            "Train ith Epoch 66 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.664\n",
            "epoch:  67 loss: 0.0044\n",
            "train_steps: 241\n",
            "Train Epoch: 67 [241/241]\tLambda: 0.0000, Class: 0.009136, CORAL: 1.631749, Total_Loss: 0.009152\n",
            "Train ith Epoch 67 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.663\n",
            "epoch:  68 loss: 0.0046\n",
            "train_steps: 241\n",
            "Train Epoch: 68 [241/241]\tLambda: 0.0000, Class: 0.000967, CORAL: 1.147372, Total_Loss: 0.000978\n",
            "Train ith Epoch 68 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.664\n",
            "epoch:  69 loss: 0.0040\n",
            "train_steps: 241\n",
            "Train Epoch: 69 [241/241]\tLambda: 0.0000, Class: 0.004648, CORAL: 1.002316, Total_Loss: 0.004658\n",
            "Train ith Epoch 69 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.664\n",
            "epoch:  70 loss: 0.0042\n",
            "train_steps: 241\n",
            "Train Epoch: 70 [241/241]\tLambda: 0.0000, Class: 0.009116, CORAL: 1.345649, Total_Loss: 0.009129\n",
            "Train ith Epoch 70 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.663\n",
            "epoch:  71 loss: 0.0040\n",
            "train_steps: 241\n",
            "Train Epoch: 71 [241/241]\tLambda: 0.0000, Class: 0.001516, CORAL: 1.192662, Total_Loss: 0.001528\n",
            "Train ith Epoch 71 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.664\n",
            "epoch:  72 loss: 0.0039\n",
            "train_steps: 241\n",
            "Train Epoch: 72 [241/241]\tLambda: 0.0000, Class: 0.002990, CORAL: 2.142765, Total_Loss: 0.003012\n",
            "Train ith Epoch 72 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.662\n",
            "epoch:  73 loss: 0.0038\n",
            "train_steps: 241\n",
            "Train Epoch: 73 [241/241]\tLambda: 0.0000, Class: 0.003740, CORAL: 1.553643, Total_Loss: 0.003755\n",
            "Train ith Epoch 73 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.663\n",
            "epoch:  74 loss: 0.0037\n",
            "train_steps: 241\n",
            "Train Epoch: 74 [241/241]\tLambda: 0.0000, Class: 0.003422, CORAL: 0.694181, Total_Loss: 0.003429\n",
            "Train ith Epoch 74 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.662\n",
            "epoch:  75 loss: 0.0038\n",
            "train_steps: 241\n",
            "Train Epoch: 75 [241/241]\tLambda: 0.0000, Class: 0.002479, CORAL: 1.051638, Total_Loss: 0.002490\n",
            "Train ith Epoch 75 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.662\n",
            "epoch:  76 loss: 0.0035\n",
            "train_steps: 241\n",
            "Train Epoch: 76 [241/241]\tLambda: 0.0000, Class: 0.001327, CORAL: 0.944936, Total_Loss: 0.001336\n",
            "Train ith Epoch 76 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.662\n",
            "epoch:  77 loss: 0.0036\n",
            "train_steps: 241\n",
            "Train Epoch: 77 [241/241]\tLambda: 0.0000, Class: 0.005815, CORAL: 2.243035, Total_Loss: 0.005837\n",
            "Train ith Epoch 77 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.663\n",
            "epoch:  78 loss: 0.0032\n",
            "train_steps: 241\n",
            "Train Epoch: 78 [241/241]\tLambda: 0.0000, Class: 0.000652, CORAL: 2.797711, Total_Loss: 0.000680\n",
            "Train ith Epoch 78 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.663\n",
            "epoch:  79 loss: 0.0034\n",
            "train_steps: 241\n",
            "Train Epoch: 79 [241/241]\tLambda: 0.0000, Class: 0.004378, CORAL: 1.702922, Total_Loss: 0.004395\n",
            "Train ith Epoch 79 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.663\n",
            "epoch:  80 loss: 0.0033\n",
            "train_steps: 241\n",
            "Train Epoch: 80 [241/241]\tLambda: 0.0000, Class: 0.005281, CORAL: 2.025180, Total_Loss: 0.005301\n",
            "Train ith Epoch 80 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.663\n",
            "epoch:  81 loss: 0.0034\n",
            "train_steps: 241\n",
            "Train Epoch: 81 [241/241]\tLambda: 0.0000, Class: 0.004806, CORAL: 2.497459, Total_Loss: 0.004831\n",
            "Train ith Epoch 81 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.662\n",
            "epoch:  82 loss: 0.0031\n",
            "train_steps: 241\n",
            "Train Epoch: 82 [241/241]\tLambda: 0.0000, Class: 0.005021, CORAL: 1.472850, Total_Loss: 0.005035\n",
            "Train ith Epoch 82 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.662\n",
            "epoch:  83 loss: 0.0031\n",
            "train_steps: 241\n",
            "Train Epoch: 83 [241/241]\tLambda: 0.0000, Class: 0.003713, CORAL: 1.774131, Total_Loss: 0.003730\n",
            "Train ith Epoch 83 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.694\n",
            "test_acc: 0.662\n",
            "epoch:  84 loss: 0.0029\n",
            "train_steps: 241\n",
            "Train Epoch: 84 [241/241]\tLambda: 0.0000, Class: 0.002393, CORAL: 2.412585, Total_Loss: 0.002417\n",
            "Train ith Epoch 84 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.660\n",
            "epoch:  85 loss: 0.0030\n",
            "train_steps: 241\n",
            "Train Epoch: 85 [241/241]\tLambda: 0.0000, Class: 0.006325, CORAL: 3.653134, Total_Loss: 0.006362\n",
            "Train ith Epoch 85 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.662\n",
            "epoch:  86 loss: 0.0030\n",
            "train_steps: 241\n",
            "Train Epoch: 86 [241/241]\tLambda: 0.0000, Class: 0.001269, CORAL: 1.297031, Total_Loss: 0.001282\n",
            "Train ith Epoch 86 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.661\n",
            "epoch:  87 loss: 0.0028\n",
            "train_steps: 241\n",
            "Train Epoch: 87 [241/241]\tLambda: 0.0000, Class: 0.002646, CORAL: 2.543433, Total_Loss: 0.002671\n",
            "Train ith Epoch 87 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.662\n",
            "epoch:  88 loss: 0.0026\n",
            "train_steps: 241\n",
            "Train Epoch: 88 [241/241]\tLambda: 0.0000, Class: 0.002617, CORAL: 2.479493, Total_Loss: 0.002642\n",
            "Train ith Epoch 88 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.660\n",
            "epoch:  89 loss: 0.0030\n",
            "train_steps: 241\n",
            "Train Epoch: 89 [241/241]\tLambda: 0.0000, Class: 0.001830, CORAL: 2.366967, Total_Loss: 0.001854\n",
            "Train ith Epoch 89 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.661\n",
            "epoch:  90 loss: 0.0028\n",
            "train_steps: 241\n",
            "Train Epoch: 90 [241/241]\tLambda: 0.0000, Class: 0.000636, CORAL: 2.803492, Total_Loss: 0.000664\n",
            "Train ith Epoch 90 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.661\n",
            "epoch:  91 loss: 0.0025\n",
            "train_steps: 241\n",
            "Train Epoch: 91 [241/241]\tLambda: 0.0000, Class: 0.001149, CORAL: 2.610420, Total_Loss: 0.001175\n",
            "Train ith Epoch 91 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.662\n",
            "epoch:  92 loss: 0.0026\n",
            "train_steps: 241\n",
            "Train Epoch: 92 [241/241]\tLambda: 0.0000, Class: 0.001170, CORAL: 1.266439, Total_Loss: 0.001183\n",
            "Train ith Epoch 92 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.662\n",
            "epoch:  93 loss: 0.0025\n",
            "train_steps: 241\n",
            "Train Epoch: 93 [241/241]\tLambda: 0.0000, Class: 0.000498, CORAL: 1.288442, Total_Loss: 0.000511\n",
            "Train ith Epoch 93 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.662\n",
            "epoch:  94 loss: 0.0026\n",
            "train_steps: 241\n",
            "Train Epoch: 94 [241/241]\tLambda: 0.0000, Class: 0.003140, CORAL: 2.259589, Total_Loss: 0.003162\n",
            "Train ith Epoch 94 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.661\n",
            "epoch:  95 loss: 0.0025\n",
            "train_steps: 241\n",
            "Train Epoch: 95 [241/241]\tLambda: 0.0000, Class: 0.000922, CORAL: 2.264854, Total_Loss: 0.000945\n",
            "Train ith Epoch 95 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.663\n",
            "epoch:  96 loss: 0.0026\n",
            "train_steps: 241\n",
            "Train Epoch: 96 [241/241]\tLambda: 0.0000, Class: 0.002592, CORAL: 1.149829, Total_Loss: 0.002603\n",
            "Train ith Epoch 96 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.660\n",
            "epoch:  97 loss: 0.0023\n",
            "train_steps: 241\n",
            "Train Epoch: 97 [241/241]\tLambda: 0.0000, Class: 0.003754, CORAL: 1.121355, Total_Loss: 0.003765\n",
            "Train ith Epoch 97 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.661\n",
            "epoch:  98 loss: 0.0025\n",
            "train_steps: 241\n",
            "Train Epoch: 98 [241/241]\tLambda: 0.0000, Class: 0.002411, CORAL: 2.187015, Total_Loss: 0.002433\n",
            "Train ith Epoch 98 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.692\n",
            "test_acc: 0.662\n",
            "epoch:  99 loss: 0.0024\n",
            "train_steps: 241\n",
            "Train Epoch: 99 [241/241]\tLambda: 0.0000, Class: 0.000521, CORAL: 1.159617, Total_Loss: 0.000533\n",
            "Train ith Epoch 99 result:\n",
            "train_acc: 0.999\n",
            "train_tgt_acc: 0.693\n",
            "test_acc: 0.662\n",
            "epoch: 100 loss: 0.0025\n",
            "Accuracy: 0.662\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0YAAAEWCAYAAABCNYfGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdeZxcdZX//9eppffOHpaQhAQmQCAkgYQYQJRFMCITwI3o4LgN6HfEZRwzBocvKH5VZlxG+Sn6AGVUXEBhhkFhZBmJMsoWYoQQEpJAQva9O93prZbz++PeqrpdvaQSurs63e/n43Efd7/33KWq7rmfez9l7o6IiIiIiMhwFit3ACIiIiIiIuWmxEhERERERIY9JUYiIiIiIjLsKTESEREREZFhT4mRiIiIiIgMe0qMRERERERk2FNiJCIyyJjZF8zsp/24/BfN7Pyw28zs381sn5k9Y2bnmdmafljnZDNrNrN4Xy97uDCzDWb2lh7G9ctxExEZTpQYiYiUgZm9z8yWhcnCNjP7bzN740Cs291Pc/elYe8bgYuBie4+z92fcPeTX+86ii/i3f01d69z98zrXbZ0Vepx6++kW0TkSKbESERkgJnZZ4BvAV8BjgYmA7cBl5chnOOBDe5+oAzrPuKZWaLcMQyk4ba9IjK8KDESERlAZjYSuBn4uLv/h7sfcPeUu//a3Rf3MM+vzGy7mTWa2R/M7LTIuEvNbJWZNZnZFjP7bDh8nJn9xswazGyvmT1hZrFw3AYze4uZfQT4AXB2WHL1RTM738w2R5Y/ycz+w8x2mdkeM/tOOPxEM/tdOGy3mf3MzEaF4+4iSPZ+HS73n8xsipl57sLazCaY2QNhbOvM7JrIOr9gZr80s5+E2/Wimc3tZZ9+28w2mdl+M3vOzM6LjIub2efNbH24rOfMbFI47jQzezSMYYeZfT4c/iMz+3+RZRTvkw1m9jkzex44YGYJM1sSWccqM7uyKMZrzOylyPgzzWyxmd1XNN2tZvbtnrYVmG1mz4fnwj1mVtVDjJ8Lz4cmM1tjZheZ2QLg88BV4XH5S4nH4l4z+6mZ7QeWmFmLmY2NTHNmeH4ke4lbRGTQU2IkIjKwzgaqgP88hHn+G5gGHAUsB34WGfdD4KPuXg/MAH4XDv9HYDMwnqBU6vOARxfq7j8EPgY8GT7mdlN0vAXvA/0G2AhMAY4D7s6NBr4KTACmA5OAL4TLfT/wGvDX4XL/tZttujuMbwLwLuArZnZhZPzCcJpRwAPAd3rZP88Cs4ExwM+BX+USBuAzwHuBS4ERwIeBFjOrBx4DfhvG8FfA//SyjmLvBd4OjHL3NLAeOA8YCXwR+KmZHQtgZu8m2Dd/G8awENgD/BRYEEkoE8Ai4Ce9rPc9wAJgKjAT+GDxBGZ2MnAdcFZ4XryVoFTwtwSllPeEx2VWOMvBjsXlwL0Ex+IbwNIwjpz3A3e7e6qXuEVEBj0lRiIiA2sssDu8mC6Ju9/p7k3u3k5wgT3LgpIngBRwqpmNcPd97r48MvxY4PiwROoJd/euS+/VPIKL5cVhyVabu/9vGNM6d3/U3dvdfRfwTeDNpSw0LLE5F/hcuMwVBCVXfxuZ7H/d/aHwnaS7gFndLIowlp+6+x53T7v7N4BKIPe+zd8BN7j7Gg/8xd33AJcB2939G2EMTe7+9CHsm1vdfZO7t4Yx/Mrdt7p71t3vAdYS7L9cDP/q7s+GMaxz943uvg34A/DucLoFBOfGcwdZ71Z33wv8miAhLJYJ98GpZpZ09w3uvr67hZV4LJ509/vDbWsFfgxcHc4fJ0gS7+p1b4mIHAGUGImIDKw9wDgr8V2N8FGwW8LHtPYDG8JR48L2OwlKQzaa2e/N7Oxw+NeAdcAjZvaKmS05jFgnARu7S+LM7Ggzuzt8XGs/QenHuC5L6N4EYK+7N0WGbSQokcrZHuluAap62mdm9tnwMbVGM2sgKLXJxTKJoDSnu23rNlko0aaiGP7WzFZY8OhiA0Hp3cFigEiSEbYPlmAU75e64gncfR3waYIkemd4nCb0sLxSjsWmzrPwXwRJ11SCijsa3f2Zg8QtIjLoKTESERlYTwLtwBUlTv8+gkeZ3kJwwT8lHG4AYSnE5QSP2d0P/DIc3uTu/+juJxA8uvUZM7voEGPdBEzuISH5CsGjeae7+wiCi3qLjO+tdGorMCZ8nC1nMrDlEOMjfJ/onwge7Rrt7qOAxkgsm4ATu5l1E3BCD4s9ANRE+o/pZpr89pnZ8cAdBI+vjQ1jWFlCDBAcs5lmNoOgFOtnPUx3SNz95+7+RoLKNRz4l+K4Q6Uci+JHMNsIzrOrCR6jU2mRiAwJSoxERAaQuzcCNwLfNbMrzKzGzJJm9jYz6+5dnHqCRGoPwcX6V3IjzKzCzP7GzEaG73fsB7LhuMvM7K/MzAgShUxu3CF4BtgG3GJmtWZWZWbnRuJqBhrN7DiguOKIHfSQeLj7JuBPwFfDZc4EPkJQ6nSo6oE0sAtImNmNBO/x5PwA+JKZTbPAzLDigN8Ax5rZp82s0szqzewN4TwrgEvNbIyZHUNQ+tKbWoLkYReAmX2IoMQoGsNnzWxOGMNfhclULsm4l+DdqGfc/bXD2AedmNnJZnahmVUCbUArhWO/A5hiYUUcr+NY/ITg/aaFKDESkSFCiZGIyAAL34P5DHADwcX0JoLShvu7mfwnBI82bQFWAU8VjX8/sCF8nO1jwN+Ew6cRVC7QTFBKdZu7P36IcWaAvyaomOA1ghf0rwpHfxE4kyDpehD4j6LZvwrcED5a9tluFv9egtKvrQQVUdzk7o8dSnyhhwkqUHiZYD+10fnRr28SlG48QpA4/hCoDh8duzjcvu0E7wRdEM5zF/AXgscWHwHu6S0Ad19FUCnBkwSJx+nAHyPjfwV8mSD5aSI4zmMii/hxOE9fJRiVwC3AboJtOwq4Phz3q7C9x8xy76Md8rFw9z8SJFvL3X1jH8UtIlJWdujv4oqIiEhfMbPJwGrgGHffX+54SmVmvwN+7u4/KHcsIiJ9QYmRiIhImYSPtH0TGOHuHy53PKUys7OAR4FJRRU3iIgcsfQP1iIiImVgZrUEj95tJKiq+4hgZj8mqDzkU0qKRGQoUYmRiIiIiIgMe6p8QUREREREhr0h8yjduHHjfMqUKeUOQ0REREREBrHnnntut7uPLx4+ZBKjKVOmsGzZsnKHISIiIiIig5iZdfs3A3qUTkREREREhj0lRiIiIiIiMuwpMRIRERERkWFPiZGIiIiIiAx7SoxERERERGTY67fEyMzuNLOdZrayh/FmZrea2Toze97MzoyM+4CZrQ2bD/RXjCIiIiIiItC/JUY/Ahb0Mv5twLSwuRb4HoCZjQFuAt4AzANuMrPR/RiniIiIiIgMc/32P0bu/gczm9LLJJcDP3F3B54ys1FmdixwPvCou+8FMLNHCRKsX/RXrCIiIv3J3XGHrDsOuIMTDCtME7bD4U44vfc8fzhDtNVpnBeNyzHADAwL28HA3DqyDtmsR/qDYUSW2yWObre787ZGtyHrPc/X3XJ6YtZ5u3JR5vdbpDubi7co/tz2lCq3nui6Dz1WC2MN44oe48hxgFx/11gNiFlwDGPhQcwdz9z2EZknt73R9TlONls43wrnk/d4/hTvh562rdMERfEfwuHvc532o0f2B3Tav36QIC3c6NxmFh+73HqywYEMjk1unsjnr/NnpPPxDpZv+f1r3SwnHw/dnJB0/Yzmj6t3PTdyYhYsLRYrrCdm1uW7K7oMoOv4cGBdVYJr33Rir/tzMCjnH7weB2yK9G8Oh/U0vAszu5agtInJkyf3T5QickTIZJ10Ntvp4it/EZktXIRFf/CiX+RZd9JZJ5NxUtlssLyM55ebDafJZJ1s1slEfkx74g6pTJZ01oN2JlhWKlxuJuudLjozWS9cvBVd3HpkobkLmFxMuR/eXD90/oH2cHuzWYK4w/hz689kO1+gd/dD22k50R/+ou3vdKEfXnB1t42ZyIS5dVjRtVSn7YtcxGWyhWXltiEdHpfoxbZFNqTzBXPni+Xc9X50+6IX8p0v5jyynB4PvYiIRBw7skqJUX9z99uB2wHmzp2rnyiRXrg7HZksHeks7emg3ZHO0pHJ0p7K5sfl2+ksHZlMvrs9Ml/QnSGVyQKdL2wLd1KNVCYbNsG6U+lCfzpMPvKNB4lI/kKdzglONhtsQzqcPpdwpMNEZjhdpMbCO3eFO8VGLLyDmOvPt4kOh7gZsZgRj1mh26zXO9+Oh8u0yLqB3B3FonmjCUlu+kQsRjwWrCces3xsxXdmo8lqPFaIP9i+wnYHcQd3MxPh9kT3SXGClltmcSlJ8d3b3Hpy+yw3bW4bi+/cdrvjindifj9F19v1jm/xsvPTR/azRYblYyhaRmF7uh9XfNc+2h+PHOP8/o5F9plZp+V2KnHqQedtKCyXg8zndB5v3ezraGLu0Y5O53/h2NLN8Y8ek94j6rymzudYz6UnhViLlxAMi55zsaL9lI89Vhx3Yf7cjZLoDYRoHIVj1/l87vaYQJfzq/j8KWxDZN93u22dx0fPg2gsA63wPUDRedI5tnyhV9ENGyi6MRL9nsx/D0X3tRUdr2hpS9BffIyix6ewvs43ZYpLnHvazp50912Y2/7uzivC/k7fUz19n0Kn8+hIUs7EaAswKdI/MRy2heBxuujwpQMWlUgfyGadtnSGtlSWtlQmbLK0pTO0h+22jgytqbDpCKZpDadLRZKUVMbpSGdIZTw/PFcCkco46UjyEZRuOJlo0uG5RKLvMoeKeIzKRIxkovCaYnEJjLtTkYiRjOcaIxmPUZGIkYgZiXiMRCxGVdLCC+fCRXruAjr3AxO9KI/HjEQ8uNDOLSeZ648XJQpFP0yFC7quF4/xcHnR5cdz/WFsxYlF9AevOwYk47H88pLxMN5cYpJfbiE5ySUD0YuUaMwiIjIcDdxvQO7nJj6A6xwsypkYPQBcZ2Z3E1S00Oju28zsYeArkQoXLgGuL1eQMvS5O+3pLC0dGQ60pznQkeZAe4aWsN3cnqa5LUVze5qm9jQH2tM0t6WD4e1pWjsytIRNayqYry2VPaxYYgbVyXinhKIy152w/LCqZIz6qgSJWIyKRCEpSMZixOOF5CJ6UZ9bVkU8RmUyaFckwibSHUyTi8HCYfHCPPFY/pljERERkaGi3xIjM/sFQcnPODPbTFDTXBLA3b8PPARcCqwDWoAPheP2mtmXgGfDRd2cq4hBJKotlaGhJcW+lg4aWlI0tnawvzXN/rYU+9vS7G9NBd2taZraUrSls7SnMrSng1KcXLstlQlfcD24ikSM+soEtWFTVxlnZE0FE0bFqa6IU1MRp6YiQXUy6K9KxKhKxsMmRmUyTlUi6K6uiAfTJeNUVQTDk3FTyYCIiIhIGdjBatw4UsydO9eXLVtW7jDkMLk7+1pSbNrbwu7mdva1pGho6WBfS0eh+0CKhtbC8N5KZcygrjLBiKokI6qT1FcmqKqIU5lLVMJ2rr+mMk5dZYKaigS1FXFqKoN2dUWc+sokdVUJaivjVCbiA7hXRERERKSvmdlz7j63ePgRXfmCHDncncbWFFsb2tjW2MqmvS1s2he0X9vbwuZ9rTS3p7vMF48Zo6qTjKpJMrqmguNGVXHahBGMrkkyqqYiP3xUTZKR1UEzojpJXUVCj3uJiIiISMmUGEmfcHd2NbWzcW8LG/cEyc7Whla2NbayraGNbY1ttKYyneapSsaYPKaGSaNrmH/CWCaNqWHS6GqOHlHFqDDxqa9UgiMiIiIi/U+JkRySdCbL+l0HWLmlkdXb97NhTwuvhYlQNPGJGRxVX8Wxo6o45dh6LjjlKI4dWcWEUdUcO7KKiaNrGFdXofdpRERERGRQUGIkPUpnsqzd2cwLWxpZGTartu3Pv9tTmQhKfI4fW8Mbp43j+LE1YX8tx42qpiJSlbOIiIiIyGCmxEjyWjrSrHitgWc37GPZxr0s37iPAx1BKVBtRZzTJozkffOOZ8ZxIzj9uJGcML6OuB5zExEREZEhQInRMJbKZPnjut38cd1untmwjxe3NJLOOmZw8tH1vOPMicw5fjQzjhvJCeNq9a6PiIiIiAxZSoyGmXQmy5Ov7OHB57fx2xe309CSoiIeY9akkVz7phM4a8oYzpw8mpE1yXKHKiIiIiIyYJQYDQOZrPP0q3v4zfPb+O3K7ew90EFtRZyLTz2at8+cwHnTxlGV1P/ziIiIiMjwpcRoCNvZ1Mbdz2ziZ09vZMf+dqqTcS6afhSXzZzA+SePVzIkIiIiIhJSYjTEuDt/3tTAT/60gQdf2EYq47zppPHceNkkLjzlKKorlAyJiIiIiBRTYjREtKUy/Ob5bfz4Txt4YUsj9ZUJrp5/PO+ffzwnjK8rd3giIiIiIoOaEqMh4OEXt/PP/7mS3c3tTDuqji9dMYMrzziOukodXhERERGRUujK+QjW1Jbii79exb3PbWbGcSP49qLZnHPiWMxUrbaIiIiIyKFQYnSEeuqVPfzjL//CtsZWPnHhX/GJC6dRkYiVOywRERERkSOSEqMjTFsqwzcffZk7nniF48fU8KuPncOc40eXOywRERERkSOaEqMjyKqt+/mHe1awZkcTf/OGyXz+0unU6j0iEREREZHXTVfVR4g/vLyLj/z4WUbVVPDvHzyLC045qtwhiYiIiIgMGf36UoqZLTCzNWa2zsyWdDP+eDP7HzN73syWmtnEyLiMma0Imwf6M87BbtPeFj559585cXwdD3/6TUqKRERERET6WL+VGJlZHPgucDGwGXjWzB5w91WRyb4O/MTdf2xmFwJfBd4fjmt199n9Fd+Roi2V4aN3PUcm63z/6jmMqa0od0giIiIiIkNOf5YYzQPWufsr7t4B3A1cXjTNqcDvwu7Huxk/rLk7n//PF1i1bT/fXjSbKeNqyx2SiIiIiMiQ1J+J0XHApkj/5nBY1F+Ad4TdVwL1ZjY27K8ys2Vm9pSZXdHdCszs2nCaZbt27erL2AeFnz61kf9YvoVPv2UaF55ydLnDEREREREZssr9xzefBd5sZn8G3gxsATLhuOPdfS7wPuBbZnZi8czufru7z3X3uePHjx+woAfCcxv38sVfr+KiU47ikxdOK3c4IiIiIiJDWn/WSrcFmBTpnxgOy3P3rYQlRmZWB7zT3RvCcVvC9itmthQ4A1jfj/EOGjub2vg/P13OcaOr+eZVs4nFrNwhiYiIiIgMaf1ZYvQsMM3MpppZBbAI6FS7nJmNM7NcDNcDd4bDR5tZZW4a4FwgWmnDkJXKZPn4z5bT1Jbm+1fPYWR1stwhiYiIiIgMef2WGLl7GrgOeBh4Cfilu79oZjeb2cJwsvOBNWb2MnA08OVw+HRgmZn9haBShluKarMbsr784Es8u2Eft7zzdKYfO6Lc4YiIiIiIDAv9+gev7v4Q8FDRsBsj3fcC93Yz35+A0/sztsHo4Re386M/beDD507l8tnF9VSIiIiIiEh/KXflCxJyd/7t0Zf5q6PquP7SU8odjoiIiIjIsKLEaJD43eqdrN7exN+ffyLJuA6LiIiIiMhA0hX4IODufOfxdRw3qpq/njWh3OGIiIiIiAw7SowGgade2cufX2vgY28+QaVFIiIiIiJloKvwQeC2pesYV1fJu+dOOvjEIiIiIiLS55QYldnzmxt4Yu1uPvLGqVQl4+UOR0RERERkWFJiVGa3Pb6e+qoEV8+fXO5QRERERESGLSVGZbRuZxO/fXE7HzxnCvVVyXKHIyIiIiIybCkxKqPvLX2F6mScD507tdyhiIiIiIgMa0qMymTT3hbuX7GFRfMmMaa2otzhiIiIiIgMa0qMyuSOJ14hZnDNeSeUOxQRERERkWFPiVEZ7Gpq555nN/GOMyYyYVR1ucMRERERERn2lBiVwZ1/fJWOTJaPvlmlRSIiIiIig4ESowHW2Jriric3cunpx3LC+LpyhyMiIiIiIpSQGJnZ2IEIZLj42dMbaW5P8/fnn1juUEREREREJFRKidFTZvYrM7vUzKzfIxrinly/h1OPHcFpE0aWOxQREREREQmVkhidBNwOvB9Ya2ZfMbOT+jesoWvN9iZOOba+3GGIiIiIiEjEQRMjDzzq7u8FrgE+ADxjZr83s7N7m9fMFpjZGjNbZ2ZLuhl/vJn9j5k9b2ZLzWxiZNwHzGxt2HzgMLZt0Nl3oIOdTe2ccowSIxERERGRwaSkd4zM7FNmtgz4LPAJYBzwj8DPe5kvDnwXeBtwKvBeMzu1aLKvAz9x95nAzcBXw3nHADcBbwDmATeZ2ehD3LZBZ82OJgBOPmZEmSMREREREZGoUh6lexIYAVzh7m939/9w97S7LwO+38t884B17v6Ku3cAdwOXF01zKvC7sPvxyPi3Ao+6+1533wc8CiwobZMGrzXbw8ToaJUYiYiIiIgMJqUkRie7+5fcfXPxCHf/l17mOw7YFOnfHA6L+gvwjrD7SqA+rAWvlHkxs2vNbJmZLdu1a9fBt6TMVm9vYmR1kqNHVJY7FBERERERiSglMXrEzEblesxstJk93Efr/yzwZjP7M/BmYAuQKXVmd7/d3ee6+9zx48f3UUj95+UdTZx8TD2q3E9EREREZHApJTEa7+4NuZ7w0bajSphvCzAp0j8xHJbn7lvd/R3ufgbwz+GwhlLmPdK4Oy9vb9JjdCIiIiIig1ApiVHGzCbneszseMBLmO9ZYJqZTTWzCmAR8EB0AjMbZ2a5GK4H7gy7HwYuCUunRgOXhMOOWFsaWmlqT3OyaqQTERERERl0EiVM88/A/5rZ7wEDzgOuPdhM7p42s+sIEpo4cKe7v2hmNwPL3P0B4Hzgq2bmwB+Aj4fz7jWzLxEkVwA3u/veQ9u0weXlsEY6VdUtIiIiIjL4HDQxcvffmtmZwPxw0KfdfXcpC3f3h4CHiobdGOm+F7i3h3nvpFCCdMRbHdZIN02P0omIiIiIDDqllBhBUCHCTqAKONXMcPc/9F9YQ8+a7U1MGFnFyOpkuUMREREREZEiB02MzOzvgE8RVICwgqDk6Engwv4NbWhZs71J7xeJiIiIiAxSpVS+8CngLGCju18AnAE09D6LRKUyWdbvauYkJUYiIiIiIoNSKYlRm7u3AZhZpbuvBk7u37CGlld3HyCVcVW8ICIiIiIySJXyjtHm8A9e7wceNbN9wMb+DWtoWRNWvHDy0SPKHImIiIiIiHSnlFrprgw7v2BmjwMjgd/2a1RDzJrtTcRjxolH1ZY7FBERERER6UaviZGZxYEX3f0UAHf//YBENcSs3t7E1HG1VCbi5Q5FRERERES60es7Ru6eAdaY2eQBimdIenlHEyfr/4tERERERAatUt4xGg28aGbPAAdyA919Yb9FNYQcaE/z2t4W3jVnYrlDERERERGRHpSSGP3ffo9iCHt5R1jxgmqkExEREREZtEqpfEHvFb0O+cRIj9KJiIiIiAxaB02MzKwJ8LC3AkgCB9xddU+XYPX2JqqTcSaPqSl3KCIiIiIi0oNSSozyRR1mZsDlwPz+DGooWbO9iZOOriMWs3KHIiIiIiIiPei1VrpiHrgfeGs/xTPkvLyjiZP0GJ2IiIiIyKBWyqN074j0xoC5QFu/RTSE7G5uZ3dzhypeEBEREREZ5Eqple6vI91pYAPB43RyEGu2BxUvnHKMXscSERERERnMSnnH6EMDEchQlEuMTjqmrsyRiIiIiIhIb0p5lO7HwKfcvSHsHw18w90/XMK8C4BvA3HgB+5+S9H4ycCPgVHhNEvc/SEzmwK8BKwJJ33K3T9W6kYNFmu2NzGmtoLxdZXlDkVEREREjnCpVIrNmzfT1qa3WkpRVVXFxIkTSSaTJU1fyqN0M3NJEYC77zOzMw42k5nFge8CFwObgWfN7AF3XxWZ7Abgl+7+PTM7FXgImBKOW+/us0vaikFq9Y4mTj66nqAyPxERERGRw7d582bq6+uZMmWKri8Pwt3Zs2cPmzdvZurUqSXNU0qtdLGwlAgAMxtDaQnVPGCdu7/i7h3A3XR9N8mB3As4I4GtJSz3iJDNOmt3NKniBRERERHpE21tbYwdO1ZJUQnMjLFjxx5S6VopCc43gCfN7Fdh/7uBL5cw33HApkj/ZuANRdN8AXjEzD4B1AJviYybamZ/BvYDN7j7E8UrMLNrgWsBJk+eXEJIA2fzvlZaOjJKjERERESkzygpKt2h7quDlhi5+0+AdwA7wuYd7n7XYUXX1XuBH7n7ROBS4C4ziwHbgMnufgbwGeDnZtalajd3v93d57r73PHjx/dRSH1j9fb9AEqMRERERESOAAdNjMxsPrDJ3b/j7t8BNptZcclPd7YAkyL9E8NhUR8Bfgng7k8CVcA4d2939z3h8OeA9cBJJaxz0Hh5R1gjnf7cVURERESGgIaGBm677bbDmvfSSy+loaHh4BOWUSnvGH0PaI70N4fDDuZZYJqZTTWzCmAR8EDRNK8BFwGY2XSCxGiXmY0PK2/AzE4ApgGvlLDOQWP19iYmjq6mrrKUpxVFRERERAa33hKjdDrd67wPPfQQo0aN6rNYDra+w1FKYmTu7rked89S2v8fpYHrgIcJqt7+pbu/aGY3m9nCcLJ/BK4xs78AvwA+GK7rTcDzZrYCuBf4mLvvPZQNK7c125s4RY/RiYiIiMgQsWTJEtavX8/s2bNZvHgxS5cu5bzzzmPhwoWceuqpAFxxxRXMmTOH0047jdtvvz0/75QpU9i9ezcbNmxg+vTpXHPNNZx22mlccskltLa2dlnXr371K2bMmMGsWbN405veBMCPfvQjFi5cyIUXXshFF11Ec3MzH/rQhzj99NOZOXMm99133+vavlKKM14xs09SKCX6e0osvXH3hwiq4I4OuzHSvQo4t5v57gNe35aVUXs6w6u7D3DxqUeXOxQRERERGYK++OsXWbV1f58u89QJI7jpr0/rcfwtt9zCypUrWbFiBQBLly5l+fLlrFy5Ml8l9p133smYMWNobW3lrLPO4p3vfCdjx47ttJy1a9fyi1/8gjvuuIP3vOc93HfffVx99dWdprn55pt5+OGHOe644zo9grd8+XKef6JkmiYAACAASURBVP55xowZw+c+9zlGjhzJCy+8AMC+ffte1/aXUmL0MeAcgveDcjXLXfu61jrEvbLrAOmsq+IFERERERnS5s2b1+l/gm699VZmzZrF/Pnz2bRpE2vXru0yz9SpU5k9O/i70jlz5rBhw4Yu05x77rl88IMf5I477iCTyeSHX3zxxYwZMwaAxx57jI9//OP5caNHj+6ynENRyiNxOwneD5ISrdkeVLygxEhERERE+kNvJTsDqba2Nt+9dOlSHnvsMZ588klqamo4//zzu/0focrKynx3PB7v9lG673//+zz99NM8+OCDzJkzh+eee67L+vpaKbXSVZnZx83sNjO7M9f0W0RDwJodTSRixgnj6sodioiIiIhIn6ivr6epqanH8Y2NjYwePZqamhpWr17NU089ddjrWr9+PW94wxu4+eabGT9+PJs2beoyzcUXX8x3v/vdfP9APEp3F3AM8Fbg9wTVbve8R4Q125s4cXwdFYlSdq+IiIiIyOA3duxYzj33XGbMmMHixYu7jF+wYAHpdJrp06ezZMkS5s+ff9jrWrx4MaeffjozZszgnHPOYdasWV2mueGGG9i3b1++kobHH3/8sNcHQY1zvU9g9md3P8PMnnf3mWaWBJ5w98Pf0n4wd+5cX7ZsWbnDAOD//PQ5RlYnueWdM8sdioiIiIgMES+99BLTp08vdxhHlO72mZk95+5zi6ctpVa6VNhuMLMZwHbgqNcd5RD2vavnlDsEERERERE5BKUkRreb2WjgBoI/aK0D/m+/RiUiIiIiIjKASqmV7gdh5x+AE/o3HBERERERkYGn2gFERERERGTYU2IkIiIiIiLDnhIjEREREREZ9kpKjMzsHDN7n5n9ba7p78BERERERGTwaGho4LbbbjuseS+99FIaGhpKnv7+++9n1apVB51uxYoVPPTQQ4cVU7GDJkZmdhfwdeCNwFlh06XebxERERERGbp6S4zS6XSv8z700EOMGjWq5HUNysSIIAk6193/3t0/ETaf7JO1i4iIiIjIEWHJkiWsX7+e2bNns3jxYpYuXcp5553HwoULOfXUUwG44oormDNnDqeddhq33357ft4pU6awe/duNmzYwPTp07nmmms47bTTuOSSS2htbe20nj/96U888MADLF68mNmzZ7N+/XqeffZZZs6cmV/3jBkz6Ojo4MYbb+See+5h9uzZ3HPPPa9r+0r5H6OVwDHAtte1JhERERER6Rv/vQS2v9C3yzzmdHjbLT2OvuWWW1i5ciUrVqwAYOnSpSxfvpyVK1cydepUAO68807GjBlDa2srZ511Fu985zsZO3Zsp+WsXbuWX/ziF9xxxx285z3v4b777uPqq6/Ojz/nnHNYuHAhl112Ge9617sAuPzyy7njjjs4++yzWbJkCQAVFRXcfPPNLFu2jO985zuve/NLKTEaB6wys4fN7IFc87rXLCIiIiIiR7R58+blkyKAW2+9lVmzZjF//nw2bdrE2rVru8wzdepUZs+eDcCcOXPYsGFDr+toaGigqamJs88+G4D3ve99fbcBEaWUGH2hX9YsIiIiIiKHp5eSnYFUW1ub7166dCmPPfYYTz75JDU1NZx//vm0tbV1maeysjLfHY/HuzxKVy4HLTFy998Dq4H6sHkpHCYiIiIiIsNEfX09TU1NPY5vbGxk9OjR1NTUsHr1ap566qk+WdeoUaOor6/n6aefBuDuu+8uOaZDUUqtdO8BngHeDbwHeNrM3lXKws1sgZmtMbN1Zrakm/GTzexxM/uzmT1vZpdGxl0fzrfGzN5a+iaJiIiIiEhfGzt2LOeeey4zZsxg8eLFXcYvWLCAdDrN9OnTWbJkCfPnzz/sdS1atIivfe1rnHHGGaxfv54f/vCHXHPNNcyePZsDBw4wcuRIAC644AJWrVrVJ5UvmLv3PoHZX4CL3X1n2D8eeMzdZx1kvjjwMnAxsBl4Fnivu6+KTHM78Gd3/56ZnQo85O5Twu5fAPOACcBjwEnunulpfXPnzvVly5YddINFRERERI5EL730EtOnTy93GGXR3NxMXV0dEFQCsW3bNr797W8fdL7u9pmZPefuXf5+qJTKF2K5pCi0p8T55gHr3P0Vd+8A7gYuL5rGgRFh90hga9h9OXC3u7e7+6vAunB5IiIiIiIyzDz44IPMnj2bGTNm8MQTT3DDDTf0+TpKqXzht2b2MEEJDsBVQCn/onQcsCnSvxl4Q9E0XwAeMbNPALXAWyLzRh9K3BwO68TMrgWuBZg8eXIJIYmIiIiIyJHmqquu4qqrrurXdZRS+cJi4HZgZtjc7u6f66P1vxf4kbtPBC4F7jKzUkqjcrHd7u5z3X3u+PHj+ygkEREREREZbkopMcLd7wPuO8RlbwEmRfonhsOiPgIsCNfxpJlVEfxvUinzioiIiIiI9IkeS2fM7H/DdpOZ7Y80TWa2v4RlPwtMM7OpZlYBLAKK/xj2NeCicD3TgSpgVzjdIjOrNLOpwDSCmvFERERERET6XI8lRu7+xrBdfzgLdve0mV0HPAzEgTvd/UUzuxlY5u4PAP8I3GFm/0BQEcMHPagm70Uz+yWwCkgDH++tRjoREREREZHXo5T/MbqrlGHdcfeH3P0kdz/R3b8cDrsxTIpw91Xufq67z3L32e7+SGTeL4fznezu/136JomIiIiISF9raGjgtttuO+z5v/Wtb9HS0nLI46Luv/9+Vq1addDpDkcpFR2cFu0xswQwp1+iERERERGRQWnYJkZmdr2ZNQEzo+8XATuA/+qXaEREREREZFBasmQJ69evZ/bs2SxevBiAr33ta5x11lnMnDmTm266CYADBw7w9re/nVmzZjFjxgzuuecebr31VrZu3coFF1zABRdc0Gm53Y374Q9/yEknncS8efO45ppruO666/jTn/7EAw88wOLFi5k9ezbr16/v0+3r7R2jrwJfNbOvuvv1fbpWERERERE5bP/yzL+weu/qPl3mKWNO4XPzev5XnltuuYWVK1eyYsUKAB555BHWrl3LM888g7uzcOFC/vCHP7Br1y4mTJjAgw8+CEBjYyMjR47km9/8Jo8//jjjxo3rtNxPfvKTncZt3bqVL33pSyxfvpz6+nouvPBCZs2axTnnnMPChQu57LLLeNe73tWn2w6l/Y/R9WY22szmmdmbck2fRyIiIiIiIkeMRx55hEceeYQzzjiDM888k9WrV7N27VpOP/10Hn30UT73uc/xxBNPMHLkyENa7jPPPMOb3/xmxowZQzKZ5N3vfnc/bUFnB/0fIzP7O+BTBP8ltAKYDzwJXNi/oYmIiIiISHd6K9kZKO7O9ddfz0c/+tEu45YvX85DDz3EDTfcwEUXXcSNN95YhggPTSmVL3wKOAvY6O4XAGcADf0alYiIiIiIDCr19fU0NTXl+9/61rdy55130tzcDMCWLVvYuXMnW7dupaamhquvvprFixezfPnybufvadlnnXUWv//979m3bx/pdJr77ruvxxj60kFLjIA2d28zM8ys0t1Xm9nJ/RKNiIiIiIgMSmPHjuXcc89lxowZvO1tb+NrX/saL730EmeffTYAdXV1/PSnP2XdunUsXryYWCxGMpnke9/7HgDXXnstCxYsYMKECTz++OOdll087vOf/zzz5s1jzJgxnHLKKfnH8RYtWsQ111zDrbfeyr333suJJ57YZ9tnwf+p9jKB2X8CHwI+TfD43D4g6e6X9lkUfWDu3Lm+bNmycochIiIiItIvXnrpJaZPn17uMAZEc3MzdXV1pNNprrzySj784Q9z5ZVXHvJyuttnZvacu88tnvagJUbunovgC2b2ODAS+O0hRyUiIiIiIlKCL3zhCzz22GO0tbVxySWXcMUVV/T7OkupfGE+8KK7N7n7781sBMF7Rk/3e3QiIiIiIjLsfP3rXx/wdZZS+cL3gOZIf3M4TEREREREBtDBXoORgkPdV6UkRuaRpbp7ltIqbRARERERkT5SVVXFnj17lByVwN3Zs2cPVVVVJc9TSoLzipl9kkIp0d8DrxxGfCIiIiIicpgmTpzI5s2b2bVrV7lDOSJUVVUxceLEkqcvJTH6GHArcAPgwP8A1x5WdCIiIiIicliSySRTp04tdxhDVim10u0EFg1ALCIiIiIiImXRY2JkZv/k7v9qZv8fQUlRJ+7+yX6NTEREREREZID0VmK0KmzrX1NFRERERGRI6y0xugr4DTDK3b99OAs3swXAt4E48AN3v6Vo/L8BF4S9NcBR7j4qHJcBXgjHvebuCw8nBhERERERkYPpLTGaY2YTgA+b2U8Ai4509729LdjM4sB3gYuBzcCzZvaAu+dKonD3f4hM/wmCP47NaXX32SVviYiIiIiIyGHqLTH6PkENdCcAz9E5MfJweG/mAevc/RUAM7sbuJzCI3rF3gvcVELMIiIiIiIifarHP3h191vdfTpwp7uf4O5TI83BkiKA44BNkf7N4bAuzOx4YCrwu8jgKjNbZmZPmdkVPcx3bTjNMtXnLiIiIiIih6u3WulGuPt+4J/NbEzx+IM9SneIFgH3unsmMux4d99iZicAvzOzF9x9fVEMtwO3A8ydO1d/ASwiIiIiIoelt0fpfg5cRvAYnXPoj9JtASZF+ieGw7qzCPh4dIC7bwnbr5jZUoL3j9Z3nVVEREREROT16TExcvfLwvbh/r3us8A0M5tKkBAtAt5XPJGZnQKMBp6MDBsNtLh7u5mNA84F/vUw4xAREREREelVj+8Y5ZjZuWZWG3ZfbWbfNLPJB5vP3dPAdcDDwEvAL939RTO72cyiVW8vAu529+ijcNOBZWb2F+Bx4JZobXYiIiIiIiJ9yTrnI91MYPY8MAuYCfwI+AHwHnd/c79Hdwjmzp3ry5bpv2hFRERERKRnZvacu88tHn7QEiMgHZbmXA58x92/C9T3dYAiIiIiIiLl0lvlCzlNZnY9cDXwJjOLAcn+DUtERERERGTglFJidBXQDnzE3bcT1C73tX6NSkREREREZAAdtMQoTIa+Gel/DfhJfwYlIiIiIiIykEqplW6+mT1rZs1m1mFmGTNrHIjgREREREREBkIpj9J9B3gvsBaoBv4OuK0/gxIRERERERlIpSRGuPs6IO7uGXf/d2BB/4YlIiIiIiIycEqpla7FzCqAFWb2r8A2SkyoREREREREjgSlJDjvB+LAdcABYBLwzv4MSkREREREZCCVUivdxrCzFfhi/4YjIiIiIiIy8HpMjMzsBcB7Gu/uM/slIhERERERkQHWW4nRZQMWhYiIiIiISBn1lhglgaPd/Y/RgWZ2LrC9X6MSEREREREZQL1VvvAtYH83w/eH40RERERERIaE3hKjo939heKB4bAp/RaRiIiIiIjIAOstMRrVy7jqvg5ERERERESkXHpLjJaZ2TXFA83s74Dn+i8kERERERGRgdVb5QufBv7TzP6GQiI0F6gArixl4Wa2APg2wR/E/sDdbyka/2/ABWFvDXCUu48Kx30AuCEc9//c/celrFNERERERORQ9ZgYufsO4BwzuwCYEQ5+0N1/V8qCzSwOfBe4GNgMPGtmD7j7qsg6/iEy/SeAM8LuMcBNBImYA8+F8+47lI0TEREREREpRW8lRgC4++PA44ex7HnAOnd/BcDM7gYuB1b1MP17CZIhgLcCj7r73nDeR4EFwC8OIw4REREREZFe9faO0et1HLAp0r85HNaFmR0PTAVypVElzWtm15rZMjNbtmvXrj4JWkREREREhp/+TIwOxSLgXnfPHMpM7n67u89197njx4/vp9BERERERGSoO+ijdK/DFmBSpH9iOKw7i4CPF817ftG8S/swNpGhyx1SrdDWAK37oLUh6PYsJGugoraoXQPxCshmIJsOpst3h+1sBjKpsDtVGJ9JQaYjbLdH+sNhsQQkqiBRCcnqoJ3rB8hmg/V5Jlhmro0Hbxfiwfbk2wTL7miG9iZo3x+2wybVCrE4xJIQTwbrjyeD/lgc0m3BNKmWsN0KHQeC4e5gsbCxSHfY3xOLhetLdF2vxYJlp9u7tjMdwXTdzRtLBOvMbXN0+/HweET2c76dCuaLJ4NjmltmvCLsjwEW2Z6wbRZ0F++zfDzxYB3pjsJ6c9uQ6SicI9FjmDt/PBvE7h52Z8PtyQb7J14RrrMisv6KYJ2djr0Xzm8Ixls8PN6Rbot3PmbR44kVYog22TDO6Lmc39aw22Kdz99o22KQbi2cU6nWwrnmGageDTVjC02uv2pksL7UAehoCc7LjgOFczTdXhRLR+dzp6Im+Aznmlx/7njlzonoeeLZyDGOtHOfkfw+z0aOXXR/ZYJhuX2WP8bZyPGJnq8E6+gUY23nWDt9PiLdngm+N5I1Xdvxisg+aQv2UfSzZbFw28Lvg/y2xgvnffE5lf88xwufw/y5lQimz38O2iPt9mB/5M7F/Gc4d05a4bumozlsh02qJdie6lFQNaprOxYvfJ5y37m5z1tun0e3JbodiaqgSVZBorrwPRxPBt+9xd/xufUUL8+7WXb0M2Vhk0kX7Ze2Qrd7sP7cd1Hu856oDJaRaYdUW9fvyWwq3IbqwrFPVIXnQlXh2OaOVafjFuv8nRDtzn2P5n97cr9D2c7fobk4c9+nsVjn/RL9vEB4zA+xzME9cmxTwX7MRj63+WMZ7r9EZeE3ortl5L6Tc+dKdNuiv7HxyqLf5urgt6g/ZDPB8ayo6Z/l96H+TIyeBaaZ2VSCRGcR8L7iiczsFGA08GRk8MPAV8xsdNh/CXB9P8Yqw0E2G1y85H+cWgo/TLkf+04XBV64WEq3B/NGv7CjFz+p1siFUVvQjXX9kasaGXQnqqCtsXPikuvuaAq/xBOFH+RYpJ1NR744I1+kmY4gOWhrCLqHi0QVVNYHTaK6kLwV76NsJpg2fzEZ/sjWjClc3PZ0Idib3MVER0vX9Xq2c0KYrA4uihOVwY9sp4udVOGHLN0WWUFR8gLBD3VlfVFSUVH4USu+GM5dIKc76Hox6J23o8sPdNjEKyBREfyYRrsTVd1cjEQvKMML0fzFVFGSkk13jjG6H3Lb3Kkd2e+5pCaanHkm2KRoAhY9pvkLpFh4kWSF/tx25fZtInIRh0c+/5GL8LbGYJ2J6sjxjVzIWSz4bLfsgf1bYPtKaNlddIwJpkvWBvPkztFEZbiPu4kpm+58sX1gVyGpyqYLF0/5C7sw2bVY4Zh2utAO959FjlX+uOWOXfQCM9Lf5QZC5JhBEFPT9jDxiySAFCUkiepg+3IXgRYLvk9zNzIy7b1/FuO58zPZ+QZPNhW54O9jFi/s6+IbSp2mi0FFXeGmVEVt0F81Mtgnu9cWfguKz43hzOIH/w4eSBbrfKOm+4kKv9n55DiXjBXdCIzeVDjUOOLhZyTTEZzjfcHihd/DnsTCaXKf1dx3VKIq2JbozZ3cZz7TDiMmwmde7Js4+1G/JUbunjaz6wiSnDhwp7u/aGY3A8vc/YFw0kXA3e6FWxLuvtfMvkSQXAHcnKuIQYaZbCa48GjZC617wx+OxuDHo60x0jRAe3N4sdIWufPURqdSgr4USxbuWkXvYCWqgyTIs8HF0J71hXi7+wJM1gbJUvXoYL4Rx4UXctE7eenw7lm6cOczlxBESxwq6zsvq3p0ISmzWOSOdO4CJbxTnS+96O4ufOROa6z4LnMivHjr4e5aNt01ocwdH4hcoMY6rzv/pRy9KA77YwmoGgGVI4ILi0RF3x5XkYHQ0RJ8JyQqC0lQbyWTQ4178F2Qu2FRyp3qbKZwIyrTXrh7nkvWe7tTn7/RVXQBWVx66tGSlEznpBEi33fh3fZYvPf15RL4Qzm+qbbC71quhK9TaUiy6HuyeFvCfZX77cvd2MvdtMuV5ke/46Ml1bnldoo3UsqW27Z8d5goxOKF/RJN6uPhtheXOucaz0ZKYCM3knLHNJOK3IhsiVx0t3U+PtGEv9vSoGgiYsGyi0uac6Wm2eKbS9FS+eiNg8iNHqBLiXm0JD16E6H49674yYFoqb1nC6WpxSVy7t08IZDrjyRn0RtCuZtVmcgNnlT0d7qtaylh4cQuXI/k44k08SSMmNC1FDt3E/IIYN7jxh9Z5s6d68uWLSt3GMNbqg2atgV3Bpu2QfNOOLAzbO8qtA/sDr5McndVi9up1iAJatkb/Dj0dmcmUR3ccasaGSQFnR7XqiokLYmq4CI69yHNdeceJ4s+flP8hZf/oq/qvOzDKS7Pleik2wslSLqwFxERERkwZvacu88tHt6fj9LJkSybgf1bgyQn9x5H/r2O8N2O1oZCErR/a5DMFLM41I6HuvFQexSMPzl4th4iJTutnduV9TD6eKgeEz6LPyborhkTeSwtTIZy76ocCcyCko6qEeWORERERESKKDEarjKpQilO42bYtyFsXg3aDa/1/p5Kojq4wK8/BkZOgknzoH4CjDgW6sOm7uggsTnUkhURERERkQGmxGgoa9kLm56Bzc8EiU7zDmjeFbS7K92pHAljpsDRM+CUy2D0lOB9l6oRQSlORV3hJfd4cqC3RkRERESk3ygxGircYe8rsOlpeO2poNm9JhgXS8DIiUEJztgT4fhzoO6ooKk9KnhRbszUoHRHRERERGQYUmJ0pNq/Dbb+GbatCNpblgdVwEJQ8jNpHsx8D0yeDxPOPCLqjhcRERERKRclRkeCbAY2L4NXlsLW5bB1BTRvD8ZZDMafAtMugUlnwaT5Qb/e6xERERERKZkSo8GqeSes+x9Y+wis/11QxTMG406CE86HCWfAhNlwzOlBldMiIiIiInLYlBgNJg2vwfK7YN2jweNxELwDdPKlMO0tcMIFR8wfZImIiIiIHEmUGA0GzbvgiW/Ash8G/yg88Sy44AaYdjEcM1OPxYmIiIiI9DMlRuXUth+e/A48+V1ItcDsv4HzlwQ1yImIiIiIyIBRYlQOqbagdOgPXw/+T+jUy4MSovEnlTsyERlm3J2dLTs5kD7A8fXHE4/Fyx2SiIhIWSgxGmjrH4f/ug72bw7eGbroRjjuzHJHJTIg3J0dLTvY1LSJVCZFMp4kGUtSEa+gIlYRtOMVJGNJqhJVVMYrScRK+5pyd9Kepj3dTlumjY5MB22ZNtrT7bRn2kllU9QmaxlRMYL6inrqK+qJWefHVPe17ePVxlfZsH9D0G7cwIb9G8h6lpGVI/PNqMpRjKwYyYjKEVTEK+jIdNCeCdaTW19HpgPHqU3WUpuspS5ZF7Qr6vLDahNBuyZZQ22ytsu2ujst6Rb2t+9nf0ehyXqWGDHMDMOIWaE7HouTjBX2a647GU/S3NHMq42v8ur+YNtebXyVjfs30pJuAaA6Uc1pY0/j9PGnc/q4oDm65mjMrFNMzalm9rTuYU/bHhraGqiIV1BXUUddso76ivr89hXv3/6QyqTY3rKd7QeCZk/rHkZUjmBM1ZhOTU1y4P+ywN3JelbJpojIEUKJ0UA6sBvu/TDUjoO//a+gdjkpm6xn2dWyix0tO3CchCWIx+LELU48FidhCcyMxvZGdrbs7NTsat3FzpadZD1LfUU9dRV11CcL7dzFYV1FXXDxWxFcGNcka6hL1lGVqOp0YQvkL26BThfY7ZnChX57pp1sNkuWLO6O4/mLL8c7JQNtmTba0m35djKeZFTlqOCiPnJxP6pqFAlL0NjRSEN7A43tjTS2F7oPpA5gZsSIEYvFgnYYa9ziVMYr80lMZbyS6kQ1lfFKkvEk2w9sZ+P+jflmU9MmWtOth3ScouuoiFcQtzjpbJpUNpVv57oPhWHUVdQxomIEtcladrbspKG9IT++IlbB5BGTmTZ6Wn7/7Gvbx4bGDTS2N9KUaup2uclYksp4JRXxCgBaUi20ZdpKiqkyXkltspbqRDUHUgdo6mgi45lD2q5SGMaxtccyZeQUzjjqDKaOnEpVoopVe1axcvdK7lp1V35/jq8ez4mjTqS5o5k9bXvY07qHjmxHSevIJaKjqgrn3ejK0flux9nXti9o2oN2Q3sD+9r20ZHpoDpZTU2ihupEddCE/QA7Duxg24Ft7G7djeMHjac6Uc2oylHELEY6mybrWTKeIZ1Nk/EMWc+SsER+PVXxKqoT1VQlgnbu3EvEEvnviFx/OpumqaMpn7g2dTTlk9lUNkUylixsQ6SpSdZQk6jJJ5K5BLk2EXQnYon8Zy/XxiBGjMp4ZTB/0TwV8Yp88trY3khjR2P+M93Y3khTRxPNqWYOpA7QkmqhOdVMS6qFA6kDtGXaqIhX5D/LnZpEJVXxKqoSVfl27rNemagEh1Q2ld+n0SYRS+S3tzpROKY1yRoqYhXEYrFgn1qcmAXdsVhwnPa27mVv2978uZfrbupooiZRk7/JUV9RT10y+DxXJ6tpSbXQ0N6Q/x7Lde9v34/j+WOXa5KxJAkLuqPHNne8E7EEjpPKpPI3PjqyHUE7E3wexlSNYWz1WMZWjWVs9VjGVY9jbNVYRlSOyB+PhrbOMTWnmhlRMYJjao/h6Jqj///27j1IsrK84/j36Z6ZnpmendteZ2fZ3Vl2ISIoi0iIEKSMJBAtSBUpAY0RCkJphUUtNaL/SKxolEolSkQrREGilqgbRbAMioiAimQB5baEhV12YWd3YS9z65np6duTP85luuey15npWfr32TrV55w+3f2ePu+8533eSy9L00tZ1ryMtlRbRYNEOXcnV8pRLAVlQ/l9JFovlAoM54eDpTAcX+Ph/DC5Yi74nsP7XPn5GlZ5DX38OhZLxSAfRtfIEhXXq6W+JW6I6GjsIJVMHbowkuC+HpY/yURy1huURvIjjBRGWNi4cNo8VsvM/dA3lOPBmWee6Y899li1k3FwG6+GzT+GDz4MS95Q7dRURb6UZ+fQThqTjSxsWhhXHqczMDbA9sHtbB/Yzo7BHQzmBuNKZ3kvQyqZoj5RP6mgjgrvYqnI7uHdvDL0Cr2ZfTgyoAAAEoJJREFUXnYO7WRXZtdhVfDKJS3JoqZFLGlewuKmxSQTyaCikcuQyWcYyg0xlBs64vedTUlL0ljXSK6YI1/KH9FrDYtb2kteihd3p0Swfih1VseKBStY2bqSVa2rWLVgFStbV9JU11RZuSjlyBfz8frEnp9cMUe2kKXoxaASE1Vmwsf6ZFCxKQ/SokpblD+iaxRVWqPKbCaXYXHzYla3rqanrYfVbatZnl5+0Jb+QqkQVHyL+Thoa0g0TPmafCkfV0IzuaBSWl4hnVh5yRayNNc309rQGiypoJcr6u1KWnL8mlACH18veSm+1vliPvhew/Wm+iZ6WntY1bqKxrrGac8tV8zx/IHneWrfUzyz7xm2D2ynNdUaV/jKH9sb28kVc0Eglx9iOBecW3Su5ZXS/rF++rP9FUGlYUHA3thOR6qDjsZgaUg0kC1mGcmPMFoYZbQwGq+XKLG0eSld6S660l0sSy9jWXoZXekuOps6yeQyHMgeqFxGD9A31gcQlxFRpTfaLngh/qxoyRayjBZGyRVzFL1IsVSk4EElMdo2s/g6RddsQcMCWlOtNCWbyBazk943Op+RwgjDufHrfzhB3sHUJepw94MG1HWJurgXs7m+Oe65TCVT5Ev5yh7Q8qUwxmhx9IgbIWZS0pJ0NnbS0tDCaGGUodwQw/nhg74mlUyNNwal2uLgeNISBgFxwBxe62gdGL/3lN13UskUJS/FQdvhfj91VkdLQ8uUDSCpZIolzUuoS9RN6pGeT/eXg0nXp+lIdcQ9tlGDWty4hsUV85KXKHghaPiL1r1EwhIsaVoS/41Hf+fL0stoT7VzIHsgvqf3ZnrH7+/Du0haMm6gjHrso0bLkpfGy94JS12ijuXp5SxvCZaudFf82J5qD/5my46PGhpGC6Px/TFqtHQ8vm/2jfWNB/dhj/u+0X2T8q9hFUF5FChF31UcBGPUJ+orGgfKl7pEHQdGxxsVosdohEBLfQvrOtaxrn0dJ3WcxEmdJ7GufR0tDS2TrqW7UygVyBazDOWG4rJ8YuPDWHEsDs7Lr69htKZa2bB+w+xktqNgZo+7+5mT9iswmiPP/w9893I4/9Nw/iernZpDGsmP8NLgSwxkB2hNtcbDhqYafjSVkpfoHerlhf4XeLH/RV7se5EX+l9g++D2iptGa0Mri5sWBy1rTUFlayg3FAdCUUUGxm8iUQtdwY/85rygYQErWlawYsEKulu66W7ppivdRTKRnFThiW6QbQ1tLEkvYWnzUjpSHYc1LCZXzDGUGwoquoVhMrkMI4WROIDKFrIVBWf5OgS9FVNV8BsSDfFwq+jGkrCgFdmw+NioVTdVF9y4ISjYRgujFRXVaL1QKlQME4sqEIe63iUvxTfrbDEb9G6FvVS5Yo6lzUtZ3rL8sIfDSW0olAr0j/WTtCStDa0aahYqeYlsIRtXuOKGiLByFR1TohQHo1EwPVIYidfNgmCztaG1IiCI/qaPtSU/X8rHf/OjhVHGCkFlKO6BCVu+o4aLQqlQEdiOFEbi7ajnI+rBi3rviqUiyUQyDsI7Gzvj3peJZVKxVIyD8ShQSten4/Nuqms6pvM9Eu7OYG6woiI6mBukpb6l4jq0p9pJ16cxC3p39o/u59WRV4NlePyxRCluDCx/TCVTJC0Z3zuAiqA6YYm4FzEeuhv2SjYmG8e/74mBvhfjnrOJPWblQXf0+vL3yeSDBom+bF/F44HsAUYLoxUjHKJ8Hd334h7DRII6q6torHht5DX2DO+Z1LCXsMSkxrmFjQvpXtDN8vRygIrGmuH8cHwPTloyHsERfUfReq6YY9fwLnZndpPJZ2Y0f7Sl2iY1MHU2dlKXqJsUiEfBenndYOK1zhVzZPKZuKEvaqgdyg1R8ALtqfbxzyv7zKa6JrYPbGdL3xZe6HuhorFqWXoZSUvGAXn0eKhGm6hsmZje6N/ipsX86JIfzej3eSwUGFVTdgBuORua2uHaB6Hu4L0kcyVfytOf7ac308vW/q1sG9jG1oGtvNT/EruGd035mijqb2toI12fpuAF8sV8xXCmfClPtpCtKMS6W7pZ276Wte1r6WnroVAqsHd0L/tG97F/NGg12Te6j/3Z/aTr06xuXc2q1lVB63243r2gO67kQ3AzLB/KEA3jiAvs0njBnbAEXS1dtDa0zvr3KiIiIjMn6pGLhtDuGd7D/ux+FjUtYkVL0NC5vGX5Yc0ljOq9hzOMbDA3yO7Mbnozvewe3s3g2GBFIFURdNY1x0MNy3tLopEsbQ1t1CfrD/GJM+NI5jdGc3+39G1hS98WtvVvA4Le0Wg0RBSM1yfqKxpcyoP9460BVIFRNd3zEXjiDrjmF9D9ljn7WHdna/9Wft37a14ZeqWiG7dvrI+BsYGK41PJFD1tPfS09bCmbQ0ntp9IZ2MnQ7mheHz6YG4wHrc+kh8ZH8ZUNpSpPhlM+l7dupq17Ws5sf1E0vXpOTtvEREREZHpTBcYzWp4Z2YXAl8GksDX3f0LUxzzHuBGwIEn3f294f4i8HR42MvufvFspnXWvPQwPH47vG3DnARFuWKOTXs28eDOB3lo50P0ZnoB4rH7nY2dnNRxEh2NHSxsXEhHYwdd6S7WtK855JwKEREREZHXq1kLjMwsCdwCXADsBDaZ2d3uvrnsmHXAp4Bz3L3PzJaUvcWou58+W+mbE7kRuOd66OgJ5hYdo2whO2nCdjQJsH+sn9/t+h2P7H6E0cIojclGzu46m6tPu5rzus9jaXrpDJyQiIiIiMjr02z2GJ0FvOju2wDM7E7gEmBz2TF/B9zi7n0A7v7aLKZn7v3qn+HANvjAPdBwdP+Hxp7hPdz/8v3ct+M+nnj1iYNOfluWXsbFJ17M21e8nbcue+tBf3VKRERERETGzWZg1A28Ura9E/jjCcecBGBmvyEYbneju98bPtdoZo8BBeAL7n7XxA8ws2uBawFWrlw5s6k/Vr1PwCNfgbdcCT3nHdlLM738YscvuG/HfTy590kA1rav5ZrTrmFx8+KK/7OifPLfkuYl+k16EREREZGjUO2fkKgD1gHnAyuAh8zsNHfvB1a5e6+ZrQF+aWZPu/vW8he7+63ArRD8+MLcJv0gCjn48XXQshQu+OxhvWSsOMY9W+9h45aNPLv/WQDe0PkGNqzfwDtXvZM1bWtmM8UiIiIiIjVtNgOjXuCEsu0V4b5yO4FH3T0PvGRmWwgCpU3u3gvg7tvM7FfAemArx4PffBleexauuBMa2w566MDYAD/Y8gO+vfnb7M/u5+SOk/noWz7KBSsv4ITWEw76WhERERERmRmzGRhtAtaZWQ9BQHQ58N4Jx9wFXAHcbmaLCIbWbTOzDmDE3cfC/ecAN81iWmdOqQTbHoBTL4WTL5r2sD3De/jW5m+xcctGRgojnLP8HK469SrOWnaWhsOJiIiIiMyxWQuM3L1gZtcBPyOYP3Sbuz9rZp8FHnP3u8Pn/tzMNgNF4BPuvt/M3gb8h5mVgATBHKPN03zU/JJIBD+2kB+Z8uneTC9f/cNX+em2n+I4F/VcxJVvvJKTO0+e44SKiIiIiEhE/8HrHMrkMlz2k8vYO7qXS9ddyvtPeT/LW5ZXO1kiIiIiIjWjKv/Bq4xzd2585EZ6M73c9he3ccbSM6qdJBERERERCSWqnYBa8f3nv8/Ptv+MDes3KCgSEREREZlnFBjNgef2P8cXN32Rc7vP5apTr6p2ckREREREZAIFRrNsKDfExx78GJ2NnXz+3M+TMH3lIiIiIiLzjeYYzSJ358bf3siuzC5uv/B2Oho7qp0kERERERGZgrovZtH3nv8eP9/xc64/43rWL1lf7eSIiIiIiMg0FBjNks37N3PTppv40+4/5co3Xlnt5IiIiIiIyEEoMJoFQ7khPv7gx+ls7ORz535O84pEREREROY5zTGaYe7OZ377GXZldvHNC7+peUUiIiIiIscBBUYzzHHWtK3hTWe8idOXnF7t5IiIiIiIyGFQYDTDEpbguvXXVTsZIiIiIiJyBDT5RUREREREap4CIxERERERqXkKjEREREREpOYpMBIRERERkZqnwEhERERERGqeAiMREREREal5CoxERERERKTmKTASEREREZGaZ+5e7TTMCDPbC+yodjrKLAL2VTsRclxS3pFjofwjR0t5R46F8o8crWrknVXuvnjiztdNYDTfmNlj7n5mtdMhxx/lHTkWyj9ytJR35Fgo/8jRmk95R0PpRERERESk5ikwEhERERGRmqfAaPbcWu0EyHFLeUeOhfKPHC3lHTkWyj9ytOZN3tEcIxERERERqXnqMRIRERERkZqnwEhERERERGqeAqMZZmYXmtnzZvaimd1Q7fTI/GZmJ5jZA2a22cyeNbMPh/s7zew+M3shfOyodlplfjKzpJn93sx+Em73mNmjYRn0PTNrqHYaZX4ys3Yz22hm/2dmz5nZn6jskcNhZh8N71nPmNl3zaxRZY9Mx8xuM7PXzOyZsn1TljUWuDnMR0+Z2RlzmVYFRjPIzJLALcBFwCnAFWZ2SnVTJfNcAfiYu58CnA38fZhnbgDud/d1wP3htshUPgw8V7b9ReDf3H0t0AdcXZVUyfHgy8C97v5HwJsJ8pHKHjkoM+sGrgfOdPdTgSRwOSp7ZHrfBC6csG+6suYiYF24XAt8bY7SCCgwmmlnAS+6+zZ3zwF3ApdUOU0yj7n7bnd/IlwfIqiYdBPkmzvCw+4A/qo6KZT5zMxWAO8Cvh5uG/AOYGN4iPKOTMnM2oDzgG8AuHvO3ftR2SOHpw5oMrM6oBnYjcoemYa7PwQcmLB7urLmEuC/PPA7oN3MuuYmpQqMZlo38ErZ9s5wn8ghmdlqYD3wKLDU3XeHT+0BllYpWTK/fQn4B6AUbi8E+t29EG6rDJLp9AB7gdvDoZhfN7M0KnvkENy9F/gX4GWCgGgAeByVPXJkpitrqlqXVmAkMg+YWQvw38BH3H2w/DkPflNfv6svFczs3cBr7v54tdMix6U64Azga+6+HhhmwrA5lT0ylXAuyCUEwfVyIM3kYVIih20+lTUKjGZWL3BC2faKcJ/ItMysniAo+o67/zDc/WrUdRw+vlat9Mm8dQ5wsZltJxi2+w6COSPt4fAWUBkk09sJ7HT3R8PtjQSBksoeOZR3Ai+5+153zwM/JCiPVPbIkZiurKlqXVqB0czaBKwLf5mlgWAy4t1VTpPMY+GckG8Az7n7v5Y9dTfwgXD9A8CP5zptMr+5+6fcfYW7ryYoa37p7u8DHgD+OjxMeUem5O57gFfM7ORw158Bm1HZI4f2MnC2mTWH97Ao76jskSMxXVlzN/C34a/TnQ0MlA25m3UW9F7JTDGzvyQY958EbnP3z1U5STKPmdm5wMPA04zPE/k0wTyj7wMrgR3Ae9x94sRFEQDM7Hzg4+7+bjNbQ9CD1An8Hvgbdx+rZvpkfjKz0wl+uKMB2AZcRdBgqrJHDsrM/hG4jOCXVX8PXEMwD0Rlj0xiZt8FzgcWAa8CnwHuYoqyJgy2v0IwPHMEuMrdH5uztCowEhERERGRWqehdCIiIiIiUvMUGImIiIiISM1TYCQiIiIiIjVPgZGIiIiIiNQ8BUYiIiIiIlLzFBiJiMi8ZGZFM/tD2XLDDL73ajN7ZqbeT0REjn91hz5ERESkKkbd/fRqJ0JERGqDeoxEROS4YmbbzewmM3vazP7XzNaG+1eb2S/N7Ckzu9/MVob7l5rZj8zsyXB5W/hWSTP7TzN71sx+bmZN4fHXm9nm8H3urNJpiojIHFNgJCIi81XThKF0l5U9N+DupxH8D+lfCvf9O3CHu78J+A5wc7j/ZuBBd38zcAbwbLh/HXCLu78R6AcuDfffAKwP3+eDs3VyIiIyv5i7VzsNIiIik5hZxt1bpti/HXiHu28zs3pgj7svNLN9QJe758P9u919kZntBVa4+1jZe6wG7nP3deH2J4F6d/8nM7sXyAB3AXe5e2aWT1VEROYB9RiJiMjxyKdZPxJjZetFxufdvgu4haB3aZOZaT6uiEgNUGAkIiLHo8vKHh8J138LXB6uvw94OFy/H/gQgJklzaxtujc1swRwgrs/AHwSaAMm9VqJiMjrj1rBRERkvmoysz+Ubd/r7tFPdneY2VMEvT5XhPs2ALeb2SeAvcBV4f4PA7ea2dUEPUMfAnZP85lJ4Nth8GTAze7eP2NnJCIi85bmGImIyHElnGN0prvvq3ZaRETk9UND6UREREREpOapx0hERERERGqeeoxERERERKTmKTASEREREZGap8BIRERERERqngIjERERERGpeQqMRERERESk5v0/JMw0VBh1KP0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1008x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdhzBAldzeva",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "1d7de72c-81b0-4225-a439-203f7fc8562d"
      },
      "source": [
        "data_test = np.load('/content/content/My Drive/Colab Notebooks/kddworkshop/fulldata/test_138_day.npz')\n",
        "\n",
        "passive =1\n",
        "\n",
        "#load common data\n",
        "latlon_test = data_test['latlon']\n",
        "iff_test = data_test['iff']\n",
        "\n",
        "# if passive ==1:\n",
        "x_t_test = data_test['viirs']\n",
        "y_t_test = data_test['label']\n",
        "# else:\n",
        "x_s_test = data_test['calipso']\n",
        "y_s_test = data_test['label']\n",
        "    \n",
        "inds_test,vals_test = np.where(y_t_test>0)\n",
        "\n",
        "# process common data\n",
        "Latlon_test = latlon_test[inds_test]\n",
        "Iff_test = iff_test[inds_test]\n",
        "\n",
        "Y_t_test = y_t_test[inds_test]\n",
        "X_t_test = x_t_test[inds_test]\n",
        "\n",
        "Y_s_test = y_s_test[inds_test]\n",
        "X_s_test = x_s_test[inds_test]\n",
        "\n",
        "# 0 =< SZA <= 83\n",
        "print('original X_t_test: ', X_t_test.shape)\n",
        "rows_test = np.where((X_t_test[:,0] >= 0) & (X_t_test[:,0] <= 83) & (X_t_test[:,15] > 100) & (X_t_test[:,15] < 400) & (X_t_test[:,16] > 100) & (X_t_test[:,16] < 400) & (X_t_test[:,17] > 100) & (X_t_test[:,17] < 400) & (X_t_test[:,18] > 100) & (X_t_test[:,18] < 400) & (X_t_test[:,19] > 100) & (X_t_test[:,19] < 400) & (X_t_test[:,10] > 0))\n",
        "print(\"rows_test:\", rows_test)\n",
        "print(\"rows_test.shape:\", len(rows_test))\n",
        "\n",
        "Latlon_test = Latlon_test[rows_test]\n",
        "Iff_test = Iff_test[rows_test]\n",
        "\n",
        "Y_t_test = Y_t_test[rows_test]\n",
        "X_t_test = X_t_test[rows_test]\n",
        "\n",
        "Y_s_test = Y_s_test[rows_test]\n",
        "X_s_test = X_s_test[rows_test]\n",
        "\n",
        "X_s_test = np.nan_to_num(X_s_test)\n",
        "X_t_test = np.nan_to_num(X_t_test)\n",
        "\n",
        "print('after SZA X_t_test: ', X_t_test.shape)\n",
        "print('after SZA X_s_test: ', X_s_test.shape)\n",
        "\n",
        "# pca = decomposition.PCA(n_components=20)\n",
        "# pca.fit(X_s_test)\n",
        "# X_s_test = pca.transform(X_s_test)\n",
        "# print (X_s_test.shape)\n",
        "\n",
        "#concanate common data\n",
        "# X_t_test = np.concatenate((X_t_test, Latlon_test, Iff_test), axis=1)\n",
        "X_s_test = np.concatenate((X_s_test, Latlon_test, Iff_test), axis=1)\n",
        "\n",
        "print (X_s_test.shape)\n",
        "print (X_t_test.shape)\n",
        "\n",
        "X_test=np.concatenate((X_t_test, X_s_test), axis=1)\n",
        "\n",
        "# scaler_t = StandardScaler()\n",
        "# scaler_t.fit(X_t_test)\n",
        "# X_t_test = scaler_t.transform(X_t_test)\n",
        "\n",
        "# scaler_s = StandardScaler()\n",
        "# scaler_s.fit(X_s_test)\n",
        "# X_s_test= scaler_s.transform(X_s_test)\n",
        "\n",
        "x_test2=sc_X.transform(X_test)\n",
        "\n",
        "# x_train_v = x_train[:, 0:20]\n",
        "# x_train_c = x_train[:, 20:45]\n",
        "# x_train_comm = x_train[:, 45:51]\n",
        "# print(x_train_v.shape)\n",
        "# print(x_train_c.shape)\n",
        "# print(x_train_comm.shape)\n",
        "\n",
        "# x_valid_v = x_valid[:, 0:20]\n",
        "# x_valid_c = x_valid[:, 20:45]\n",
        "# x_valid_comm = x_valid[:, 45:51]\n",
        "\n",
        "# print(x_valid_v.shape)\n",
        "# print(x_valid_c.shape)\n",
        "# print(x_valid_comm.shape)\n",
        "\n",
        "X_t_test = x_test2[:, 0:20]\n",
        "x_test_c2 = x_test2[:, 20:45]\n",
        "x_test_comm2 = x_test2[:, 45:51]\n",
        "\n",
        "\n",
        "# DLR imputed target domain\n",
        "# x_test_t_pt = model_reg.predict(X_t_test)\n",
        "# print(x_test_t_pt.shape)\n",
        "\n",
        "x_test_pt_test = np.concatenate((X_t_test, x_test_comm2),axis=1)\n",
        "print(x_test_pt_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8e3769e40981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/content/My Drive/Colab Notebooks/kddworkshop/fulldata/test_138_day.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpassive\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#load common data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TecWVAgxzgvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_t = x_train_pt\n",
        "Y_t = y_train\n",
        "\n",
        "# X_s_test = X_s_test\n",
        "# Y_s_test = Y_s_test\n",
        "X_t_test = x_test_pt_test\n",
        "Y_t_test = Y_s_test\n",
        "\n",
        "\n",
        "train_tgt, test_tgt = prepare_data(X_t, Y_t, X_t_test, Y_t_test)\n",
        "\n",
        "\n",
        "# evaluate the model\n",
        "acc = evaluate_model(test_tgt, model)\n",
        "print('test Accuracy: %.3f' % acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}