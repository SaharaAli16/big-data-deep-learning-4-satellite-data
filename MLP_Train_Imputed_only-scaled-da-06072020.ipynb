{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fiujXQtQvxMf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/xin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/xin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/xin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/xin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/xin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/xin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_v shape:\n",
      "(2099451, 20)\n",
      "X_c shape:\n",
      "(2099451, 25)\n",
      "X_v\n",
      "[[ 97.43999481 -55.29999924   2.52999997 ... 246.61909485 246.91960144\n",
      "  246.64631653]\n",
      " [ 97.43999481 -55.2899971    2.51999998 ... 247.22109985 247.5663147\n",
      "  247.28843689]\n",
      " [ 97.40999603 -55.22999954   2.5        ... 248.27958679 248.7721405\n",
      "  248.57876587]\n",
      " ...\n",
      " [ 94.68000031 -62.64999771   2.12999988 ... 228.96557617 228.62413025\n",
      "  227.59910583]\n",
      " [ 94.68000031 -62.62999725   2.12999988 ... 229.51361084 229.17892456\n",
      "  227.92941284]\n",
      " [ 94.66999817 -62.6099968    2.12999988 ... 230.20637512 230.03952026\n",
      "  228.89453125]]\n",
      "X_c\n",
      "[[ 1.000e+00  2.000e+00  0.000e+00 ...        nan -9.999e+03        nan]\n",
      " [ 1.000e+00  2.000e+00  0.000e+00 ...        nan -9.999e+03        nan]\n",
      " [ 1.000e+00  2.000e+00  0.000e+00 ...        nan -9.999e+03        nan]\n",
      " ...\n",
      " [ 1.000e+00  1.000e+00  0.000e+00 ...        nan -9.999e+03        nan]\n",
      " [ 1.000e+00  1.000e+00  0.000e+00 ...        nan -9.999e+03        nan]\n",
      " [ 1.000e+00  1.000e+00  0.000e+00 ...        nan -9.999e+03        nan]]\n",
      "original X_v:  (1667850, 20)\n",
      "rows: (array([    402,     403,     404, ..., 1666325, 1666326, 1666327]),)\n",
      "rows.shape: 1\n",
      "after SZA X_v:  (704800, 20)\n",
      "after SZA X_c:  (704800, 25)\n",
      "(704800, 20)\n",
      "(704800, 31)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 12-10-2019\n",
    "\n",
    "@author: Xin Huang\n",
    "\"\"\"\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# data = np.load('Sat_data_small.npz')\n",
    "data = np.load('train10.npz')\n",
    "\n",
    "# passive = 1\n",
    "\n",
    "#load common data\n",
    "latlon = data['latlon']\n",
    "iff = data['iff']\n",
    "\n",
    "X_v = data['viirs']\n",
    "Y_v = data['label']\n",
    "print ('X_v shape:');\n",
    "print (X_v.shape);\n",
    "\n",
    "X_c = data['calipso']\n",
    "Y_c = data['label']\n",
    "print ('X_c shape:');\n",
    "print (X_c.shape);\n",
    "\n",
    "inds,vals = np.where(Y_v>0)\n",
    "\n",
    "# process common data\n",
    "Latlon = latlon[inds]\n",
    "Iff = iff[inds]\n",
    "\n",
    "Y_v = Y_v[inds]\n",
    "X_v = X_v[inds]\n",
    "print ('X_v')\n",
    "print (X_v)\n",
    "\n",
    "# inds_c,vals_c = np.where(Y_c>0)\n",
    "\n",
    "Y_c = Y_c[inds]\n",
    "X_c = X_c[inds]\n",
    "print ('X_c')\n",
    "print (X_c)\n",
    "\n",
    "\n",
    "\n",
    "# 0 =< SZA <= 83\n",
    "print('original X_v: ', X_v.shape)\n",
    "rows = np.where((X_v[:,0] >= 0) & (X_v[:,0] <= 83) & (X_v[:,15] > 100) & (X_v[:,15] < 400) & (X_v[:,16] > 100) & (X_v[:,16] < 400) & (X_v[:,17] > 100) & (X_v[:,17] < 400) & (X_v[:,18] > 100) & (X_v[:,18] < 400) & (X_v[:,19] > 100) & (X_v[:,19] < 400) & (X_v[:,10] > 0))\n",
    "print(\"rows:\", rows)\n",
    "print(\"rows.shape:\", len(rows))\n",
    "\n",
    "Latlon = Latlon[rows]\n",
    "Iff = Iff[rows]\n",
    "\n",
    "Y_v = Y_v[rows]\n",
    "X_v = X_v[rows]\n",
    "\n",
    "Y_c = Y_c[rows]\n",
    "X_c = X_c[rows]\n",
    "\n",
    "X_c = np.nan_to_num(X_c)\n",
    "X_v = np.nan_to_num(X_v)\n",
    "\n",
    "print('after SZA X_v: ', X_v.shape)\n",
    "print('after SZA X_c: ', X_c.shape)\n",
    "\n",
    "# pca = decomposition.PCA(n_components=20)\n",
    "# pca.fit(X_s)\n",
    "# X_s = pca.transform(X_s)\n",
    "# print (X_s.shape)\n",
    "\n",
    "#concanate common data\n",
    "# X_v = np.concatenate((X_v, Latlon, Iff), axis=1)\n",
    "X_c = np.concatenate((X_c, Latlon, Iff), axis=1)\n",
    "\n",
    "print (X_v.shape)\n",
    "print (X_c.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "(704800,)\n"
     ]
    }
   ],
   "source": [
    "print(X_c[:,0])\n",
    "print(X_c[:,2])\n",
    "print(X_c[:,1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(704800, 51)\n",
      "[[2]\n",
      " [2]\n",
      " [2]\n",
      " ...\n",
      " [3]\n",
      " [3]\n",
      " [3]]\n",
      "(493360, 20)\n",
      "(493360, 25)\n",
      "(493360, 6)\n",
      "[[-7.33811578e-02 -1.15862914e+00  8.21589628e-01 -2.13746582e-01\n",
      "   1.99623797e+00  2.01215955e+00  1.98477600e+00  1.88350589e+00\n",
      "   1.69663707e+00  6.92207008e-01  1.26696591e+00  6.06449143e-02\n",
      "  -3.94587737e-01 -7.82200217e-01 -6.94167704e-01 -8.56558481e-01\n",
      "  -4.81644112e-01 -1.49800418e-01 -1.26641380e-01 -9.40186576e-02]\n",
      " [ 6.25914276e-02  2.04895210e-01 -1.16085896e+00 -2.88055129e-01\n",
      "  -1.09706561e+00 -1.06784619e+00 -1.02367801e+00 -9.02692722e-01\n",
      "  -7.93362880e-01  5.22677969e-01 -3.54132287e-01  1.10853550e-01\n",
      "  -4.09015819e-01  5.40459857e-01  2.59291590e-01  1.40943711e+00\n",
      "   1.54027862e+00  1.59139303e+00  1.65528737e+00  1.70020683e+00]\n",
      " [ 1.35700696e+00  1.18958808e-01  2.62199887e-01 -3.74379567e-01\n",
      "  -1.21855056e+00 -1.16112649e+00 -1.11789304e+00 -1.08221613e+00\n",
      "  -1.07099576e+00 -1.87262899e+00 -1.12241390e+00 -1.09622570e+00\n",
      "  -4.19836883e-01 -9.34002519e-01 -9.99211466e-01 -4.46442106e-01\n",
      "  -1.05497700e-01  2.50525343e-01  2.77256008e-01  3.06589804e-01]\n",
      " [-1.09692240e+00 -8.87158233e-01  8.47243213e-01 -1.62837270e-01\n",
      "   8.95070913e-01  8.45394997e-01  7.95555361e-01  7.46678693e-01\n",
      "   6.73389000e-01  7.25058154e-01  5.94006338e-01  7.83186409e-01\n",
      "   2.29228221e+00  4.62248652e-01  9.49788874e-01 -5.76480918e-01\n",
      "  -1.27429024e+00 -1.84788130e+00 -1.95965659e+00 -2.02731508e+00]\n",
      " [ 1.01761114e+00  2.37225551e+00 -8.72256641e-01 -6.80151593e-01\n",
      "   7.05160653e-01  8.32286589e-01  9.07442904e-01  8.85485370e-01\n",
      "   9.80545984e-01  6.64222574e-01  9.10259974e-01  4.64873349e-01\n",
      "  -3.78533511e-02 -2.93132816e-01  9.68912900e-02 -1.54190932e+00\n",
      "  -1.37312813e+00 -1.15067940e+00 -1.13073145e+00 -1.10667682e+00]\n",
      " [-3.50679496e-01 -1.20908926e+00 -7.11922010e-01 -3.00229157e-01\n",
      "   9.31324271e-01  9.42420530e-01  9.44050192e-01  9.59399986e-01\n",
      "   9.37065178e-01  7.73726800e-01  9.59321279e-01  1.15341425e+00\n",
      "   4.28173720e-01  1.27422603e+00  1.37040946e+00  5.97331891e-01\n",
      "  -2.59458509e-01 -8.69747851e-01 -8.23089333e-01 -7.85052288e-01]\n",
      " [-9.12235251e-01 -1.40943788e-01  2.15699894e+00 -1.76592308e-01\n",
      "  -4.50146462e-01 -6.02146867e-01 -7.30245878e-01 -8.59084247e-01\n",
      "  -9.81564900e-01 -1.65686480e+00 -1.09254146e+00 -1.09256973e+00\n",
      "  -4.04687393e-01 -9.39777606e-01 -1.01146063e+00  6.62718792e-01\n",
      "   1.06469554e+00  1.13295466e+00  1.12464430e+00  1.07284549e+00]\n",
      " [ 3.19547361e-01  1.57630174e-01 -9.40992932e-02 -2.61177600e-01\n",
      "  -6.44618345e-01 -6.08115885e-01 -5.55471822e-01 -4.59552101e-01\n",
      "  -3.55870989e-01  8.62547115e-01 -2.46155868e-01  6.01574968e-02\n",
      "   2.31053086e-02  6.22136100e-01  6.79121830e-01  9.54264599e-01\n",
      "   5.70924632e-01 -5.29834128e-04 -3.92308169e-02 -1.92576409e-01]\n",
      " [-1.33888930e+00 -6.31111825e-01  1.04035744e+00 -1.49240368e-01\n",
      "  -5.16651068e-01 -6.55516816e-01 -7.83607592e-01 -9.09286037e-01\n",
      "  -1.00142650e+00 -1.69214957e+00 -1.09420104e+00 -1.08318608e+00\n",
      "  -3.99637565e-01 -9.30042460e-01 -1.00553362e+00  2.77118029e-01\n",
      "   6.79173437e-01  9.57888584e-01  9.47136988e-01  9.91905944e-01]]\n",
      "[[-1.24009487e+00 -9.53032124e-01 -8.10443122e-01 -7.33828476e-01\n",
      "  -7.14075660e-01 -7.70101387e-01 -9.09439374e-01 -8.53439342e-01\n",
      "   7.56348435e-01  6.82453815e-01 -7.57052307e-01 -7.74024471e-01\n",
      "  -7.15493663e-02 -1.07306889e-01 -1.01035597e+00 -6.65645938e-01\n",
      "  -4.99372289e-01 -3.82144383e-01 -3.04633611e-01  5.46522617e-02\n",
      "  -6.53183415e-02 -2.07746103e-01  8.27633361e-03 -6.61297180e-01\n",
      "  -3.11184302e-01]\n",
      " [-1.24009487e+00 -9.53032124e-01 -8.10443122e-01 -7.33828476e-01\n",
      "  -7.14075660e-01 -7.70101387e-01 -9.09439374e-01 -8.53439342e-01\n",
      "   7.56348435e-01  6.82453815e-01 -7.57052307e-01 -7.74024471e-01\n",
      "  -7.15493663e-02 -1.07306889e-01 -1.01035597e+00 -6.65645938e-01\n",
      "  -4.99372289e-01 -3.82144383e-01 -3.04633611e-01  5.46522617e-02\n",
      "  -6.53183415e-02 -2.07746103e-01  8.27633361e-03 -6.61297180e-01\n",
      "  -3.11184302e-01]\n",
      " [-1.24009487e+00 -9.53032124e-01 -8.10443122e-01 -7.33828476e-01\n",
      "  -7.14075660e-01 -7.70101387e-01 -9.09439374e-01 -8.53439342e-01\n",
      "   7.56348435e-01  6.82453815e-01 -7.57052307e-01 -7.74024471e-01\n",
      "  -7.15493663e-02 -1.07306889e-01 -1.01035597e+00 -6.65645938e-01\n",
      "   2.08000934e-01 -2.52779258e-03 -9.76309097e-02 -4.06613489e-01\n",
      "  -1.24268363e-01  1.14258412e-01  8.35019537e-03  1.78930651e+00\n",
      "  -5.27785902e-03]\n",
      " [ 2.11774045e-01 -1.63665298e-01 -8.10443122e-01 -7.33828476e-01\n",
      "   1.55009887e+00  1.42647193e+00  1.67214919e+00  1.34904372e+00\n",
      "  -1.77955447e+00 -1.21784233e+00  4.85408408e-01  1.52511874e+00\n",
      "  -2.13800746e-02 -3.31602257e-02  1.12689468e+00  1.66063816e-01\n",
      "  -4.99372289e-01 -3.82144383e-01 -3.04633611e-01  5.46522617e-02\n",
      "  -6.53183415e-02 -2.07746103e-01  8.27633361e-03 -6.61297180e-01\n",
      "  -3.11184302e-01]\n",
      " [ 1.66364296e+00  2.99380201e+00 -8.10443122e-01 -7.33828476e-01\n",
      "   1.55009887e+00  1.42647193e+00 -2.13596680e-02 -2.08108410e-01\n",
      "  -3.16931274e-01 -2.37440934e-01 -3.48420409e-02 -4.29152989e-01\n",
      "   7.31438243e-02  1.50657028e-01  3.36631651e-01  1.90530734e-01\n",
      "  -4.99372289e-01 -3.82144383e-01 -3.04633611e-01  5.46522617e-02\n",
      "  -6.53183415e-02 -2.07746103e-01  8.27633361e-03 -6.61297180e-01\n",
      "  -3.11184302e-01]\n",
      " [ 2.11774045e-01  6.25701528e-01  1.35447067e+00  1.50179878e+00\n",
      "  -7.14075660e-01 -7.70101387e-01 -3.04195103e-01 -3.16269885e-01\n",
      "   1.51895279e-01  1.40905698e-01 -4.06594591e-01 -7.74024471e-01\n",
      "  -4.08729223e-03 -1.16044399e-02  9.11126315e-02  3.08290333e-01\n",
      "   2.08000934e-01  1.82701075e+00  1.91546167e+00 -2.47490722e+00\n",
      "  -2.35479492e+00 -1.60413645e-02  8.29311215e-03  5.63995454e-01\n",
      "   1.48422268e+00]\n",
      " [-1.24009487e+00 -9.53032124e-01 -8.10443122e-01 -7.33828476e-01\n",
      "  -7.14075660e-01 -7.70101387e-01 -9.09439374e-01 -8.53439342e-01\n",
      "   7.56348435e-01  6.82453815e-01 -7.57052307e-01 -7.74024471e-01\n",
      "  -7.15493663e-02 -1.07306889e-01 -1.01035597e+00 -6.65645938e-01\n",
      "   3.03749383e+00  1.94090942e-02 -1.20876775e-01  1.33382542e+00\n",
      "   1.51728036e+00  2.82853526e-02  8.33314877e-03  1.78931423e+00\n",
      "   2.18729761e-01]\n",
      " [ 1.66364296e+00  6.25701528e-01  1.35447067e+00  1.50179878e+00\n",
      "  -7.14075660e-01 -7.70101387e-01  9.87179810e-02  2.11483508e-01\n",
      "   2.18692020e-01  1.52584705e-01 -4.87369951e-01 -7.74024471e-01\n",
      "  -2.99170149e-02 -2.86035819e-02  2.22192541e-01  5.49659158e-01\n",
      "  -4.99372289e-01 -3.82144383e-01 -3.04633611e-01  5.46522617e-02\n",
      "  -6.53183415e-02 -2.07746103e-01  8.27633361e-03 -6.61297180e-01\n",
      "  -3.11184302e-01]\n",
      " [-1.24009487e+00 -9.53032124e-01 -8.10443122e-01 -7.33828476e-01\n",
      "  -7.14075660e-01 -7.70101387e-01 -9.09439374e-01 -8.53439342e-01\n",
      "   7.56348435e-01  6.82453815e-01 -7.57052307e-01 -7.74024471e-01\n",
      "  -7.15493663e-02 -1.07306889e-01 -1.01035597e+00 -6.65645938e-01\n",
      "  -1.45685677e-01 -2.35059088e-01 -2.88247068e-01  9.41109200e-01\n",
      "   1.08278124e+00  2.97068426e-02  8.37560871e-03  5.64049513e-01\n",
      "  -4.74571400e-02]]\n",
      "(211440, 20)\n",
      "(211440, 25)\n",
      "(211440, 6)\n",
      "[[-7.29154217e-01 -1.09373905e-02  1.01399110e+00 -1.93351230e-01\n",
      "  -6.52541256e-01 -7.65182556e-01 -8.46264613e-01 -9.33114524e-01\n",
      "  -1.01484649e+00 -1.73514017e+00 -1.10115054e+00 -1.08879190e+00\n",
      "  -4.16229862e-01 -9.29712455e-01 -1.00415065e+00  6.31289833e-01\n",
      "   1.04835238e+00  1.21211821e+00  1.20655256e+00  1.21126456e+00]\n",
      " [-8.09452959e-01 -9.82018815e-01  6.68381008e-01 -1.64260142e-01\n",
      "  -5.03566233e-01 -4.89320966e-01 -4.31649689e-01 -2.69733874e-01\n",
      "   3.30941644e-02  7.08429889e-01  4.12074828e-01  1.12441027e+00\n",
      "  -2.50455663e-03  1.92219085e+00  1.87045192e+00  1.66607852e+00\n",
      "   1.38766609e+00  1.08371961e+00  1.11417401e+00  1.10662806e+00]\n",
      " [ 2.05523090e-01  1.27662592e-01  1.13370765e+00 -2.43786251e-01\n",
      "   1.59745049e+00  1.61165127e+00  1.61973605e+00  1.63758666e+00\n",
      "   1.59818776e+00  6.48405076e-01  1.52814213e+00  1.51840185e+00\n",
      "   4.08966073e+00  4.69343780e-01  1.43718703e+00 -1.12723123e+00\n",
      "  -2.08219663e+00 -2.56237693e+00 -2.50080936e+00 -2.50735628e+00]\n",
      " [-5.41094270e-02  1.51240011e-01  6.03534568e-01 -2.05683275e-01\n",
      "  -8.44972439e-01 -9.11364664e-01 -9.57234062e-01 -1.00864855e+00\n",
      "  -1.06144072e+00 -1.88682402e+00 -1.13537937e+00 -1.11682100e+00\n",
      "  -4.14065649e-01 -9.61227931e-01 -1.02370980e+00  3.73106813e-01\n",
      "   8.18701290e-01  1.05430618e+00  1.01835306e+00  1.03357131e+00]\n",
      " [ 1.57327829e+00  1.56197889e-01 -1.26204792e+00 -5.39040412e-02\n",
      "  -1.06501378e+00 -9.73863617e-01 -9.07085487e-01 -8.57580517e-01\n",
      "  -8.14083358e-01 -8.10436471e-01 -8.38522063e-01 -7.86565075e-01\n",
      "  -4.06490905e-01 -6.38153035e-01 -7.75960538e-01 -1.07800048e+00\n",
      "  -8.13407138e-01 -4.91129705e-01 -4.67290748e-01 -4.11358195e-01]\n",
      " [-1.37743266e+00 -3.81896206e-01  8.50806150e-01 -1.52876799e-01\n",
      "  -4.86039765e-01 -5.91730331e-01 -6.69195469e-01 -7.48617205e-01\n",
      "  -8.43070549e-01 -1.08825313e+00 -9.39134082e-01 -9.09040055e-01\n",
      "  -2.05579833e-01 -7.69495032e-01 -8.08954261e-01  4.48838157e-01\n",
      "   8.18701290e-01  7.68820707e-01  7.31646409e-01  6.99208051e-01]\n",
      " [-1.28963940e+00 -4.48441847e-01  1.17076280e+00 -1.55406391e-01\n",
      "  -5.39579525e-01 -6.69678524e-01 -7.80509199e-01 -8.88349348e-01\n",
      "  -9.89509543e-01 -1.66416510e+00 -1.08828880e+00 -1.07989571e+00\n",
      "  -4.12983543e-01 -9.24267373e-01 -1.00019930e+00  6.14801140e-01\n",
      "   1.03183722e+00  1.17918021e+00  1.17968047e+00  1.17247449e+00]\n",
      " [-1.38653319e+00 -6.50612743e-01 -1.09776408e-01 -1.76750325e-01\n",
      "  -2.77522693e-01 -3.81410784e-01 -4.55633741e-01 -5.27104768e-01\n",
      "  -5.79501801e-01  2.13912245e-02 -6.20494789e-01 -4.54481176e-01\n",
      "  -4.01801777e-01 -4.59590666e-02  8.14810759e-02  1.34086979e+00\n",
      "   9.81226855e-01  7.23340531e-01  7.08727688e-01  7.08887541e-01]\n",
      " [-4.15453766e-01  1.29425389e-01 -1.00123694e+00 -2.39517392e-01\n",
      "  -4.47025329e-01 -4.87331351e-01 -5.19094026e-01 -5.59377329e-01\n",
      "  -5.91311426e-01 -6.25620753e-02 -6.75157198e-01 -6.05472736e-01\n",
      "  -3.93144927e-01 -3.67218910e-01 -4.90475927e-01  5.62864059e-01\n",
      "   7.80128766e-01  8.73888674e-01  8.30950649e-01  8.63507758e-01]]\n",
      "[[-1.24009487e+00 -9.53032124e-01 -8.10443122e-01 -7.33828476e-01\n",
      "  -7.14075660e-01 -7.70101387e-01 -9.09439374e-01 -8.53439342e-01\n",
      "   7.56348435e-01  6.82453815e-01 -7.57052307e-01 -7.74024471e-01\n",
      "  -7.15493663e-02 -1.07306889e-01 -1.01035597e+00 -6.65645938e-01\n",
      "   2.08000934e-01 -4.20142535e-02 -1.58070182e-01  1.13457329e+00\n",
      "   1.37287706e+00  3.50001777e-02  8.34961088e-03  1.78934856e+00\n",
      "  -9.02028682e-02]\n",
      " [ 2.11774045e-01  6.25701528e-01 -8.10443122e-01 -7.33828476e-01\n",
      "   1.55009887e+00  1.42647193e+00  1.22623750e+00  1.42923241e+00\n",
      "  -1.13434915e+00 -1.43029683e+00 -7.17297489e-01 -7.74024471e-01\n",
      "  -6.43798522e-02 -9.57684328e-02  7.22641197e-01  3.05758014e-02\n",
      "  -4.99372289e-01 -3.82144383e-01 -3.04633611e-01  5.46522617e-02\n",
      "  -6.53183415e-02 -2.07746103e-01  8.27633361e-03 -6.61297180e-01\n",
      "  -3.11184302e-01]\n",
      " [ 2.11774045e-01 -1.63665298e-01 -8.10443122e-01 -7.33828476e-01\n",
      "   1.55009887e+00  1.42647193e+00  2.07983989e+00  1.36396258e+00\n",
      "  -1.91793514e+00 -1.05077598e+00  6.78508451e-01  1.52511874e+00\n",
      "  -6.35733169e-03 -1.23914668e-02  4.59929818e-01  6.87725302e-02\n",
      "  -4.99372289e-01 -3.82144383e-01 -3.04633611e-01  5.46522617e-02\n",
      "  -6.53183415e-02 -2.07746103e-01  8.27633361e-03 -6.61297180e-01\n",
      "  -3.11184302e-01]\n",
      " [-1.24009487e+00 -9.53032124e-01 -8.10443122e-01 -7.33828476e-01\n",
      "  -7.14075660e-01 -7.70101387e-01 -9.09439374e-01 -8.53439342e-01\n",
      "   7.56348435e-01  6.82453815e-01 -7.57052307e-01 -7.74024471e-01\n",
      "  -7.15493663e-02 -1.07306889e-01 -1.01035597e+00 -6.65645938e-01\n",
      "   1.97643399e+00 -2.08734793e-01 -2.69650369e-01  1.13564265e+00\n",
      "   1.25782243e+00 -7.09200596e-02  8.30746576e-03  5.64005780e-01\n",
      "  -1.11376561e-01]\n",
      " [ 2.11774045e-01 -1.63665298e-01  1.35447067e+00  1.50179878e+00\n",
      "  -7.14075660e-01 -7.70101387e-01 -5.86074996e-01 -6.16511220e-01\n",
      "   3.18693870e-01  2.75883802e-01  1.31189464e+00  1.52511874e+00\n",
      "   5.28100938e-02  1.27960414e-01 -1.32698089e-01  3.87426775e-01\n",
      "  -4.99372289e-01 -3.82144383e-01 -3.04633611e-01  5.46522617e-02\n",
      "  -6.53183415e-02 -2.07746103e-01  8.27633361e-03 -6.61297180e-01\n",
      "  -3.11184302e-01]\n",
      " [ 2.11774045e-01 -1.63665298e-01 -8.10443122e-01 -7.33828476e-01\n",
      "   1.55009887e+00  1.42647193e+00  1.31860496e+00  1.26326055e+00\n",
      "  -7.64746238e-01 -6.17288524e-01 -5.74197643e-01 -7.74024471e-01\n",
      "  -4.39353999e-02 -5.70354519e-02  8.04963567e-01  2.96187606e-01\n",
      "  -1.45685677e-01 -1.38536671e-01 -2.88247067e-01  1.34620957e+00\n",
      "   1.62709235e+00  5.77128132e-01  8.34157193e-03  2.57687288e-01\n",
      "  -7.64427691e-02]\n",
      " [-1.24009487e+00 -9.53032124e-01 -8.10443122e-01 -7.33828476e-01\n",
      "  -7.14075660e-01 -7.70101387e-01 -9.09439374e-01 -8.53439342e-01\n",
      "   7.56348435e-01  6.82453815e-01 -7.57052307e-01 -7.74024471e-01\n",
      "  -7.15493663e-02 -1.07306889e-01 -1.01035597e+00 -6.65645938e-01\n",
      "   2.08000934e-01 -5.51764013e-02 -2.04561929e-01  1.33429490e+00\n",
      "   1.44470349e+00  4.38796672e-02  8.37811797e-03  1.78937100e+00\n",
      "  -6.03665692e-02]\n",
      " [ 2.11774045e-01  6.25701528e-01  1.35447067e+00  1.50179878e+00\n",
      "  -7.14075660e-01 -7.70101387e-01 -5.47853993e-01 -5.15809151e-01\n",
      "   1.12797771e+00  1.25039615e+00 -6.90724087e-01 -7.74024471e-01\n",
      "  -5.13212263e-02 -8.99204811e-02  5.14237323e-03 -2.19209185e-01\n",
      "   2.08000934e-01 -1.99960027e-01 -2.13860275e-01  1.03245726e+00\n",
      "   1.08311931e+00  8.91303928e-01  8.10632954e-03  5.63941947e-01\n",
      "   2.47720913e+00]\n",
      " [ 2.11774045e-01 -1.63665298e-01  1.35447067e+00  1.50179878e+00\n",
      "  -7.14075660e-01 -7.70101387e-01 -5.54224147e-01 -5.49376513e-01\n",
      "   1.17782817e+00  1.31705821e+00 -4.10790698e-01 -7.74024471e-01\n",
      "  -3.32938884e-03  1.48351314e-02  5.14237323e-03  3.30654262e-01\n",
      "  -1.45685677e-01  1.85959002e-03 -2.69650369e-01  8.51854052e-01\n",
      "   8.42394844e-01  5.31698269e-01  8.19102144e-03  5.63948363e-01\n",
      "   4.18600739e-01]]\n"
     ]
    }
   ],
   "source": [
    "# combine data and split latter to define ground truth for MLR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "n1=20\n",
    "n2=25\n",
    "X=np.concatenate((X_v, X_c), axis=1)\n",
    "Y=Y_v\n",
    "\n",
    "Y=Y-1\n",
    "print (X.shape)\n",
    "print (Y_v)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X, Y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=Y)\n",
    "\n",
    "# x_valid, x_test, y_valid, y_test = train_test_split(x_temp, y_temp,\n",
    "#                                                     test_size=0.5,\n",
    "#                                                     random_state=0,\n",
    "#                                                     stratify=y_temp)\n",
    "\n",
    "\n",
    "# # feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "sc_X.fit(x_train)\n",
    "x_train=sc_X.transform(x_train)\n",
    "x_valid=sc_X.transform(x_valid)\n",
    "# x_test=sc_X.transform(x_test)\n",
    "\n",
    "# feature scaling\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc_X_v = StandardScaler()\n",
    "# sc_X_c = StandardScaler()\n",
    "\n",
    "\n",
    "x_train_v = x_train[:, 0:20]\n",
    "x_train_c = x_train[:, 20:45]\n",
    "x_train_comm = x_train[:, 45:51]\n",
    "# sc_X_v.fit(x_train_v)\n",
    "# sc_X_c.fit(x_train_c)\n",
    "print(x_train_v.shape)\n",
    "print(x_train_c.shape)\n",
    "print(x_train_comm.shape)\n",
    "print(x_train_v[1:10, ])\n",
    "print(x_train_c[1:10, ])\n",
    "\n",
    "x_valid_v = x_valid[:, 0:20]\n",
    "x_valid_c = x_valid[:, 20:45]\n",
    "x_valid_comm = x_valid[:, 45:51]\n",
    "# x_valid_v = sc_X_v.transform(x_valid_v)\n",
    "# x_valid_c = sc_X_c.transform(x_valid_c)\n",
    "print(x_valid_v.shape)\n",
    "print(x_valid_c.shape)\n",
    "print(x_valid_comm.shape)\n",
    "print(x_valid_v[1:10, ])\n",
    "print(x_valid_c[1:10, ])\n",
    "\n",
    "# x_test_v = x_test[:, 0:20]\n",
    "# x_test_c = x_test[:, 20:45]\n",
    "# x_test_v = sc_X_v.transform(x_test_v)\n",
    "# x_test_c = sc_X_c.transform(x_test_c)\n",
    "# print(x_test_v.shape)\n",
    "# print(x_test_c.shape)\n",
    "# print(x_test_v[1:10, ])\n",
    "# print(x_test_c[1:10, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #use MLP to impute the virrs data to Calipso\n",
    "# from sklearn.neural_network import MLPRegressor\n",
    "# # hidden = 100\n",
    "# mlpReg = MLPRegressor(activation = 'relu',hidden_layer_sizes = (64, 128, 128,64,),\n",
    "#                                max_iter=1000,\n",
    "#                                warm_start=True,solver='adam')\n",
    "# mlpReg.fit(x_train_v, x_train_c)\n",
    "# x_train_pt = mlpReg.predict(x_train_v)\n",
    "# x_test_pt = mlpReg.predict(x_test_v)\n",
    "# x_valid_pt = mlpReg.predict(x_valid_v)\n",
    "\n",
    "# print(x_train_pt.shape)\n",
    "# print(x_test_pt.shape)\n",
    "# print(x_valid_pt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/xin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               5376      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 25)                1625      \n",
      "=================================================================\n",
      "Total params: 179,737\n",
      "Trainable params: 179,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/xin/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/150\n",
      "493360/493360 [==============================] - 19s 39us/step - loss: 0.6203 - mean_squared_error: 0.6203 - mean_absolute_error: 0.36812s\n",
      "Epoch 2/150\n",
      "493360/493360 [==============================] - 14s 28us/step - loss: 0.5839 - mean_squared_error: 0.5839 - mean_absolute_error: 0.3410\n",
      "Epoch 3/150\n",
      "493360/493360 [==============================] - 14s 28us/step - loss: 0.5693 - mean_squared_error: 0.5693 - mean_absolute_error: 0.3332\n",
      "Epoch 4/150\n",
      "493360/493360 [==============================] - 15s 30us/step - loss: 0.5596 - mean_squared_error: 0.5596 - mean_absolute_error: 0.3287\n",
      "Epoch 5/150\n",
      "493360/493360 [==============================] - 14s 28us/step - loss: 0.5482 - mean_squared_error: 0.5482 - mean_absolute_error: 0.3229\n",
      "Epoch 6/150\n",
      "493360/493360 [==============================] - 14s 28us/step - loss: 0.5428 - mean_squared_error: 0.5428 - mean_absolute_error: 0.3215\n",
      "Epoch 7/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.5378 - mean_squared_error: 0.5378 - mean_absolute_error: 0.3204\n",
      "Epoch 8/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.5395 - mean_squared_error: 0.5395 - mean_absolute_error: 0.3198\n",
      "Epoch 9/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.5377 - mean_squared_error: 0.5377 - mean_absolute_error: 0.3220\n",
      "Epoch 10/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.5315 - mean_squared_error: 0.5315 - mean_absolute_error: 0.3166\n",
      "Epoch 11/150\n",
      "493360/493360 [==============================] - 12s 24us/step - loss: 0.5369 - mean_squared_error: 0.5369 - mean_absolute_error: 0.3196\n",
      "Epoch 12/150\n",
      "493360/493360 [==============================] - 13s 27us/step - loss: 0.5245 - mean_squared_error: 0.5245 - mean_absolute_error: 0.3131\n",
      "Epoch 13/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.5341 - mean_squared_error: 0.5341 - mean_absolute_error: 0.3196\n",
      "Epoch 14/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.5289 - mean_squared_error: 0.5289 - mean_absolute_error: 0.3156\n",
      "Epoch 15/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.5446 - mean_squared_error: 0.5446 - mean_absolute_error: 0.3257\n",
      "Epoch 16/150\n",
      "493360/493360 [==============================] - 13s 25us/step - loss: 0.5292 - mean_squared_error: 0.5292 - mean_absolute_error: 0.3144\n",
      "Epoch 17/150\n",
      "493360/493360 [==============================] - 14s 28us/step - loss: 0.5214 - mean_squared_error: 0.5214 - mean_absolute_error: 0.3128\n",
      "Epoch 18/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.5184 - mean_squared_error: 0.5184 - mean_absolute_error: 0.3096\n",
      "Epoch 19/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.5129 - mean_squared_error: 0.5129 - mean_absolute_error: 0.3132\n",
      "Epoch 20/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.5105 - mean_squared_error: 0.5105 - mean_absolute_error: 0.3080\n",
      "Epoch 21/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.5216 - mean_squared_error: 0.5216 - mean_absolute_error: 0.3189\n",
      "Epoch 22/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.5137 - mean_squared_error: 0.5137 - mean_absolute_error: 0.3136\n",
      "Epoch 23/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.5161 - mean_squared_error: 0.5161 - mean_absolute_error: 0.3079\n",
      "Epoch 24/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.5197 - mean_squared_error: 0.5197 - mean_absolute_error: 0.3113\n",
      "Epoch 25/150\n",
      "493360/493360 [==============================] - 13s 27us/step - loss: 0.5094 - mean_squared_error: 0.5094 - mean_absolute_error: 0.3085\n",
      "Epoch 26/150\n",
      "493360/493360 [==============================] - 14s 28us/step - loss: 0.5345 - mean_squared_error: 0.5345 - mean_absolute_error: 0.3203\n",
      "Epoch 27/150\n",
      "493360/493360 [==============================] - 13s 27us/step - loss: 0.5075 - mean_squared_error: 0.5075 - mean_absolute_error: 0.3091\n",
      "Epoch 28/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.5128 - mean_squared_error: 0.5128 - mean_absolute_error: 0.3094\n",
      "Epoch 29/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.5041 - mean_squared_error: 0.5041 - mean_absolute_error: 0.3056\n",
      "Epoch 30/150\n",
      "493360/493360 [==============================] - 14s 29us/step - loss: 0.5025 - mean_squared_error: 0.5025 - mean_absolute_error: 0.3052\n",
      "Epoch 31/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.5018 - mean_squared_error: 0.5018 - mean_absolute_error: 0.3051\n",
      "Epoch 32/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.5000 - mean_squared_error: 0.5000 - mean_absolute_error: 0.3038\n",
      "Epoch 33/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.5026 - mean_squared_error: 0.5026 - mean_absolute_error: 0.3081\n",
      "Epoch 34/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.4965 - mean_squared_error: 0.4965 - mean_absolute_error: 0.3023\n",
      "Epoch 35/150\n",
      "493360/493360 [==============================] - 14s 28us/step - loss: 0.4957 - mean_squared_error: 0.4957 - mean_absolute_error: 0.3029\n",
      "Epoch 36/150\n",
      "493360/493360 [==============================] - 14s 28us/step - loss: 0.4963 - mean_squared_error: 0.4963 - mean_absolute_error: 0.3018\n",
      "Epoch 37/150\n",
      "493360/493360 [==============================] - 14s 29us/step - loss: 0.5053 - mean_squared_error: 0.5053 - mean_absolute_error: 0.3043\n",
      "Epoch 38/150\n",
      "493360/493360 [==============================] - 14s 28us/step - loss: 0.5029 - mean_squared_error: 0.5029 - mean_absolute_error: 0.3027\n",
      "Epoch 39/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.4970 - mean_squared_error: 0.4970 - mean_absolute_error: 0.3031\n",
      "Epoch 40/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4964 - mean_squared_error: 0.4964 - mean_absolute_error: 0.3041\n",
      "Epoch 41/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.5053 - mean_squared_error: 0.5053 - mean_absolute_error: 0.3047\n",
      "Epoch 42/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.4831 - mean_squared_error: 0.4831 - mean_absolute_error: 0.2956\n",
      "Epoch 43/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4987 - mean_squared_error: 0.4987 - mean_absolute_error: 0.3015\n",
      "Epoch 44/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.4701 - mean_squared_error: 0.4701 - mean_absolute_error: 0.2993\n",
      "Epoch 45/150\n",
      "493360/493360 [==============================] - 14s 27us/step - loss: 0.4936 - mean_squared_error: 0.4936 - mean_absolute_error: 0.2996\n",
      "Epoch 46/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.4932 - mean_squared_error: 0.4932 - mean_absolute_error: 0.2987\n",
      "Epoch 47/150\n",
      "493360/493360 [==============================] - 14s 28us/step - loss: 0.4864 - mean_squared_error: 0.4864 - mean_absolute_error: 0.2966\n",
      "Epoch 48/150\n",
      "493360/493360 [==============================] - 15s 30us/step - loss: 0.4841 - mean_squared_error: 0.4841 - mean_absolute_error: 0.2974\n",
      "Epoch 49/150\n",
      "493360/493360 [==============================] - 14s 28us/step - loss: 0.4951 - mean_squared_error: 0.4951 - mean_absolute_error: 0.2991\n",
      "Epoch 50/150\n",
      "493360/493360 [==============================] - 16s 32us/step - loss: 0.4792 - mean_squared_error: 0.4792 - mean_absolute_error: 0.2978\n",
      "Epoch 51/150\n",
      "493360/493360 [==============================] - 14s 29us/step - loss: 0.4813 - mean_squared_error: 0.4813 - mean_absolute_error: 0.2940\n",
      "Epoch 52/150\n",
      "493360/493360 [==============================] - 14s 29us/step - loss: 0.4969 - mean_squared_error: 0.4969 - mean_absolute_error: 0.3041\n",
      "Epoch 53/150\n",
      "493360/493360 [==============================] - 15s 30us/step - loss: 0.5097 - mean_squared_error: 0.5097 - mean_absolute_error: 0.3089\n",
      "Epoch 54/150\n",
      "493360/493360 [==============================] - 14s 29us/step - loss: 0.4916 - mean_squared_error: 0.4916 - mean_absolute_error: 0.3010\n",
      "Epoch 55/150\n",
      "493360/493360 [==============================] - 14s 29us/step - loss: 0.4895 - mean_squared_error: 0.4895 - mean_absolute_error: 0.3026\n",
      "Epoch 56/150\n",
      "493360/493360 [==============================] - 14s 29us/step - loss: 0.4724 - mean_squared_error: 0.4724 - mean_absolute_error: 0.2938\n",
      "Epoch 57/150\n",
      "493360/493360 [==============================] - 15s 31us/step - loss: 0.5004 - mean_squared_error: 0.5004 - mean_absolute_error: 0.3035\n",
      "Epoch 58/150\n",
      "493360/493360 [==============================] - 14s 29us/step - loss: 0.4774 - mean_squared_error: 0.4774 - mean_absolute_error: 0.2966\n",
      "Epoch 59/150\n",
      "493360/493360 [==============================] - 14s 28us/step - loss: 0.4716 - mean_squared_error: 0.4716 - mean_absolute_error: 0.29260s - loss: 0.4716 - mean_squared_error: 0.4716 - mean_abso\n",
      "Epoch 60/150\n",
      "493360/493360 [==============================] - 15s 31us/step - loss: 0.4678 - mean_squared_error: 0.4678 - mean_absolute_error: 0.2941\n",
      "Epoch 61/150\n",
      "493360/493360 [==============================] - 14s 29us/step - loss: 0.4737 - mean_squared_error: 0.4737 - mean_absolute_error: 0.2940\n",
      "Epoch 62/150\n",
      "493360/493360 [==============================] - 14s 29us/step - loss: 0.4509 - mean_squared_error: 0.4509 - mean_absolute_error: 0.2922\n",
      "Epoch 63/150\n",
      "493360/493360 [==============================] - 14s 29us/step - loss: 0.4838 - mean_squared_error: 0.4838 - mean_absolute_error: 0.2960\n",
      "Epoch 64/150\n",
      "493360/493360 [==============================] - 15s 30us/step - loss: 0.4720 - mean_squared_error: 0.4720 - mean_absolute_error: 0.2899\n",
      "Epoch 65/150\n",
      "493360/493360 [==============================] - 14s 29us/step - loss: 0.4695 - mean_squared_error: 0.4695 - mean_absolute_error: 0.2905\n",
      "Epoch 66/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.4727 - mean_squared_error: 0.4727 - mean_absolute_error: 0.2917\n",
      "Epoch 67/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4640 - mean_squared_error: 0.4640 - mean_absolute_error: 0.2919\n",
      "Epoch 68/150\n",
      "493360/493360 [==============================] - 13s 27us/step - loss: 0.4802 - mean_squared_error: 0.4802 - mean_absolute_error: 0.2993\n",
      "Epoch 69/150\n",
      "493360/493360 [==============================] - 13s 25us/step - loss: 0.4833 - mean_squared_error: 0.4833 - mean_absolute_error: 0.2984\n",
      "Epoch 70/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4731 - mean_squared_error: 0.4731 - mean_absolute_error: 0.2912\n",
      "Epoch 71/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.5079 - mean_squared_error: 0.5079 - mean_absolute_error: 0.3079\n",
      "Epoch 72/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4941 - mean_squared_error: 0.4941 - mean_absolute_error: 0.3000\n",
      "Epoch 73/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4809 - mean_squared_error: 0.4809 - mean_absolute_error: 0.2965\n",
      "Epoch 74/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4739 - mean_squared_error: 0.4739 - mean_absolute_error: 0.2914\n",
      "Epoch 75/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4670 - mean_squared_error: 0.4670 - mean_absolute_error: 0.2898\n",
      "Epoch 76/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4712 - mean_squared_error: 0.4712 - mean_absolute_error: 0.2978\n",
      "Epoch 77/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4973 - mean_squared_error: 0.4973 - mean_absolute_error: 0.3027\n",
      "Epoch 78/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4698 - mean_squared_error: 0.4698 - mean_absolute_error: 0.2937\n",
      "Epoch 79/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4659 - mean_squared_error: 0.4659 - mean_absolute_error: 0.2935\n",
      "Epoch 80/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4444 - mean_squared_error: 0.4444 - mean_absolute_error: 0.2926\n",
      "Epoch 81/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4577 - mean_squared_error: 0.4577 - mean_absolute_error: 0.2922\n",
      "Epoch 82/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4808 - mean_squared_error: 0.4808 - mean_absolute_error: 0.2947\n",
      "Epoch 83/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4700 - mean_squared_error: 0.4700 - mean_absolute_error: 0.2903\n",
      "Epoch 84/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4552 - mean_squared_error: 0.4552 - mean_absolute_error: 0.2882\n",
      "Epoch 85/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4676 - mean_squared_error: 0.4676 - mean_absolute_error: 0.2978\n",
      "Epoch 86/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4831 - mean_squared_error: 0.4831 - mean_absolute_error: 0.2995\n",
      "Epoch 87/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4630 - mean_squared_error: 0.4630 - mean_absolute_error: 0.2869\n",
      "Epoch 88/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4629 - mean_squared_error: 0.4629 - mean_absolute_error: 0.2882\n",
      "Epoch 89/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4800 - mean_squared_error: 0.4800 - mean_absolute_error: 0.2903\n",
      "Epoch 90/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4597 - mean_squared_error: 0.4597 - mean_absolute_error: 0.2895\n",
      "Epoch 91/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4761 - mean_squared_error: 0.4761 - mean_absolute_error: 0.2949\n",
      "Epoch 92/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4713 - mean_squared_error: 0.4713 - mean_absolute_error: 0.2932\n",
      "Epoch 93/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4643 - mean_squared_error: 0.4643 - mean_absolute_error: 0.2957\n",
      "Epoch 94/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4638 - mean_squared_error: 0.4638 - mean_absolute_error: 0.2920\n",
      "Epoch 95/150\n",
      "493360/493360 [==============================] - ETA: 0s - loss: 0.4640 - mean_squared_error: 0.4640 - mean_absolute_error: 0.29 - 13s 26us/step - loss: 0.4650 - mean_squared_error: 0.4650 - mean_absolute_error: 0.2936\n",
      "Epoch 96/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.4566 - mean_squared_error: 0.4566 - mean_absolute_error: 0.2921\n",
      "Epoch 97/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.4693 - mean_squared_error: 0.4693 - mean_absolute_error: 0.2925\n",
      "Epoch 98/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4662 - mean_squared_error: 0.4662 - mean_absolute_error: 0.2882\n",
      "Epoch 99/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4638 - mean_squared_error: 0.4638 - mean_absolute_error: 0.2904\n",
      "Epoch 100/150\n",
      "493360/493360 [==============================] - 13s 25us/step - loss: 0.4677 - mean_squared_error: 0.4677 - mean_absolute_error: 0.2914\n",
      "Epoch 101/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4638 - mean_squared_error: 0.4638 - mean_absolute_error: 0.28860s - loss: 0.4668 - mean_squared_error: 0.4668 - mean_ab\n",
      "Epoch 102/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4750 - mean_squared_error: 0.4750 - mean_absolute_error: 0.2963\n",
      "Epoch 103/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4674 - mean_squared_error: 0.4674 - mean_absolute_error: 0.2927\n",
      "Epoch 104/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4702 - mean_squared_error: 0.4702 - mean_absolute_error: 0.29053s - los - ETA: 1s - loss: 0\n",
      "Epoch 105/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4599 - mean_squared_error: 0.4599 - mean_absolute_error: 0.2955\n",
      "Epoch 106/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4594 - mean_squared_error: 0.4594 - mean_absolute_error: 0.28800s - loss: 0.4650 - mean_squared_error: 0.4650 -\n",
      "Epoch 107/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4729 - mean_squared_error: 0.4729 - mean_absolute_error: 0.2902\n",
      "Epoch 108/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4547 - mean_squared_error: 0.4547 - mean_absolute_error: 0.28934s - ETA: 3s - loss: 0.4537 - mean_squared_error: \n",
      "Epoch 109/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4622 - mean_squared_error: 0.4622 - mean_absolute_error: 0.2916\n",
      "Epoch 110/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4726 - mean_squared_error: 0.4726 - mean_absolute_error: 0.29784s - loss: 0.4634 - mean_squared_error: 0. - ETA: 1s - loss: 0\n",
      "Epoch 111/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.4698 - mean_squared_error: 0.4698 - mean_absolute_error: 0.2903\n",
      "Epoch 112/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4568 - mean_squared_error: 0.4568 - mean_absolute_error: 0.28723s - loss: 0.4487 - mean_squared_error:  - ETA: 2s - loss: 0.4469 - mean_squared - ETA: 1s -\n",
      "Epoch 113/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4716 - mean_squared_error: 0.4716 - mean_absolute_error: 0.2889\n",
      "Epoch 114/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4626 - mean_squared_error: 0.4626 - mean_absolute_error: 0.2910\n",
      "Epoch 115/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4633 - mean_squared_error: 0.4633 - mean_absolute_error: 0.28810s - loss: 0.4627 - mean_squared_error: 0.4627 - mean_absolute_error: \n",
      "Epoch 116/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4600 - mean_squared_error: 0.4600 - mean_absolute_error: 0.28941s -\n",
      "Epoch 117/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4705 - mean_squared_error: 0.4705 - mean_absolute_error: 0.2933\n",
      "Epoch 118/150\n",
      "493360/493360 [==============================] - 13s 27us/step - loss: 0.4523 - mean_squared_error: 0.4523 - mean_absolute_error: 0.2896\n",
      "Epoch 119/150\n",
      "493360/493360 [==============================] - 14s 29us/step - loss: 0.4736 - mean_squared_error: 0.4736 - mean_absolute_error: 0.2951\n",
      "Epoch 120/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.4556 - mean_squared_error: 0.4556 - mean_absolute_error: 0.2880\n",
      "Epoch 121/150\n",
      "493360/493360 [==============================] - 12s 24us/step - loss: 0.4640 - mean_squared_error: 0.4640 - mean_absolute_error: 0.2946\n",
      "Epoch 122/150\n",
      "493360/493360 [==============================] - 12s 24us/step - loss: 0.4958 - mean_squared_error: 0.4958 - mean_absolute_error: 0.2968\n",
      "Epoch 123/150\n",
      "493360/493360 [==============================] - 12s 25us/step - loss: 0.4657 - mean_squared_error: 0.4657 - mean_absolute_error: 0.2888\n",
      "Epoch 124/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.4453 - mean_squared_error: 0.4453 - mean_absolute_error: 0.2851\n",
      "Epoch 125/150\n",
      "493360/493360 [==============================] - 14s 27us/step - loss: 0.4470 - mean_squared_error: 0.4470 - mean_absolute_error: 0.2865\n",
      "Epoch 126/150\n",
      "493360/493360 [==============================] - 14s 27us/step - loss: 0.4475 - mean_squared_error: 0.4475 - mean_absolute_error: 0.2917\n",
      "Epoch 127/150\n",
      "493360/493360 [==============================] - 14s 29us/step - loss: 0.4542 - mean_squared_error: 0.4542 - mean_absolute_error: 0.2878\n",
      "Epoch 128/150\n",
      "493360/493360 [==============================] - 14s 28us/step - loss: 0.4524 - mean_squared_error: 0.4524 - mean_absolute_error: 0.2915\n",
      "Epoch 129/150\n",
      "493360/493360 [==============================] - 13s 27us/step - loss: 0.4713 - mean_squared_error: 0.4713 - mean_absolute_error: 0.2986\n",
      "Epoch 130/150\n",
      "493360/493360 [==============================] - 13s 26us/step - loss: 0.4441 - mean_squared_error: 0.4441 - mean_absolute_error: 0.2894\n",
      "Epoch 131/150\n",
      "493360/493360 [==============================] - 14s 27us/step - loss: 0.4289 - mean_squared_error: 0.4289 - mean_absolute_error: 0.2851\n",
      "Epoch 132/150\n",
      "493360/493360 [==============================] - 14s 28us/step - loss: 0.4660 - mean_squared_error: 0.4660 - mean_absolute_error: 0.2911\n",
      "Epoch 133/150\n",
      "493360/493360 [==============================] - 14s 28us/step - loss: 0.4787 - mean_squared_error: 0.4787 - mean_absolute_error: 0.2957\n",
      "Epoch 134/150\n",
      "493360/493360 [==============================] - 14s 28us/step - loss: 0.4480 - mean_squared_error: 0.4480 - mean_absolute_error: 0.2853\n",
      "Epoch 135/150\n",
      "493360/493360 [==============================] - 14s 28us/step - loss: 0.4472 - mean_squared_error: 0.4472 - mean_absolute_error: 0.2895\n",
      "Epoch 136/150\n",
      "493360/493360 [==============================] - 14s 28us/step - loss: 0.4266 - mean_squared_error: 0.4266 - mean_absolute_error: 0.2847\n",
      "Epoch 137/150\n",
      "493360/493360 [==============================] - 14s 28us/step - loss: 0.4435 - mean_squared_error: 0.4435 - mean_absolute_error: 0.2880\n",
      "Epoch 138/150\n",
      "493360/493360 [==============================] - 13s 27us/step - loss: 0.4561 - mean_squared_error: 0.4561 - mean_absolute_error: 0.2904\n",
      "Epoch 139/150\n",
      "493360/493360 [==============================] - 13s 27us/step - loss: 0.4607 - mean_squared_error: 0.4607 - mean_absolute_error: 0.2897\n",
      "Epoch 140/150\n",
      "493360/493360 [==============================] - 13s 27us/step - loss: 0.4552 - mean_squared_error: 0.4552 - mean_absolute_error: 0.2867\n",
      "Epoch 141/150\n",
      "493360/493360 [==============================] - 13s 27us/step - loss: 0.4704 - mean_squared_error: 0.4704 - mean_absolute_error: 0.2922\n",
      "Epoch 142/150\n",
      "493360/493360 [==============================] - 13s 27us/step - loss: 0.4434 - mean_squared_error: 0.4434 - mean_absolute_error: 0.2788\n",
      "Epoch 143/150\n",
      "493360/493360 [==============================] - 13s 27us/step - loss: 0.4530 - mean_squared_error: 0.4530 - mean_absolute_error: 0.2852\n",
      "Epoch 144/150\n",
      "493360/493360 [==============================] - 14s 27us/step - loss: 0.4547 - mean_squared_error: 0.4547 - mean_absolute_error: 0.2884\n",
      "Epoch 145/150\n",
      "493360/493360 [==============================] - 13s 27us/step - loss: 0.4538 - mean_squared_error: 0.4538 - mean_absolute_error: 0.2922\n",
      "Epoch 146/150\n",
      "493360/493360 [==============================] - 13s 27us/step - loss: 0.4561 - mean_squared_error: 0.4561 - mean_absolute_error: 0.2918\n",
      "Epoch 147/150\n",
      "493360/493360 [==============================] - 14s 28us/step - loss: 0.4457 - mean_squared_error: 0.4457 - mean_absolute_error: 0.2887\n",
      "Epoch 148/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493360/493360 [==============================] - 13s 27us/step - loss: 0.4361 - mean_squared_error: 0.4361 - mean_absolute_error: 0.2860\n",
      "Epoch 149/150\n",
      "493360/493360 [==============================] - 13s 27us/step - loss: 0.4519 - mean_squared_error: 0.4519 - mean_absolute_error: 0.2918\n",
      "Epoch 150/150\n",
      "493360/493360 [==============================] - 14s 27us/step - loss: 0.4534 - mean_squared_error: 0.4534 - mean_absolute_error: 0.2838\n",
      "(493360, 25)\n",
      "(211440, 25)\n"
     ]
    }
   ],
   "source": [
    "# define the sequential model with keras\n",
    "model_reg = Sequential()\n",
    "model_reg.add(Dense(256,input_shape=(x_train_v.shape[1],)))\n",
    "model_reg.add(Dense(256, activation= \"relu\"))\n",
    "model_reg.add(Dense(256, activation= \"relu\"))\n",
    "model_reg.add(Dense(128, activation= \"relu\"))\n",
    "model_reg.add(Dense(64, activation= \"relu\"))\n",
    "model_reg.add(Dense(x_train_c.shape[1], activation= \"linear\"))\n",
    "model_reg.summary()\n",
    "model_reg.compile(loss= \"mse\" , optimizer='adam', metrics=['mse','mae'])\n",
    "model_reg.fit(x_train_v, x_train_c, epochs=100, batch_size=128, verbose=1)\n",
    "\n",
    "x_train_pt = model_reg.predict(x_train_v)\n",
    "# x_test_pt = model_reg.predict(x_test_v)\n",
    "x_valid_pt = model_reg.predict(x_valid_v)\n",
    "print(x_train_pt.shape)\n",
    "# print(x_test_pt.shape)\n",
    "print(x_valid_pt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine the viirs and imputed calipso data\n",
    "# x_train_pt = np.concatenate((x_train_v, x_train_pt),axis=1)\n",
    "# print(x_train_pt.shape)\n",
    "# x_valid_pt = np.concatenate((x_valid_v, x_valid_pt),axis=1)\n",
    "# print(x_valid_pt.shape)\n",
    "# x_test_pt = np.concatenate((x_test_v, x_test_pt),axis=1)\n",
    "# print(x_test_pt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(493360, 25)\n",
      "(211440, 25)\n"
     ]
    }
   ],
   "source": [
    "# xt_train = x_train_v\n",
    "# xt_valid = x_valid_v\n",
    "# xt_test = x_test_v\n",
    "\n",
    "# xs_train = x_train_pt[:, 0:np.shape(xt_train)[1]]\n",
    "# xs_valid = x_valid_pt[:, 0:np.shape(xt_valid)[1]]\n",
    "# xs_test= x_test_pt[:, 0:np.shape(xt_test)[1]]\n",
    "# xs_train = x_train_pt[:, 5:25]\n",
    "# xs_valid = x_valid_pt[:, 5:25]\n",
    "# xs_test= x_test_pt[:, 5:25]\n",
    "\n",
    "xs_train = x_train_c\n",
    "xs_valid = x_valid_c\n",
    "# xs_test = x_test_c\n",
    "\n",
    "xt_train = x_train_pt \n",
    "xt_valid = x_valid_pt \n",
    "# xt_test = x_test_pt \n",
    "\n",
    "print(xs_train.shape)\n",
    "print(xs_valid.shape)\n",
    "# print(xs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_train = y_train\n",
    "yt_train = y_train\n",
    "\n",
    "ys_valid = y_valid\n",
    "yt_valid = y_valid\n",
    "\n",
    "batch_size = 1024\n",
    "epochs = 30\n",
    "\n",
    "#SAMPLING_ITERATIONS = 3000\n",
    "n_iters = 300\n",
    "ip_size = 25\n",
    "n_classes = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Activation, BatchNormalization, PReLU, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def build_models(n_neurons):\n",
    "    \"\"\"Creates three different models, one used for source only training, two used for domain adaptation\"\"\"\n",
    "    inputs = Input(shape=(n_neurons,)) \n",
    "    #n_classes = 6\n",
    "    x1 = Dense(n_neurons, activation='relu')(inputs)\n",
    "    #x1 = BatchNormalization()(x1)\n",
    "    x2 = Dense(200, activation='relu')(x1)\n",
    "    #x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "    x3 = Dense(100, activation='relu')(x2)\n",
    "    #x3 = BatchNormalization()(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "    x4 = Dense(100, activation='relu')(x3)\n",
    "    #x4 = BatchNormalization()(x4)\n",
    "    #x4 = Activation(\"elu\")(x4)  \n",
    "\n",
    "    \n",
    "    x5 = Dense(50, activation='relu', name=\"mo1\")(x4)\n",
    "    #x5 = BatchNormalization()(x5)\n",
    "    source_classifier = Dense(n_classes, activation='softmax', name=\"mo\")(x5)  \n",
    "    \n",
    "    \n",
    "    domain_classifier = Dense(100, activation='relu', name=\"do4\")(x4)\n",
    "    domain_classifier = Dense(32, activation='relu', name=\"do5\")(domain_classifier)\n",
    "    #domain_classifier = BatchNormalization(name=\"do6\")(domain_classifier)\n",
    "    #domain_classifier = Activation(\"elu\", name=\"do7\")(domain_classifier)\n",
    "    domain_classifier = Dropout(0.5)(domain_classifier)\n",
    "\n",
    "    domain_classifier = Dense(2, activation='softmax', name=\"do\")(domain_classifier)\n",
    "\n",
    "    comb_model = Model(inputs=inputs, outputs=[source_classifier, domain_classifier])\n",
    "    comb_model.compile(optimizer=\"Adam\",\n",
    "              loss={'mo': 'categorical_crossentropy', 'do': 'binary_crossentropy'},\n",
    "              loss_weights={'mo': 2, 'do': 1}, metrics=['accuracy'], )\n",
    "\n",
    "    source_classification_model = Model(inputs=inputs, outputs=[source_classifier])\n",
    "    source_classification_model.compile(optimizer=\"Adam\",\n",
    "              loss={'mo': 'categorical_crossentropy'}, metrics=['categorical_accuracy'], )\n",
    "\n",
    "\n",
    "    domain_classification_model = Model(inputs=inputs, outputs=[domain_classifier])\n",
    "    domain_classification_model.compile(optimizer=\"Adam\",\n",
    "                  loss={'do': 'binary_crossentropy'}, metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    embeddings_model = Model(inputs=inputs, outputs=[x4])\n",
    "    embeddings_model.compile(optimizer=\"Adam\",loss = 'categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "    plot_model(comb_model, to_file='comb_model.png', show_shapes=True, show_layer_names=True)\n",
    "    plot_model(embeddings_model, to_file='embeddings_model.png', show_shapes=True, show_layer_names=True)\n",
    "    plot_model(source_classification_model, to_file='source_classifier_model.png', show_shapes=True, show_layer_names=True)\n",
    "    plot_model(domain_classification_model, to_file='domain_classifier_model.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "\n",
    "    from IPython.display import Image\n",
    "    Image(retina=True, filename='comb_model.png')\n",
    "\n",
    "                        \n",
    "    return comb_model, source_classification_model, domain_classification_model, embeddings_model\n",
    "\n",
    "def batch_generator(data, batch_size):\n",
    "    \"\"\"Generate batches of data.\n",
    "\n",
    "    Given a list of numpy data, it iterates over the list and returns batches of the same size\n",
    "    This\n",
    "    \"\"\"\n",
    "    all_examples_indices = len(data[0])\n",
    "    while True:\n",
    "        mini_batch_indices = np.random.choice(all_examples_indices, size=batch_size, replace=False)\n",
    "        tbr = [k[mini_batch_indices] for k in data]\n",
    "        yield tbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "\n",
    "def train(Xs, ys, Xt, yt, Xs_valid, ys_valid, Xt_valid, yt_valid, enable_dann = True, n_iterations = n_iters):\n",
    "        \n",
    "    model, source_classification_model, domain_classification_model, embeddings_model = build_models(ip_size)\n",
    "\n",
    "    \n",
    "    y_class_dummy = np.ones((len(Xt), 2))\n",
    "    y_adversarial_1 = to_categorical(np.array(([1] * batch_size + [0] * batch_size)))\n",
    "    \n",
    "    sample_weights_class = np.array(([1] * batch_size + [0] * batch_size))\n",
    "    sample_weights_adversarial = np.ones((batch_size * 2,))\n",
    "\n",
    "    S_batches = batch_generator([Xs, to_categorical(ys)], batch_size)\n",
    "    print (S_batches)\n",
    "    T_batches = batch_generator([Xt, np.zeros(shape = (len(Xt),2))], batch_size)\n",
    "    \n",
    "    #print S_batches, T_batches\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # # print(y_class_dummy.shape, ys.shape)\n",
    "        y_adversarial_2 = to_categorical(np.array(([0] * batch_size + [1] * batch_size)))\n",
    "#         print (\"y_adversarial_2:\", y_adversarial_2)\n",
    "\n",
    "        X0, y0 = next(S_batches)\n",
    "#         print('X0 size:', X0.shape)\n",
    "#         print('X0:', X0)\n",
    "        X1, y1 = next(T_batches)\n",
    "#         print('X1 size:', X1.shape)\n",
    "#         print('X1:', X1)\n",
    "\n",
    "        X_adv = np.concatenate([X0, X1])\n",
    "        \n",
    "        #print 'X_adv shape',X_adv.shape,'X0 shape', X0.shape, 'X1 shape',X1.shape\n",
    "        y_class = np.concatenate([y0, np.zeros_like(y0)])\n",
    "\n",
    "        adv_weights = []\n",
    "        for layer in model.layers:\n",
    "            if (layer.name.startswith(\"do\")):\n",
    "                adv_weights.append(layer.get_weights())\n",
    "\n",
    "        if(enable_dann):\n",
    "            # note - even though we save and append weights, the batchnorms moving means and variances\n",
    "            # are not saved throught this mechanism \n",
    "            \n",
    "            #print 'y class shape',y_class.shape,'y_adv1  shape',y_adversarial_1.shape\n",
    "\n",
    "            \n",
    "            stats = model.train_on_batch(X_adv, [y_class, y_adversarial_1],\n",
    "                                     sample_weight=[sample_weights_class, sample_weights_adversarial])\n",
    "            \n",
    "            k = 0\n",
    "            for layer in model.layers:\n",
    "                if (layer.name.startswith(\"do\")):\n",
    "                    layer.set_weights(adv_weights[k])\n",
    "                    k += 1\n",
    "\n",
    "            class_weights = []\n",
    "            \n",
    "        \n",
    "            for layer in model.layers:\n",
    "                if (not layer.name.startswith(\"do\")):\n",
    "                    class_weights.append(layer.get_weights())\n",
    "            \n",
    "            \n",
    "            #print 'y_adv2  shape',y_adversarial_2.shape\n",
    "            stats2 = domain_classification_model.train_on_batch(X_adv, [y_adversarial_2])\n",
    "\n",
    "            k = 0\n",
    "            for layer in model.layers:\n",
    "                if (not layer.name.startswith(\"do\")):\n",
    "                    layer.set_weights(class_weights[k])\n",
    "                    k += 1\n",
    "\n",
    "        else:\n",
    "            source_classification_model.train_on_batch(X0,y0)\n",
    "            \n",
    "       \n",
    "        if ((i + 1) % 100 == 0):\n",
    "            # print(i, stats)\n",
    "            ##y_test_hat_t = source_classification_model.predict(Xt).argmax(1)\n",
    "            ##y_test_hat_s = source_classification_model.predict(Xs).argmax(1)\n",
    "            ##print(\"Iteration %d, source accuracy =  %.3f, target accuracy = %.3f\"%(i, accuracy_score(ys, y_test_hat_s), accuracy_score(yt, y_test_hat_t)))\n",
    "            \n",
    "            # calculate train source and target accuracy\n",
    "            y_test_hat_t = source_classification_model.predict(Xt).argmax(1)\n",
    "            y_test_hat_s = source_classification_model.predict(Xs).argmax(1)\n",
    "            \n",
    "            #print y_test_hat_t.shape, yt.shape\n",
    "            #print y_test_hat_t, yt.reshape(-1)\n",
    "            \n",
    "            y_test_hat_t1 = keras.utils.to_categorical(np.asarray(y_test_hat_t)-1, n_classes)\n",
    "            y_test_hat_s1 = keras.utils.to_categorical(np.asarray(y_test_hat_s)-1, n_classes)\n",
    "            yt1 = keras.utils.to_categorical(np.asarray(yt)-1, n_classes)\n",
    "            ys1 = keras.utils.to_categorical(np.asarray(ys)-1, n_classes)\n",
    "\n",
    "            print(\"Iteration %d, source train accuracy =  %.3f, target train accuracy = %.3f\"%(i, accuracy_score(ys, y_test_hat_s), accuracy_score(yt, y_test_hat_t)))\n",
    "            print(\"Iteration %d, source train auroc =  %.3f, target train auroc = %.3f\"%(i, roc_auc_score(ys1, y_test_hat_s1), roc_auc_score(yt1, y_test_hat_t1)))\n",
    "            \n",
    "            # calculate valid source and target accuracy\n",
    "            y_valid_hat_t = source_classification_model.predict(Xt_valid).argmax(1)\n",
    "            y_valid_hat_s = source_classification_model.predict(Xs_valid).argmax(1)\n",
    "            \n",
    "            y_valid_hat_t1 = keras.utils.to_categorical(np.asarray(y_valid_hat_t)-1, n_classes)\n",
    "            y_valid_hat_s1 = keras.utils.to_categorical(np.asarray(y_valid_hat_s)-1, n_classes)\n",
    "            \n",
    "            yt1_valid = keras.utils.to_categorical(np.asarray(yt_valid)-1, n_classes)\n",
    "            ys1_valid = keras.utils.to_categorical(np.asarray(ys_valid)-1, n_classes)\n",
    "            \n",
    "            print(\"Iteration %d, source valid accuracy =  %.3f, target valid accuracy = %.3f\"%(i, accuracy_score(ys_valid, y_valid_hat_s), accuracy_score(yt_valid, y_valid_hat_t)))\n",
    "            print(\"Iteration %d, source valid auroc =  %.3f, target valid auroc = %.3f\"%(i, roc_auc_score(ys1_valid, y_valid_hat_s1), roc_auc_score(yt1_valid, y_valid_hat_t1)))\n",
    "            \n",
    "    return source_classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/xin/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "<generator object batch_generator at 0x1a2bf3a318>\n",
      "Iteration 99, source train accuracy =  0.983, target train accuracy = 0.774\n",
      "Iteration 99, source train auroc =  0.985, target train auroc = 0.840\n",
      "Iteration 99, source valid accuracy =  0.983, target valid accuracy = 0.766\n",
      "Iteration 99, source valid auroc =  0.986, target valid auroc = 0.834\n",
      "Iteration 199, source train accuracy =  0.993, target train accuracy = 0.776\n",
      "Iteration 199, source train auroc =  0.994, target train auroc = 0.846\n",
      "Iteration 199, source valid accuracy =  0.994, target valid accuracy = 0.767\n",
      "Iteration 199, source valid auroc =  0.995, target valid auroc = 0.840\n",
      "Iteration 299, source train accuracy =  0.996, target train accuracy = 0.775\n",
      "Iteration 299, source train auroc =  0.997, target train auroc = 0.847\n",
      "Iteration 299, source valid accuracy =  0.996, target valid accuracy = 0.767\n",
      "Iteration 299, source valid auroc =  0.997, target valid auroc = 0.842\n"
     ]
    }
   ],
   "source": [
    "embs = train(xs_train, ys_train, xt_train, yt_train, xs_valid, ys_valid, xt_valid, yt_valid, enable_dann = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Final Trained model \n",
    "# y_valid_hat_t = embs.predict(xt_valid).argmax(1)\n",
    "# y_valid_hat_s = embs.predict(xs_valid).argmax(1)\n",
    "\n",
    "# y_valid_hat_t1 = keras.utils.to_categorical(np.asarray(y_valid_hat_t)-1, n_classes)\n",
    "# y_valid_hat_s1 = keras.utils.to_categorical(np.asarray(y_valid_hat_s)-1, n_classes)\n",
    "\n",
    "# yt1_valid = keras.utils.to_categorical(np.asarray(yt_valid)-1, n_classes)\n",
    "# ys1_valid = keras.utils.to_categorical(np.asarray(ys_valid)-1, n_classes)\n",
    "\n",
    "# print(\"Final trained model, source valid accuracy =  %.3f, target valid accuracy = %.3f\"%(accuracy_score(ys_valid, y_valid_hat_s), accuracy_score(yt_valid, y_valid_hat_t)))\n",
    "# print(\"Final trained model, source valid auroc =  %.3f, target valid auroc = %.3f\"%(roc_auc_score(ys1_valid, y_valid_hat_s1), roc_auc_score(yt1_valid, y_valid_hat_t1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original X_t_test:  (494657, 20)\n",
      "rows_test: (array([     0,      1,      2, ..., 494654, 494655, 494656]),)\n",
      "rows_test.shape: 1\n",
      "after SZA X_t_test:  (208608, 20)\n",
      "after SZA X_s_test:  (208608, 25)\n",
      "(208608, 31)\n",
      "(208608, 20)\n"
     ]
    }
   ],
   "source": [
    "# data_test = np.load('/content/content/My Drive/Colab Notebooks/kddworkshop/fulldata/test_144_day.npz')\n",
    "data_test = np.load('test_144_day.npz')\n",
    "\n",
    "passive =1 \n",
    "\n",
    "#load common data\n",
    "latlon_test = data_test['latlon']\n",
    "iff_test = data_test['iff']\n",
    "\n",
    "# if passive ==1:\n",
    "x_t_test = data_test['viirs']\n",
    "y_t_test = data_test['label']\n",
    "# else:\n",
    "x_s_test = data_test['calipso']\n",
    "y_s_test = data_test['label']\n",
    "    \n",
    "inds_test,vals_test = np.where(y_t_test>0)\n",
    "\n",
    "# process common data\n",
    "Latlon_test = latlon_test[inds_test]\n",
    "Iff_test = iff_test[inds_test]\n",
    "\n",
    "Y_t_test = y_t_test[inds_test]\n",
    "X_t_test = x_t_test[inds_test]\n",
    "\n",
    "Y_s_test = y_s_test[inds_test]\n",
    "X_s_test = x_s_test[inds_test]\n",
    "\n",
    "# 0 =< SZA <= 83\n",
    "print('original X_t_test: ', X_t_test.shape)\n",
    "rows_test = np.where((X_t_test[:,0] >= 0) & (X_t_test[:,0] <= 83) & (X_t_test[:,15] > 100) & (X_t_test[:,15] < 400) & (X_t_test[:,16] > 100) & (X_t_test[:,16] < 400) & (X_t_test[:,17] > 100) & (X_t_test[:,17] < 400) & (X_t_test[:,18] > 100) & (X_t_test[:,18] < 400) & (X_t_test[:,19] > 100) & (X_t_test[:,19] < 400) & (X_t_test[:,10] > 0))\n",
    "print(\"rows_test:\", rows_test)\n",
    "print(\"rows_test.shape:\", len(rows_test))\n",
    "\n",
    "Latlon_test = Latlon_test[rows_test]\n",
    "Iff_test = Iff_test[rows_test]\n",
    "\n",
    "Y_t_test = Y_t_test[rows_test]\n",
    "X_t_test = X_t_test[rows_test]\n",
    "\n",
    "Y_s_test = Y_s_test[rows_test]\n",
    "X_s_test = X_s_test[rows_test]\n",
    "\n",
    "X_s_test = np.nan_to_num(X_s_test)\n",
    "X_t_test = np.nan_to_num(X_t_test)\n",
    "\n",
    "print('after SZA X_t_test: ', X_t_test.shape)\n",
    "print('after SZA X_s_test: ', X_s_test.shape)\n",
    "\n",
    "#concanate common data\n",
    "# X_t_test = np.concatenate((X_t_test, Latlon_test, Iff_test), axis=1)\n",
    "X_s_test = np.concatenate((X_s_test, Latlon_test, Iff_test), axis=1)\n",
    "\n",
    "print (X_s_test.shape)\n",
    "print (X_t_test.shape)\n",
    "\n",
    "X_test=np.concatenate((X_t_test, X_s_test), axis=1)\n",
    "\n",
    "x_test2=sc_X.transform(X_test)\n",
    "\n",
    "X_t_test = x_test2[:, 0:20]\n",
    "x_test_c2 = x_test2[:, 20:45]\n",
    "x_test_comm2 = x_test2[:, 45:51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.75204325 -0.60750973 -0.36847982 ... -0.022455    1.8166993\n",
      "   0.32035637]\n",
      " [-0.71309817 -0.58306277 -0.5306941  ... -0.01449026  1.8505003\n",
      "   0.26749146]\n",
      " [-0.8837462  -0.7165735  -0.6131298  ... -0.00974993  1.885965\n",
      "   0.23349693]\n",
      " ...\n",
      " [-0.94342333 -0.72023124 -0.75856835 ...  0.03272877  0.596092\n",
      "   0.1441794 ]\n",
      " [-0.7397635  -0.5320477  -0.6472182  ...  0.0152457  -0.00934259\n",
      "   0.3145504 ]\n",
      " [-1.195288   -0.92080253 -0.81429476 ...  0.03569934  0.588303\n",
      "  -0.06273407]]\n",
      "(208608, 25)\n"
     ]
    }
   ],
   "source": [
    "# MLP regressor transform the viirs data\n",
    "x_test_pt = model_reg.predict(X_t_test)\n",
    "print(x_test_pt)\n",
    "\n",
    "# x_test_pt_test = np.concatenate((x_test_pt, x_test_comm2),axis=1)\n",
    "x_test_pt_test = x_test_pt\n",
    "print(x_test_pt_test.shape)\n",
    "\n",
    "yt_test = Y_t_test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final trained model, target test_xxx_day accuracy = 0.602\n",
      "Final trained model, target test_xxx_day auroc = 0.739\n"
     ]
    }
   ],
   "source": [
    "# Predict on test_xxx_day data\n",
    "y_test_hat_t = embs.predict(x_test_pt_test).argmax(1)\n",
    "\n",
    "y_test_hat_t1 = keras.utils.to_categorical(np.asarray(y_test_hat_t)-1, n_classes)\n",
    "\n",
    "yt1_test = keras.utils.to_categorical(np.asarray(yt_test)-1, n_classes)\n",
    "\n",
    "print(\"Final trained model, target test_xxx_day accuracy = %.3f\"%(accuracy_score(yt_test, y_test_hat_t)))\n",
    "print(\"Final trained model, target test_xxx_day auroc = %.3f\"%(roc_auc_score(yt1_test, y_test_hat_t1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis Below, No need to see/use below this line for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 12-10-2019\n",
    "\n",
    "@author: Xin Huang\n",
    "\"\"\"\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# data = np.load('Sat_data_small.npz')\n",
    "data = np.load('train10.npz')\n",
    "\n",
    "# passive = 1\n",
    "\n",
    "#load common data\n",
    "latlon = data['latlon']\n",
    "iff = data['iff']\n",
    "\n",
    "X_v = data['viirs']\n",
    "Y_v = data['label']\n",
    "print ('X_v shape:');\n",
    "print (X_v.shape);\n",
    "\n",
    "X_c = data['calipso']\n",
    "Y_c = data['label']\n",
    "print ('X_c shape:');\n",
    "print (X_c.shape);\n",
    "\n",
    "inds,vals = np.where(Y_v>0)\n",
    "\n",
    "# process common data\n",
    "Latlon = latlon[inds]\n",
    "Iff = iff[inds]\n",
    "\n",
    "Y_v = Y_v[inds]\n",
    "X_v = X_v[inds]\n",
    "print ('X_v')\n",
    "print (X_v)\n",
    "\n",
    "# inds_c,vals_c = np.where(Y_c>0)\n",
    "\n",
    "Y_c = Y_c[inds]\n",
    "X_c = X_c[inds]\n",
    "print ('X_c')\n",
    "print (X_c)\n",
    "\n",
    "\n",
    "\n",
    "# 0 =< SZA <= 83\n",
    "print('original X_v: ', X_v.shape)\n",
    "rows = np.where((X_v[:,0] >= 0) & (X_v[:,0] <= 83) & (X_v[:,15] > 100) & (X_v[:,15] < 400) & (X_v[:,16] > 100) & (X_v[:,16] < 400) & (X_v[:,17] > 100) & (X_v[:,17] < 400) & (X_v[:,18] > 100) & (X_v[:,18] < 400) & (X_v[:,19] > 100) & (X_v[:,19] < 400) & (X_v[:,10] > 0))\n",
    "print(\"rows:\", rows)\n",
    "print(\"rows.shape:\", len(rows))\n",
    "\n",
    "Latlon = Latlon[rows]\n",
    "Iff = Iff[rows]\n",
    "\n",
    "Y_v = Y_v[rows]\n",
    "X_v = X_v[rows]\n",
    "\n",
    "Y_c = Y_c[rows]\n",
    "X_c = X_c[rows]\n",
    "\n",
    "X_c = np.nan_to_num(X_c)\n",
    "X_v = np.nan_to_num(X_v)\n",
    "\n",
    "print('after SZA X_v: ', X_v.shape)\n",
    "print('after SZA X_c: ', X_c.shape)\n",
    "\n",
    "# pca = decomposition.PCA(n_components=20)\n",
    "# pca.fit(X_s)\n",
    "# X_s = pca.transform(X_s)\n",
    "# print (X_s.shape)\n",
    "\n",
    "#concanate common data\n",
    "# X_v = np.concatenate((X_v, Latlon, Iff), axis=1)\n",
    "X_c = np.concatenate((X_c, Latlon, Iff), axis=1)\n",
    "\n",
    "print (X_v.shape)\n",
    "print (X_c.shape)\n",
    "\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.DataFrame(X_v, columns=['band1', 'band2', 'band3', 'band4', 'band5', 'band6', 'band7', 'band8', 'band9', 'band10','band11', 'band12', 'band13', 'band14', 'band15', 'band16', 'band17', 'band18','band19', 'band20'])\n",
    "df['label'] = Y_v\n",
    "print(df.head(5))\n",
    "fig=plt.figure()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.53182329 -0.53182332 -0.53182079 ...  0.0420989   0.08963409\n",
      "   0.31619685]\n",
      " [-0.53182329 -0.53182332 -0.53182079 ...  0.00996393 -0.02501277\n",
      "   0.46844565]\n",
      " [ 0.01953407  0.01953405  0.0195366  ...  0.26004047  0.07456922\n",
      "   0.33871202]\n",
      " ...\n",
      " [-0.53182329 -0.53182332 -0.53182079 ...  0.36861761  0.07131892\n",
      "   0.23990399]\n",
      " [ 0.01953941  0.01953779  0.01953991 ...  0.15939287  0.0604022\n",
      "   0.59214814]\n",
      " [ 3.63269252  3.63269249  3.63269522 ... -0.11353965 -0.03907268\n",
      "  -0.54627213]]\n"
     ]
    }
   ],
   "source": [
    "print (x_train_pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X2 = StandardScaler()\n",
    "sc_X2.fit(x_train_pt)\n",
    "x_train_pt=sc_X2.transform(x_train_pt)\n",
    "x_valid_pt=sc_X2.transform(x_valid_pt)\n",
    "x_test_pt=sc_X2.transform(x_test_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xin/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.746409663926994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8284767921594982"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use Logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                         multi_class='multinomial').fit(x_train_pt, y_train)\n",
    "y_pred = clf.predict(x_test_pt)\n",
    "print (clf.score(x_test_pt, y_test))\n",
    "\n",
    "num_classes1 = 6\n",
    "y_lr_gd = keras.utils.to_categorical(y_test-1, num_classes1)\n",
    "y_lr_pred = keras.utils.to_categorical(y_pred-1, num_classes1)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_lr_gd, y_lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train\n",
      "[[6]\n",
      " [6]\n",
      " [2]\n",
      " ...\n",
      " [2]\n",
      " [2]\n",
      " [6]]\n",
      "y_train converted\n",
      "[[0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_109 (Dense)            (None, 512)               23552     \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 198,310\n",
      "Trainable params: 198,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 136755 samples, validate on 102567 samples\n",
      "Epoch 1/30\n",
      "136755/136755 [==============================] - 11s 79us/step - loss: 1.0432 - acc: 0.6459 - val_loss: 0.7484 - val_acc: 0.7354\n",
      "Epoch 2/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.8626 - acc: 0.7152 - val_loss: 0.7233 - val_acc: 0.7488\n",
      "Epoch 3/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.8297 - acc: 0.7254 - val_loss: 0.7085 - val_acc: 0.7506\n",
      "Epoch 4/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.8099 - acc: 0.7301 - val_loss: 0.7070 - val_acc: 0.7545\n",
      "Epoch 5/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.8014 - acc: 0.7324 - val_loss: 0.6918 - val_acc: 0.7528\n",
      "Epoch 6/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7920 - acc: 0.7343 - val_loss: 0.7006 - val_acc: 0.7521\n",
      "Epoch 7/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7854 - acc: 0.7368 - val_loss: 0.6838 - val_acc: 0.7575\n",
      "Epoch 8/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7812 - acc: 0.7383 - val_loss: 0.6832 - val_acc: 0.7567\n",
      "Epoch 9/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7763 - acc: 0.7389 - val_loss: 0.6815 - val_acc: 0.7583\n",
      "Epoch 10/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7745 - acc: 0.7406 - val_loss: 0.6968 - val_acc: 0.7560\n",
      "Epoch 11/30\n",
      "136755/136755 [==============================] - 6s 43us/step - loss: 0.7689 - acc: 0.7406 - val_loss: 0.6851 - val_acc: 0.7587\n",
      "Epoch 12/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7679 - acc: 0.7410 - val_loss: 0.6760 - val_acc: 0.7611\n",
      "Epoch 13/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7671 - acc: 0.7417 - val_loss: 0.6789 - val_acc: 0.7599\n",
      "Epoch 14/30\n",
      "136755/136755 [==============================] - 6s 43us/step - loss: 0.7666 - acc: 0.7427 - val_loss: 0.6774 - val_acc: 0.7609\n",
      "Epoch 15/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7648 - acc: 0.7426 - val_loss: 0.6761 - val_acc: 0.7592\n",
      "Epoch 16/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7671 - acc: 0.7426 - val_loss: 0.6798 - val_acc: 0.7625\n",
      "Epoch 17/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7643 - acc: 0.7441 - val_loss: 0.6807 - val_acc: 0.7596\n",
      "Epoch 18/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7628 - acc: 0.7445 - val_loss: 0.6701 - val_acc: 0.7629\n",
      "Epoch 19/30\n",
      "136755/136755 [==============================] - 6s 43us/step - loss: 0.7636 - acc: 0.7433 - val_loss: 0.6707 - val_acc: 0.7627\n",
      "Epoch 20/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7596 - acc: 0.7429 - val_loss: 0.6739 - val_acc: 0.7605\n",
      "Epoch 21/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7629 - acc: 0.7420 - val_loss: 0.6663 - val_acc: 0.7623\n",
      "Epoch 22/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7593 - acc: 0.7446 - val_loss: 0.6743 - val_acc: 0.7636\n",
      "Epoch 23/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7603 - acc: 0.7439 - val_loss: 0.6707 - val_acc: 0.7613\n",
      "Epoch 24/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7615 - acc: 0.7445 - val_loss: 0.6742 - val_acc: 0.7616\n",
      "Epoch 25/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7597 - acc: 0.7433 - val_loss: 0.6693 - val_acc: 0.7629\n",
      "Epoch 26/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7584 - acc: 0.7447 - val_loss: 0.6722 - val_acc: 0.7628\n",
      "Epoch 27/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7606 - acc: 0.7459 - val_loss: 0.6795 - val_acc: 0.7627\n",
      "Epoch 28/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7577 - acc: 0.7460 - val_loss: 0.6793 - val_acc: 0.7632\n",
      "Epoch 29/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7583 - acc: 0.7441 - val_loss: 0.6801 - val_acc: 0.7615\n",
      "Epoch 30/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.7613 - acc: 0.7432 - val_loss: 0.6676 - val_acc: 0.7635\n",
      "Test loss: 0.6620687058554082\n",
      "Test accuracy: 0.7639786676032213\n",
      "Test score: [0.6620687058554082, 0.7639786676032213]\n"
     ]
    }
   ],
   "source": [
    "# use deep learning model\n",
    "num_classes = 6\n",
    "\n",
    "print ('y_train')\n",
    "print (y_train)\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train-1, num_classes)\n",
    "y_valid = keras.utils.to_categorical(y_valid-1, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test-1, num_classes)\n",
    "\n",
    "print ('y_train converted')\n",
    "print (y_train)\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 30\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(x_train_pt.shape[1],)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train_pt, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_valid_pt, y_valid))\n",
    "score = model.evaluate(x_test_pt, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('Test score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6620687058554082\n",
      "Test accuracy: 0.7639786676032213\n",
      "Test score: [0.6620687058554082, 0.7639786676032213]\n",
      "[[1.46878022e-03 5.26535511e-01 5.38771898e-02 1.71978876e-01\n",
      "  7.05717830e-03 2.39082426e-01]\n",
      " [5.75361044e-11 2.02527426e-05 9.79642272e-01 2.04527215e-03\n",
      "  4.91435266e-08 1.82922501e-02]\n",
      " [1.77109632e-06 1.12161669e-03 1.46082355e-08 2.24123795e-08\n",
      "  9.33094084e-01 6.57825097e-02]\n",
      " ...\n",
      " [2.00776439e-02 1.32450342e-01 4.94697243e-01 9.42886472e-02\n",
      "  3.79949361e-02 2.20491171e-01]\n",
      " [8.04204121e-02 7.15925358e-03 4.13641427e-03 1.36366025e-05\n",
      "  8.28366876e-01 7.99033195e-02]\n",
      " [2.30168662e-05 4.75512724e-03 3.51433840e-07 6.42637588e-07\n",
      "  9.00022626e-01 9.51981917e-02]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9489682439851773"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(x_test_pt, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('Test score:', score)\n",
    "predict_result = model.predict_proba(x_test_pt);\n",
    "print (predict_result)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, predict_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_73 (Dense)             (None, 512)               23552     \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 198,310\n",
      "Trainable params: 198,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 136755 samples, validate on 102567 samples\n",
      "Epoch 1/15\n",
      "136755/136755 [==============================] - 7s 49us/step - loss: 0.3644 - acc: 0.8706 - val_loss: 0.0302 - val_acc: 0.9956\n",
      "Epoch 2/15\n",
      "136755/136755 [==============================] - 5s 38us/step - loss: 0.0646 - acc: 0.9859 - val_loss: 0.0191 - val_acc: 0.9976\n",
      "Epoch 3/15\n",
      "136755/136755 [==============================] - 5s 39us/step - loss: 0.0463 - acc: 0.9906 - val_loss: 0.0174 - val_acc: 0.9981\n",
      "Epoch 4/15\n",
      "136755/136755 [==============================] - 5s 38us/step - loss: 0.0450 - acc: 0.9921 - val_loss: 0.0141 - val_acc: 0.9986\n",
      "Epoch 5/15\n",
      "136755/136755 [==============================] - 5s 38us/step - loss: 0.0413 - acc: 0.9929 - val_loss: 0.0141 - val_acc: 0.9985\n",
      "Epoch 6/15\n",
      "136755/136755 [==============================] - 5s 39us/step - loss: 0.0411 - acc: 0.9931 - val_loss: 0.0132 - val_acc: 0.9987\n",
      "Epoch 7/15\n",
      "136755/136755 [==============================] - 5s 39us/step - loss: 0.0385 - acc: 0.9936 - val_loss: 0.0169 - val_acc: 0.9985\n",
      "Epoch 8/15\n",
      "136755/136755 [==============================] - 5s 39us/step - loss: 0.0384 - acc: 0.9939 - val_loss: 0.0130 - val_acc: 0.9988\n",
      "Epoch 9/15\n",
      "136755/136755 [==============================] - 5s 39us/step - loss: 0.0416 - acc: 0.9938 - val_loss: 0.0116 - val_acc: 0.9989\n",
      "Epoch 10/15\n",
      "136755/136755 [==============================] - 5s 40us/step - loss: 0.0406 - acc: 0.9939 - val_loss: 0.0162 - val_acc: 0.9986\n",
      "Epoch 11/15\n",
      "136755/136755 [==============================] - 5s 38us/step - loss: 0.0428 - acc: 0.9937 - val_loss: 0.0119 - val_acc: 0.9990\n",
      "Epoch 12/15\n",
      "136755/136755 [==============================] - 5s 40us/step - loss: 0.0392 - acc: 0.9941 - val_loss: 0.0132 - val_acc: 0.9989\n",
      "Epoch 13/15\n",
      "136755/136755 [==============================] - 6s 40us/step - loss: 0.0395 - acc: 0.9946 - val_loss: 0.0147 - val_acc: 0.9989\n",
      "Epoch 14/15\n",
      "136755/136755 [==============================] - 5s 39us/step - loss: 0.0440 - acc: 0.9946 - val_loss: 0.0134 - val_acc: 0.9989\n",
      "Epoch 15/15\n",
      "136755/136755 [==============================] - 5s 37us/step - loss: 0.0502 - acc: 0.9939 - val_loss: 0.0182 - val_acc: 0.9986\n",
      "Test loss: 0.016849071714543588\n",
      "Test accuracy: 0.9986740374584419\n",
      "Test score: [0.016849071714543588, 0.9986740374584419]\n"
     ]
    }
   ],
   "source": [
    "# Experiments, references \n",
    "\n",
    "# use deep learning model\n",
    "batch_size = 256\n",
    "epochs = 15\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_valid, y_valid))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('Test score:', score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9996603756408294"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_result = model.predict_proba(x_test);\n",
    "print (predict_result)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, predict_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6611631949910323\n",
      "[ 3.63297909e+00 -1.01091895e+00  4.17880312e-02  1.54816115e-01\n",
      "  1.61379276e+00 -6.09930324e-01 -3.79627864e+00  1.35149422e-04\n",
      "  1.26073034e-03  2.61727207e-01  1.29948456e-02  2.37170662e-01\n",
      " -1.33159116e-01  3.56930404e-03 -3.91941027e-01 -5.58686480e-04\n",
      " -1.45929821e-04 -3.79621786e-05 -1.05407018e-02 -3.11334816e-03]\n"
     ]
    }
   ],
   "source": [
    "# references\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "mlr = LinearRegression()\n",
    "# Fit linear regression.\n",
    "mlr.fit(X_v, X_c[:,0])\n",
    "# Get the slope and intercept of the line best fit.\n",
    "print(mlr.intercept_)\n",
    "print(mlr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(341889, 45)\n",
      "[[3]\n",
      " [3]\n",
      " [3]\n",
      " ...\n",
      " [6]\n",
      " [6]\n",
      " [6]]\n"
     ]
    }
   ],
   "source": [
    "X=np.concatenate((X_v, X_c), axis=1)\n",
    "Y=Y_v\n",
    "print (X.shape)\n",
    "print (Y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iowWhxQHvxMo"
   },
   "outputs": [],
   "source": [
    "x_train, x_temp, y_train, y_temp = train_test_split(X, Y,\n",
    "                                                    test_size=0.6,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=Y)\n",
    "\n",
    "x_valid, x_test, y_valid, y_test = train_test_split(x_temp, y_temp,\n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.32186763  0.06481409 -0.55881887 -0.07118032  0.2677794   1.61927334]\n",
      "[[-2.68158752e+05  1.36456457e+05  2.34556886e+04  8.17350437e+04\n",
      "  -3.26284936e+05  6.10197570e+05 -1.86717785e+05  1.15136665e-03\n",
      "  -4.23970582e-02  4.61985570e+04  3.93571740e+00 -8.99035966e+04\n",
      "  -3.51468101e+04  3.21619644e+00  8.16656542e+03 -5.64750382e+00\n",
      "  -2.94516298e-03  3.27020060e-02  7.04622481e+00 -1.06227746e+01\n",
      "   7.77573029e-02 -7.82765662e-02 -4.98097120e-01  1.73382745e-01\n",
      "  -9.23511099e-02  1.41456985e-01  2.23518550e+00 -2.62107860e-01\n",
      "  -1.93921942e+00  4.74158021e-02 -3.01991512e-01  2.65116590e-01\n",
      "  -1.66443332e-02  6.09429334e-02 -8.28398469e-02 -2.04355877e-01\n",
      "   5.08505685e-02  4.22842343e-01  1.43619552e-01  8.35576448e-02\n",
      "  -1.69183285e-01 -1.42307477e-02 -8.89169534e-02  1.27865461e-01\n",
      "  -2.23144599e-02]\n",
      " [ 2.11309529e+05 -5.96722870e+04  8.92815771e+03 -5.18681658e+04\n",
      "  -1.35859512e+03  1.05206316e+03 -1.07246505e+05 -8.95203165e-03\n",
      "   2.50560537e-02 -4.82359736e+04 -3.52553311e+00  3.20776745e+04\n",
      "  -4.77637415e+03  2.62335494e+00  1.97916505e+04  1.61128046e+00\n",
      "   6.21772751e-03 -2.96820205e-03 -5.49119289e+00  6.03775848e+00\n",
      "  -4.28751602e-02  3.39289774e-02 -1.15760336e-01 -1.16231531e-01\n",
      "   1.16921273e-01 -4.09387584e-01  2.19789902e-01  3.54220257e-01\n",
      "  -3.37860130e-01 -3.18749476e-01 -2.09314276e-01 -3.21178075e-02\n",
      "  -2.25180957e-02  1.57344250e-01  2.49927561e-01  6.60070547e-02\n",
      "  -1.34682447e-01 -3.02078500e-01 -1.12073871e-01 -1.43725449e-01\n",
      "   2.81236736e-02  1.60387562e-01  3.41633673e-01 -1.81585241e-01\n",
      "  -7.35657088e-02]\n",
      " [ 4.95674330e+04 -4.67866660e+04 -4.58671128e+03 -2.08309853e+03\n",
      "   2.25552710e+05 -3.80064341e+05  1.34203798e+05  3.06126389e-03\n",
      "   4.43777836e-02  1.05254572e+04 -6.21522587e+00  4.34552836e+04\n",
      "   1.15883298e+04 -5.57263876e+00 -4.13736754e+04  2.59200740e+00\n",
      "  -1.80754740e-02 -4.08275814e-02  1.45623960e+01 -6.94401693e+00\n",
      "  -7.73854815e-02  9.18176465e-02 -2.56228702e-01  3.26607292e-02\n",
      "   9.08073657e-02  1.10949888e-01  1.13068383e+00 -3.03964602e-02\n",
      "  -1.09467328e+00 -2.94106139e-01 -3.78944904e-02 -4.53251325e-01\n",
      "   1.11826380e-01 -3.07088650e-01 -7.57206457e-02  1.34412185e-02\n",
      "   2.52016398e-01  9.75325851e-02  1.13379107e-01 -2.98321677e-02\n",
      "   2.83872133e-01 -1.38383371e-01  6.51519570e-02  1.14205443e-01\n",
      "  -3.24702362e-03]\n",
      " [ 7.22260970e+04 -4.33333149e+04  4.67680049e+03 -1.02134743e+04\n",
      "  -7.02984470e+03 -1.77817929e+04 -8.19654586e+03  1.59272542e-02\n",
      "  -2.64397155e-02 -7.90971951e+02  4.79904210e+00  1.93299916e+04\n",
      "   2.29564801e+02 -6.23401000e-01 -9.11534025e+03  1.09613725e+00\n",
      "   1.12648912e-02  1.58250367e-03 -1.35954836e+01  9.52934851e+00\n",
      "  -1.17782461e-01  3.55474487e-04 -1.49061897e-01 -8.06295066e-02\n",
      "  -1.43360307e-01 -1.21735321e-01  1.22669039e-03  1.98486580e-01\n",
      "  -7.92313875e-02  5.55407506e-01  5.22635868e-02  5.85052113e-02\n",
      "  -6.78929846e-02  2.66446396e-02 -4.15354862e-02  1.51543880e-01\n",
      "  -6.28451179e-02 -2.33966609e-01  3.24434374e-03  6.58421389e-03\n",
      "  -1.37744145e-01  1.74692067e-02 -4.03054980e-01 -2.60734056e-01\n",
      "   3.11889556e-02]\n",
      " [ 2.60363703e+05 -1.29669077e+05 -2.29808354e+04 -8.21511557e+04\n",
      "   3.22692334e+05 -6.02490058e+05  1.84334305e+05  3.67466870e-04\n",
      "   4.08566241e-02 -4.71304229e+04 -3.74466887e+00  8.86908999e+04\n",
      "   3.48800588e+04 -2.76446107e+00 -6.53780635e+03  5.35761229e+00\n",
      "   2.38653999e-03 -2.96882924e-02 -7.25711354e+00  1.04319798e+01\n",
      "  -8.67538388e-02  7.97926145e-02  4.75007899e-01 -2.76921947e-01\n",
      "  -3.08048877e-01 -1.26819151e-01 -2.16451292e+00  2.71613205e-01\n",
      "   1.90711361e+00 -9.99325613e-02  2.37185505e-01 -5.17946949e-02\n",
      "  -3.04093131e-02 -6.55783252e-02  2.85719007e-02  1.85556795e-01\n",
      "  -2.98694970e-01 -3.86690029e-01 -1.36970467e-01  3.19132381e-02\n",
      "  -2.18502029e-01 -8.46444403e-03  8.47045141e-02 -3.19050859e-01\n",
      "  -1.08615729e-01]\n",
      " [-3.25308011e+05  1.43004887e+05 -9.49310018e+03  6.45808507e+04\n",
      "  -2.13571669e+05  3.89086560e+05 -1.63772667e+04 -1.15553200e-02\n",
      "  -4.14536877e-02  3.94333543e+04  4.75066836e+00 -9.36502531e+04\n",
      "  -6.77476913e+03  3.12094946e+00  2.90686061e+04 -5.00953358e+00\n",
      "   1.15147829e-03  3.91995661e-02  4.73516915e+00 -8.43229528e+00\n",
      "   2.47039639e-01 -1.27618147e-01  5.44140156e-01  2.67739511e-01\n",
      "   3.36031655e-01  4.05535182e-01 -1.42237300e+00 -5.31815721e-01\n",
      "   1.54387060e+00  1.09964868e-01  2.59751186e-01  2.13542026e-01\n",
      "   2.56383462e-02  1.27735153e-01 -7.84034832e-02 -2.12193070e-01\n",
      "   1.93355568e-01  4.02360209e-01 -1.11986645e-02  5.15025194e-02\n",
      "   2.13433652e-01 -1.67782062e-02  4.81789437e-04  5.19299252e-01\n",
      "   1.76553966e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()  \n",
    "regressor.fit(x_train, y_train)\n",
    "print(regressor.intercept_)\n",
    "print(regressor.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rWHGcSo8vxMt",
    "outputId": "14d7754b-07c3-4069-acf8-343493fd40eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xin/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9979818070139519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                         multi_class='multinomial').fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print (clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_1s2lr8vxM2",
    "outputId": "a83db141-a3f5-402d-c6c7-28458e8661e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9984782962250461"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes1 = 6\n",
    "y_lr_gd = keras.utils.to_categorical(y_test-1, num_classes1)\n",
    "y_lr_pred = keras.utils.to_categorical(y_pred-1, num_classes1)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_lr_gd, y_lr_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Dety8ALvxM8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train\n",
      "[[6]\n",
      " [6]\n",
      " [2]\n",
      " ...\n",
      " [2]\n",
      " [2]\n",
      " [6]]\n",
      "y_train converted\n",
      "[[0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 6\n",
    "\n",
    "print ('y_train')\n",
    "print (y_train)\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train-1, num_classes)\n",
    "y_valid = keras.utils.to_categorical(y_valid-1, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test-1, num_classes)\n",
    "\n",
    "print ('y_train converted')\n",
    "print (y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRTkepA7vxNB",
    "outputId": "72e500e6-0bc2-40f0-fd0f-86d8a8555b40",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/xin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/xin/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               23552     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 198,310\n",
      "Trainable params: 198,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/xin/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 136755 samples, validate on 102567 samples\n",
      "Epoch 1/30\n",
      "136755/136755 [==============================] - 6s 41us/step - loss: 0.5617 - acc: 0.7880 - val_loss: 0.0673 - val_acc: 0.9839\n",
      "Epoch 2/30\n",
      "136755/136755 [==============================] - 5s 37us/step - loss: 0.1426 - acc: 0.9650 - val_loss: 0.0227 - val_acc: 0.9952\n",
      "Epoch 3/30\n",
      "136755/136755 [==============================] - 5s 39us/step - loss: 0.1000 - acc: 0.9771 - val_loss: 0.0255 - val_acc: 0.9952\n",
      "Epoch 4/30\n",
      "136755/136755 [==============================] - 6s 41us/step - loss: 0.0924 - acc: 0.9804 - val_loss: 0.0260 - val_acc: 0.9943\n",
      "Epoch 5/30\n",
      "136755/136755 [==============================] - 5s 39us/step - loss: 0.0778 - acc: 0.9836 - val_loss: 0.0196 - val_acc: 0.9961\n",
      "Epoch 6/30\n",
      "136755/136755 [==============================] - 6s 41us/step - loss: 0.0733 - acc: 0.9853 - val_loss: 0.0224 - val_acc: 0.9963\n",
      "Epoch 7/30\n",
      "136755/136755 [==============================] - 5s 39us/step - loss: 0.0718 - acc: 0.9866 - val_loss: 0.0163 - val_acc: 0.9970\n",
      "Epoch 8/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.0655 - acc: 0.9875 - val_loss: 0.0238 - val_acc: 0.9963\n",
      "Epoch 9/30\n",
      "136755/136755 [==============================] - 5s 39us/step - loss: 0.0628 - acc: 0.9882 - val_loss: 0.0234 - val_acc: 0.9969\n",
      "Epoch 10/30\n",
      "136755/136755 [==============================] - 6s 43us/step - loss: 0.0643 - acc: 0.9885 - val_loss: 0.0228 - val_acc: 0.9968\n",
      "Epoch 11/30\n",
      "136755/136755 [==============================] - 6s 41us/step - loss: 0.0680 - acc: 0.9881 - val_loss: 0.0199 - val_acc: 0.9973\n",
      "Epoch 12/30\n",
      "136755/136755 [==============================] - 5s 38us/step - loss: 0.0638 - acc: 0.9888 - val_loss: 0.0262 - val_acc: 0.9967\n",
      "Epoch 13/30\n",
      "136755/136755 [==============================] - 5s 35us/step - loss: 0.0636 - acc: 0.9889 - val_loss: 0.0293 - val_acc: 0.9961\n",
      "Epoch 14/30\n",
      "136755/136755 [==============================] - 5s 37us/step - loss: 0.0639 - acc: 0.9889 - val_loss: 0.0284 - val_acc: 0.9962\n",
      "Epoch 15/30\n",
      "136755/136755 [==============================] - 6s 45us/step - loss: 0.0619 - acc: 0.9895 - val_loss: 0.0205 - val_acc: 0.9973\n",
      "Epoch 16/30\n",
      "136755/136755 [==============================] - 6s 47us/step - loss: 0.0619 - acc: 0.9892 - val_loss: 0.0228 - val_acc: 0.9975\n",
      "Epoch 17/30\n",
      "136755/136755 [==============================] - 6s 41us/step - loss: 0.0655 - acc: 0.9893 - val_loss: 0.0132 - val_acc: 0.9980\n",
      "Epoch 18/30\n",
      "136755/136755 [==============================] - 6s 40us/step - loss: 0.0614 - acc: 0.9893 - val_loss: 0.0180 - val_acc: 0.9977\n",
      "Epoch 19/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.0602 - acc: 0.9901 - val_loss: 0.0210 - val_acc: 0.9975\n",
      "Epoch 20/30\n",
      "136755/136755 [==============================] - 6s 45us/step - loss: 0.0606 - acc: 0.9899 - val_loss: 0.0200 - val_acc: 0.9977\n",
      "Epoch 21/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.0662 - acc: 0.9902 - val_loss: 0.0255 - val_acc: 0.9965\n",
      "Epoch 22/30\n",
      "136755/136755 [==============================] - 6s 42us/step - loss: 0.0672 - acc: 0.9894 - val_loss: 0.0295 - val_acc: 0.9960\n",
      "Epoch 23/30\n",
      "136755/136755 [==============================] - 6s 44us/step - loss: 0.0645 - acc: 0.9896 - val_loss: 0.0224 - val_acc: 0.9965\n",
      "Epoch 24/30\n",
      "136755/136755 [==============================] - 6s 46us/step - loss: 0.0651 - acc: 0.9898 - val_loss: 0.0323 - val_acc: 0.9968\n",
      "Epoch 25/30\n",
      "136755/136755 [==============================] - 6s 46us/step - loss: 0.0605 - acc: 0.9907 - val_loss: 0.0339 - val_acc: 0.9965\n",
      "Epoch 26/30\n",
      "136755/136755 [==============================] - 6s 46us/step - loss: 0.0651 - acc: 0.9903 - val_loss: 0.0202 - val_acc: 0.9974\n",
      "Epoch 27/30\n",
      "136755/136755 [==============================] - 7s 51us/step - loss: 0.0636 - acc: 0.9903 - val_loss: 0.0229 - val_acc: 0.9975\n",
      "Epoch 28/30\n",
      "136755/136755 [==============================] - 6s 47us/step - loss: 0.0632 - acc: 0.9898 - val_loss: 0.0203 - val_acc: 0.9976\n",
      "Epoch 29/30\n",
      "136755/136755 [==============================] - 7s 49us/step - loss: 0.0643 - acc: 0.9896 - val_loss: 0.0229 - val_acc: 0.9954\n",
      "Epoch 30/30\n",
      "136755/136755 [==============================] - 6s 44us/step - loss: 0.0645 - acc: 0.9901 - val_loss: 0.0228 - val_acc: 0.9974\n",
      "Test loss: 0.021690756011512113\n",
      "Test accuracy: 0.9973480749168836\n",
      "Test score: [0.021690756011512113, 0.9973480749168836]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 30\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_valid, y_valid))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('Test score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  1.1416998e-31]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00\n",
      "  0.0000000e+00]\n",
      " ...\n",
      " [0.0000000e+00 7.6100818e-08 1.8864904e-18 2.7485150e-26 0.0000000e+00\n",
      "  9.9999988e-01]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00\n",
      "  0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00\n",
      "  0.0000000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9997734130200948"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_result = model.predict_proba(x_test);\n",
    "print (predict_result)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, predict_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b-FsgBx1vxNk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xin/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "/Users/xin/anaconda3/lib/python3.7/site-packages/dask_jobqueue/config.py:12: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n",
      "/Users/xin/anaconda3/lib/python3.7/site-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n",
      "tornado.application - ERROR - Exception in callback functools.partial(<bound method JobQueueCluster.scale_up of SLURMCluster(cores=0, memory=0 B, workers=0/0, jobs=0/0)>, 5)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/xin/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/Users/xin/anaconda3/lib/python3.7/site-packages/dask_jobqueue/core.py\", line 416, in scale_up\n",
      "    self.start_workers(n - self._count_active_and_pending_workers())\n",
      "  File \"/Users/xin/anaconda3/lib/python3.7/site-packages/dask_jobqueue/core.py\", line 330, in start_workers\n",
      "    out = self._submit_job(fn)\n",
      "  File \"/Users/xin/anaconda3/lib/python3.7/site-packages/dask_jobqueue/core.py\", line 322, in _submit_job\n",
      "    return self._call(shlex.split(self.submit_command) + [script_filename])\n",
      "  File \"/Users/xin/anaconda3/lib/python3.7/site-packages/dask_jobqueue/core.py\", line 372, in _call\n",
      "    **kwargs)\n",
      "  File \"/Users/xin/anaconda3/lib/python3.7/subprocess.py\", line 775, in __init__\n",
      "    restore_signals, start_new_session)\n",
      "  File \"/Users/xin/anaconda3/lib/python3.7/subprocess.py\", line 1522, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'sbatch': 'sbatch'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#!/usr/bin/env bash\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=1\n",
      "#SBATCH --mem=4G\n",
      "#SBATCH -t 00:30:00\n",
      "#SBATCH --exclusive\n",
      "#SBATCH --qos=normal\n",
      "#SBATCH --time=03:00:00\n",
      "JOB_ID=${SLURM_JOB_ID%;*}\n",
      "\n",
      "\n",
      "\n",
      "/Users/xin/anaconda3/bin/python -m distributed.cli.dask_worker tcp://192.168.1.166:57696 --nthreads 1 --memory-limit 4.00GB --name dask-worker--${JOB_ID}-- --death-timeout 60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import progress\n",
    "\n",
    "cluster = SLURMCluster(cores=1, memory='4 GB', job_extra=['--exclusive','--qos=normal','--time=03:00:00'])\n",
    "cluster.scale(5)\n",
    "\n",
    "client = Client(cluster)\n",
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Simple_Classifiers-shared.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
